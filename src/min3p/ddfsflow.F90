!*****Revision Informations Automatically Generated by VisualSVN*****!
!---------------------------------------------------------------------
!> $ID:$
!> $Revision: 869 $
!> $Author: dsu $
!> $Date: 2023-08-18 09:44:21 -0700 (Fri, 18 Aug 2023) $
!> $URL: https://min3psvn.ubc.ca/svn/min3p_thcm/branches/dsu_new_add_2024Jan/src/min3p/ddfsflow.F90 $
!---------------------------------------------------------------------
!********************************************************************!

!c ----------------------------------------------------------------------
!c subroutine ddfsflow
!c -----------------
!c
!c driver subroutine for fully saturated flow
!c
!c from Uli Mayer template 
!c
!c written by:      Tom Henderson - September 15, 2002
!c
!c last modified:   Tom Henderson - October 7, 2002
!c
!c                  Danyang Su    - Sept. 10, 2018
!c                  Unstructured grid and HPC capabilities
!c
!c definition of variables:
!c
!c I --> on input   * arbitrary  - initialized  + entries expected
!c O --> on output  * arbitrary  - unaltered    + altered
!c                                                                    I O
!c passed:   -
!c
!c common:   
!c gen.f:    real*8:
!c           -------
!c           avs(njavs)         = jacobian matrix                     * +
!c           afvs(njafvs)       = incomplete factorization            * +
!c           bvs(nn)            = rhs vector                          * +
!c           deltol_vs          = solver update tolerance             + -
!c           res_vs(nn)          = residual                            * *
!c           restol_vs          = solver residual tolerance           + -
!c           rmupdate           = maximum solution update (solver)    * +
!c           rnorm              = residual 2-norm                     * +
!c           rwork(8*nn)        = real*8 work array                   * *
!c           uvs(nn)            = update towards solution-vector      * +
!c           uvsnew(nn)         = solution vector (new time level)    + +
!c 
!c           integer*4:
!c           ----------
!c           igen               = unit number, generic output file    + -
!c           ilog               = unit number, logbook                + -
!c           idbg               = unit number, debugging information  + -
!c           iavs(nn+1)         = row pointer array for avs           + -
!c           iafvs(nn+1)        = row pointer array for afvs          + -
!c           iafdvs(nn)         = diagonal pointer array for afvs     + -
!c           idetail_vs         = solver information level            + -
!c           ittot_vs           = total number of iterations          + +
!c                                (variably saturated flow)
!c           iwork(2*nn+njafvs) = integer work array                  * *
!c           javs(njavs)        = connectivity list                   + -
!c           jafvs(njafvs)      = column pointer array for afvs       + -
!c           lordervs(nn)       = array containing ordering           + -
!c           invordvs(nn)       = array containing inverse ordering   + -
!c           nn                 = total number of control volumes     + -
!c           njavs              = number of global connections        + -
!c           njafvs             = number of factored connections      + -
!c           idbg               = unit number, debugging file         + -
!c           msolvit_vs         = max. number of solver iterations    + -
!c           itsolv             = actual number of solver iterations  * +
!c           itsolvtot_vs       = total number of solver              + +
!c                                iterations
!c                                (variably saturated flow)
!c           
!c           logical:
!c           --------
!c           transient_flow     = .true.  -> .not.steady_flow,        + -
!c                                        -> transient flow
!c
!c dens.f:   real*8:
!c           -------
!c           pressure(nn)       = fluid pressure                      + +
!c           density(nn)        = fluid density                       + -
!c
!c local:    real*8:
!c           -------
!c           gacc               = gravitational acceleration [m s^-2]  
!c
!c           integer*4:
!c           ----------
!c           ierr               = 0 -> memory allocation successful
!c           ilist              = pointer (integer work array)
!c           ivol               = counter (control volumes)
!c
!c           logical:
!c           --------
!c           over_flow          = .true.  -> ||r||_2 norm -> inf
!c
!c external: checkerr = check for error during memory allocation
!c           zero_r8   = clear real*8 array
!c           jacddfs   = construct jacobian matrix for fully 
!c                       saturated density dependent flow problem
!c           jacbvs    = incorporate boundary terms in Jacobian 
!c                       matrix for flow problem
!c           incompletefactorization = incomplete lu-decomposition
!c                                     of jacobian matrix
!c           ws209     = iterative solution of matrix equation
!c ----------------------------------------------------------------------
 
      subroutine ddfsflow
#ifdef PETSC
#include <petscversion.h>
#if (PETSC_VERSION_MAJOR >= 3 && PETSC_VERSION_MINOR >= 8)
#include <petsc/finclude/petscsys.h>
      use petscsys
#endif
#endif

      use parm
      use gen
      use dens
      use phys
      use dgml, only : dgm, maxwell 
      use file_unit, only : lun_get, lun_free
      use matrix_utility, only : export_arrays1d, export_mmformat,   &
                                 export_mmformat_gbl                   !for test, dsu
      use solver_results, only : solver_results_check_output
#ifdef PARDISO
      use solver_pardiso, only : pardiso_symbolicfactorization,      &
                                 pardiso_numfactorization,           &
                                 pardiso_substitution, ptvs,         &
                                 ptglob, iparm_vs, iparm_glob
#endif 

#ifdef OPENMP
      use omp_lib 
#endif 

#ifdef PETSC
      use solver_dd, only : solver_dd_snes_solve_flow_heat
      use petsc_mpi_common, only : petsc_mpi_finalize
#endif

#ifdef LIS
      use solver_lis, only : solver_lis_solve_flow
#endif

      implicit none
#ifdef PETSC
#if (PETSC_VERSION_MAJOR >= 3 && PETSC_VERSION_MINOR >= 6 && PETSC_VERSION_MINOR < 8)
#include <petsc/finclude/petscsys.h>
#elif (PETSC_VERSION_MAJOR >= 3 && PETSC_VERSION_MINOR < 6)
#include <finclude/petscsys.h>
#endif
#endif

#ifdef PETSC
      DOUBLE PRECISION :: mpireduce_in(2), mpireduce_out(2)
      integer*4 :: mpireduce_irank
      PetscErrorCode :: ierrcode
#endif

      integer :: info_debug, i, ierr, ilist, iter, iter_div, maxiter,  &
                 n_unknown_vs, n_unknown_glob, ibvs
      real*8 :: errormax, uvsmax
      real*8, external :: cputime

      integer :: ifile, idummy, ivol, iskip, nskip, maxvol
      character*256 :: strdummy, strfile
      
      external checkerr, zero_r8, infcvs_cp, jacddfs, jacbvs,          &
              incompletefactorization, ws209
      
      logical :: over_flow, b_redo_symbfac, b_freezing_pond
      
      real*8, parameter ::    &
      r0=0.0d0,               &
      r1=1.0d0,               &
      r2=2.0d0,               &
      r3=3.0d0
      integer, parameter ::   &
      i0=0
      
    
!cprovi------------------------------------------------------------------
!cprovi------------------------------------------------------------------
!cprovi------------------------------------------------------------------
      info_debug = 0

      if (b_use_zero_flow_vel) then
        not_converged = .false.
      else
        not_converged = .true.
      end if
!cprovi--------------------------------------------------------------------
!cprovi If energy balance is solved, then allocate specific arrays
!cprovi--------------------------------------------------------------------
      if (heat_transport) then        
        if (decoupled_type_vs_heat > 1) then       !fully decoupled           
          if (.not. allocated(aheat)) then
            allocate (aheat(njaheat), stat = ierr)
            aheat = 0.0d0
            call checkerr(ierr,'aheat',ilog)
            call memory_monitor(sizeof(aheat),'aheat',.true.)
          end if
        
          if (i_solver_type_flow == 0) then
            if (.not. allocated(afheat)) then
              allocate (afheat(njafheat), stat = ierr)
              afheat = 0.0d0
              call checkerr(ierr,'afheat',ilog)
              call memory_monitor(sizeof(afheat),'afheat',.true.)
            end if
          end if

          iter_heat = i0
        else                                       !fully or loosely coupled
          if (.not. allocated(aglob)) then
            allocate (aglob(njaglob), stat = ierr)
            aglob = 0.0d0
            call checkerr(ierr,'aglob',ilog)
            call memory_monitor(sizeof(aglob),'aglob',.true.)
          end if
        
          if (i_solver_type_flow == 0) then
            if (.not. allocated(afglob)) then
              allocate (afglob(njafglob), stat = ierr)
              afglob = 0.0d0
              call checkerr(ierr,'afglob',ilog)
              call memory_monitor(sizeof(afglob),'afglob',.true.)
            end if
          end if

          iter_glob = i0
        end if         
      end if  
      
!c  allocate memory for flow solver
      if (.not.heat_transport .or. decoupled_type_vs_heat > 1) then
        if(.not. allocated(avs)) then
          allocate (avs(njavs), stat = ierr)
          avs = 0.0d0
          call checkerr(ierr,'avs',ilog)
          call memory_monitor(sizeof(avs),'avs',.true.)
        end if
        
        if (i_solver_type_flow == 0) then
          if(.not. allocated(afvs)) then
            allocate (afvs(njafvs), stat = ierr)
            afvs = 0.0d0
            call checkerr(ierr,'afvs',ilog)
            call memory_monitor(sizeof(afvs),'afvs',.true.)
          end if
        end if
        
        iter_vs = i0         
      end if
!cprovi--------------------------------------------------------------------
!cprovi 
!cprovi--------------------------------------------------------------------
      do while (not_converged)          !newton iteration loop

        prt_flow_tot = cputime()
          
        errormax=r0

!cprovi--------------------------------------------
!cprovi Energy balance 
!cprovi--------------------------------------------        
        if (heat_transport .and. decoupled_type_vs_heat <= 1) then
        
          iter_glob = iter_glob + 1         !iteration counter (current)
          ittot_glob = ittot_glob + 1       !iteration counter (total)
          ittot_vs = ittot_glob
           
          if (idetail_glob.eq.2 .and. rank == 0 .and. b_enable_output) then
            write(ilog,'(/a,i3,a)') 'Newton iteration ',iter_glob,':'
            write(ilog,'(a)') '---------------------'
          end if
           
!cprovi---------------------------------------------------------------------
!cprovi Initialice arrays
!cprovi--------------------------------------------------------------------  
  
          call zero_r8(aglob, size(aglob, 1), 1, 1)  
          call zero_r8(bglob, size(bglob, 1), 1, 1)  
          call zero_r8(uglob, size(uglob, 1), 1, 1)
  
          if (i_solver_type_flow == 0) then
            call zero_r8(afglob, size(afglob, 1), 1, 1) 
          end if
         
          prt_flow_jac = cputime()
         
!c  compute influence coefficients in terms of conductivities
          if (gas_advection .or. dgm .or. maxwell) then
            call infcvs_cp
          end if 
         
!cprovi--------------------------------------------------------------------
!cprovi assemble matrix and rhs-vector for flow and heat equation  
!cprovi Parallelized, OpenMP, DSU
!cprovi--------------------------------------------------------------------
         
          call jacdd_energybal

          !Export sparse matrix dataset and right hand side. For test only, dsu.

          if((b_output_matrix.or.itimestep_output_matrix == mtime .or. &
             (itimestep_output_matrix > 0 .and. mtime == 0)).and.      &
             b_enable_output) then
            if(itype_matrix_format == 0) then
              call export_arrays1d(2*nngl, njaglob, iaglob, jaglob,    &
                   aglob, bglob, uglob, .true., .true., .false.,       &
                   "ddfsflow_glob_a", ittot_glob)
            else if(itype_matrix_format == 1) then
              call export_mmformat(2*nngl, njaglob, iaglob, jaglob,    &
                   aglob, bglob, uglob, .true., .true., .false.,       &
                   "ddfsflow_glob_a", ittot_glob)
#ifdef PETSC
              call export_mmformat_gbl(2*nn,2*nngl,njaglob,iaglob,     &
                   jaglob,aglob,bglob,uglob,.true.,.true.,.false.,     &
                   "ddfsflow_glob_a",nngl,nngbl,.false.,ittot_glob)
#endif
            end if
          end if

!cprovi--------------------------------------------------------------------
!cprovi adjust matrix and rhs vector for boundary conditions for flow
!cprovi and heat equations 
!cprovi Parallelized, OpenMP, DSU
!cprovi--------------------------------------------------------------------
          call jacbvs_energybal
       
         
          !Export sparse matrix dataset and right hand side. For test only, dsu.
             
          if((b_output_matrix.or.itimestep_output_matrix == mtime .or. &
             (itimestep_output_matrix > 0 .and. mtime == 0)).and.      &
             b_enable_output) then
            if(itype_matrix_format == 0) then
              call export_arrays1d(2*nngl, njaglob, iaglob, jaglob,    &
                   aglob, bglob, uglob, .true., .true., .false.,       &
                   "ddfsflow_glob", ittot_glob)
            else if(itype_matrix_format == 1) then
              call export_mmformat(2*nngl, njaglob, iaglob, jaglob,    &
                   aglob, bglob, uglob, .true., .true., .false.,       &
                   "ddfsflow_glob", ittot_glob)
#ifdef PETSC
              call export_mmformat_gbl(2*nn,2*nngl,njaglob,iaglob,     &
                   jaglob,aglob,bglob,uglob,.true.,.true.,.false.,     &
                   "ddfsflow_glob",nngl,nngbl,.false.,ittot_glob)
#endif
             end if
          end if
         
!cprovi--------------------------------------------------------------------
!cprovi Estimate condition number for the current matrix. 
!cprovi This is used for testing when newton iteration failed.
!cprovi--------------------------------------------------------------------   
#ifdef CONDITION_NUMBER
          if(b_output_condition_number) then
            call cond_num_cal(2*nngl, njaglob, iaglob, jaglob, aglob,  &
                      condition_number, condition_number_info)

            if (rank == 0 .and. b_enable_output) then            
              if (condition_number_info(1) .ge. 0 ) then
                write(*,"(2(a, 1pe15.6e3, 1x))")                         &
                      " classical cond. num. ", condition_number(1),     &
                      " skeel cond. num. ", condition_number(2)
                write(ilog,"(2(a, 1pe15.6e3, 1x))")                      &
                      " classical cond. num. ", condition_number(1),     &
                      " skeel cond. num. ", condition_number(2)
              else
                write(*,*)                                               &
                      ' error in estimating condition number, info(1) ', &
                      condition_number_info(1)
                write(ilog,*)                                            &
                      ' error in estimating condition number, info(1) ', &
                      condition_number_info(1)
              endif 
            
              if(i_solver_type_flow == 1) then
                if(condition_number(1) > 1.0e10 .and.                    &
                       condition_number(2) > 1.0e10) then
                    write(*,"(a)")                                       &
                          " Warning: matrix is ill-conditioned."
                    write(ilog,"(a)")                                    &
                          " Warning: matrix is ill-conditioned."
                end if
              end if            
            end if            
          end if
#endif         
          prt_flow_jac = cputime() - prt_flow_jac
         
          prt_flow_solver = 0.0d0
         
          !! use ws209 solver
          if (i_solver_type_flow == 0) then
#ifdef PARDISO         
            if (b_solver_test_pardiso) then
    !$omp parallel                                                    &
    !$omp if (njaglob > numofloops_thred_global)                      &
    !$omp num_threads(numofthreads_global)                            &
    !$omp default(shared)                                             &
    !$omp private (i)  
    !$omp do schedule(static)
              do i = 1, njaglob
                aglob_std(i) = aglob(imapglob_std(i))
              end do 
    !$omp end do
    !$omp end parallel
            
              b_redo_symbfac = .true.
            
100           prt_flow_symbfac_comp = cputime()
              if(bsymbolicfactor_glob.or.i_symfactor_type_flow == 1) then
                call pardiso_symbolicfactorization(iparm_glob,ptglob,  &
                         2*nngl,njaglob,iaglob,jaglob_std,aglob_std)
                n_unknown_glob = 2*nngl
                bsymbolicfactor_glob = .false.                
              end if  
              prt_flow_symbfac_comp = cputime() - prt_flow_symbfac_comp
            
              prt_flow_fac_comp = cputime()
              call pardiso_numfactorization(iparm_glob,ptglob,         &
                     2*nngl,njaglob, iaglob, jaglob_std, aglob_std)
              prt_flow_fac_comp = cputime() - prt_flow_fac_comp
            
              prt_flow_sub_comp = cputime()
              call pardiso_substitution(ilog,msolvit_glob,itsolv,      &
                     idetail_glob,res_glob,restol_glob,deltol_glob,    &
                     over_flow,rnorm,rmupdate,iparm_glob, ptglob,      &
                     2*nngl, njaglob, iaglob, jaglob_std, aglob_std,   &
                     bglob, uglob_std) 
              prt_flow_sub_comp = cputime() - prt_flow_sub_comp
            
              if (b_redo_symbfac .and.                                 &
                   (itsolv > n_max_iteration_flow .or.                 &
                   rnorm > r_max_residual_flow .or. over_flow)) then
                bsymbolicfactor_glob = .true.  
                b_redo_symbfac = .false.
                goto 100
              end if            
            end if         
#endif

#ifdef PETSC
            if(b_solver_test_petsc) then
              !only solver the local part, update the ghost value
              call solver_dd_snes_solve_flow_heat(ilog,.true.,         &
                      idetail_glob,aglob,bglob,uglob_std,              &
                      iaglob,jaglob,2*nngl,itsolv,over_flow,           &
                      rnorm,row_idx_l2pg_glob,col_idx_l2pg_glob,.true.)
              over_flow_vs = over_flow
#ifdef DEBUG
              if(rank == 0 .and. b_enable_output) then
                write(*,'(a, 2(1x, i5), 1x, l1, 1x, 1pe15.6e3)')       &
                  "ddfsflow-A: rank, iteration, over_flow, rnorm ",    &
                   rank, itsolv, over_flow, rnorm
              end if  
#endif
            end if
#endif

#ifdef LIS 
            if (b_solver_test_lis) then         
              !only solver the local part, update the ghost value
              call solver_lis_solve_flow(ilog,idetail_glob,aglob,      &
                          bglob,uglob_std,iaglob,jaglob,2*nngl,2*nn,2, &
                          itsolv,over_flow,rnorm,row_idx_l2pg_glob,    &
                          col_idx_l2pg_glob,.true.)
              over_flow_vs = over_flow           
#ifdef DEBUG
              if(rank == 0 .and. b_enable_output) then
                write(*,'(a, 2(1x, i5), 1x, l1, 1x, 1pe15.6e3)')       &
                  "ddfsflow-A2: rank, iteration, over_flow, rnorm ",   &
                  rank, itsolv, over_flow, rnorm 
              end if
#endif
            end if
#endif 
         
!cprovi--------------------------------------------------------------------
!cprovi Scale [avs] and {bvs} to produce unit diagonal
!cprovi Generate re-ordered preconditioner [af]
!cprovi--------------------------------------------------------------------
            ilist = 1
            prt_flow_fac = cputime()

             
            call incompletefactorization (2*nngl,njaglob,njafglob,bglob,  &
                                         aglob,afglob,rwork_max,iaglob,   &
                                         jaglob,iafglob,iafdglob,         &
                                         jafglob,iwork_max(ilist),        &
                                         lorderglob,invordglob,           &
                                         numofthreads_ws209)
         
            prt_flow_fac = cputime() - prt_flow_fac 
 
!c  solve [avs] * {uvs} = {bvs}
            prt_flow_sub = cputime() 
         
            call ws209(ilog,2*nngl,msolvit_glob,itsolv,idetail_glob,   &
                   iaglob,jaglob,iafglob,iafdglob,jafglob,lorderglob,  &
                   aglob,afglob,uglob,bglob,res_glob,rwork_max,        &
                   restol_glob,deltol_glob,njaglob,njafglob,over_flow, &
                   rnorm,rmupdate,numofthreads_ws209,                  &
                   rank,b_enable_output)
        
            prt_flow_sub = cputime() - prt_flow_sub

#ifdef PARDISO

            if (b_solver_test_pardiso) then     
              call solver_results_check_output(ittot_glob, 2*nngl, uglob, &
                       uglob_std, "ddfsflow_glob")
            end if

#endif

#ifdef PETSC
            if (b_solver_test_petsc) then        
              call solver_results_check_output(ittot_glob, 2*nngl, uglob,  &
                       uglob_std, "ddfsflow_glob_petsc") 
            end if         
#endif

#ifdef LIS
            if (b_solver_test_lis) then        
              call solver_results_check_output(ittot_glob, 2*nngl, uglob,  &
                       uglob_std, "ddfsflow_glob_lis") 
            end if           
#endif

            prt_flow_solver = prt_flow_symbfac + prt_flow_fac + prt_flow_sub
          !! use pardiso solver
          else if (i_solver_type_flow == 1) then
#ifdef PARDISO

    !$omp parallel                                                    &
    !$omp if (njaglob > numofloops_thred_global)                      &
    !$omp num_threads(numofthreads_global)                            &
    !$omp default(shared)                                             &
    !$omp private (i)  
    !$omp do schedule(static)
            do i = 1, njaglob
              aglob_std(i) = aglob(imapglob_std(i))
            end do
    !$omp end do
    !$omp end parallel            
           
            b_redo_symbfac = .true.
            
200         prt_flow_symbfac = cputime()
            if(bsymbolicfactor_glob .or. i_symfactor_type_flow == 1) then
              !write(idbg, *) "pardiso symbolic factorization for ddfsflow line 269"
              call pardiso_symbolicfactorization(iparm_glob, ptglob,   &
                       2*nngl, njaglob, iaglob, jaglob_std, aglob_std)
              n_unknown_glob = 2*nngl
              bsymbolicfactor_glob = .false.
            end if    
            prt_flow_symbfac = cputime() - prt_flow_symbfac
            
            !write(idbg, *) "pardiso numerical factorization for ddfsflow line 273"
            prt_flow_fac = cputime()
            call pardiso_numfactorization(iparm_glob, ptglob, 2*nngl,  &
                     njaglob, iaglob, jaglob_std, aglob_std)
            prt_flow_fac = cputime() - prt_flow_fac
            
            !write(idbg, *) "pardiso substitution for ddfsflow line 275"
            prt_flow_sub = cputime()
            call pardiso_substitution(ilog, msolvit_glob, itsolv,      &
                     idetail_glob,res_glob, restol_glob, deltol_glob,  &
                     over_flow, rnorm, rmupdate, iparm_glob, ptglob,   &
                     2*nngl, njaglob, iaglob, jaglob_std, aglob_std,   &
                     bglob, uglob) 
            prt_flow_sub = cputime() - prt_flow_sub
            
            if (b_redo_symbfac.and.                                    &
               (itsolv > n_max_iteration_flow .or.                     &
                rnorm > r_max_residual_flow .or. over_flow)) then
                bsymbolicfactor_glob = .true. 
                b_redo_symbfac = .false.
                goto 200
            end if

#endif 
            prt_flow_solver = prt_flow_symbfac + prt_flow_fac + prt_flow_sub
          !! use PETSc solver
          else if (i_solver_type_flow == 2) then
#ifdef PETSC          
            prt_flow_solver = cputime()
            !only solver the local part, update the ghost value
            call solver_dd_snes_solve_flow_heat(ilog,.true.,           &
                      idetail_glob,aglob,bglob,uglob,                  &
                      iaglob,jaglob,2*nngl,itsolv,over_flow,           &
                      rnorm,row_idx_l2pg_glob,col_idx_l2pg_glob,.true.)
            over_flow_vs = over_flow
           
            prt_flow_solver = cputime() - prt_flow_solver
#ifdef DEBUG
            if(rank == 0 .and. b_enable_output) then
              write(*,'(a, 2(1x, i5), 1x, l1, 1x, 1pe15.6e3)')         &
                  "ddfsflow-B: rank, iteration, over_flow, rnorm ",    &
                  rank, itsolv, over_flow, rnorm 
            end if
#endif
#endif
          !! use LIS solver
          else if (i_solver_type_flow == 3) then
#ifdef LIS          
            prt_flow_solver = cputime()
            !only solver the local part, update the ghost value
            call solver_lis_solve_flow(ilog,idetail_glob,aglob,        &
                        bglob,uglob,iaglob,jaglob,2*nngl,2*nn,2,       &
                        itsolv,over_flow,rnorm,                        &
                        row_idx_l2pg_glob,col_idx_l2pg_glob,.true.)
            over_flow_vs = over_flow           
            prt_flow_solver = cputime() - prt_flow_solver
#ifdef DEBUG
            if(rank == 0 .and. b_enable_output) then
              write(*,'(a, 2(1x, i5), 1x, l1, 1x, 1pe15.6e3)')         &
                    "ddfsflow-B2: rank, iteration, over_flow, rnorm ", &
                    rank, itsolv, over_flow, rnorm 
            end if
#endif
#endif
          end if
        
          !Export sparse matrix dataset and right hand side. For test only, dsu.       
          if((b_output_matrix.or.itimestep_output_matrix == mtime .or. &
              (itimestep_output_matrix > 0 .and. mtime == 0)).and.     &
              b_enable_output) then
            if(itype_matrix_format == 0) then
              call export_arrays1d(2*nngl, njaglob, iaglob, jaglob,    &
                       aglob, bglob, uglob, .false., .false., .true.,  &
                       "ddfsflow_glob", ittot_glob)
            else if(itype_matrix_format == 1) then
              call export_mmformat(2*nngl, njaglob, iaglob, jaglob,    &
                       aglob, bglob, uglob, .false., .false., .true.,  &
                       "ddfsflow_glob", ittot_glob)
#ifdef PETSC
              call export_mmformat_gbl(2*nn, 2*nngl, njaglob, iaglob,  &
                     jaglob,aglob,bglob,uglob,.false.,.false.,.true.,  &
                     "ddfsflow_glob",nngl,nngbl,.false.,ittot_glob)
#endif
            end if
          end if 
             
!#ifdef DEBUG
!cdsu  debug part, use external solution (written in sequential order) to test
!          if (mtime >= 1 .and. mtime <= 5) then
!                call export_mmformat_gbl(2*nn,2*nngl,njaglob,iaglob,  &
!                     jaglob,aglob,bglob,uglob,.true.,.true.,.false., &
!                     "ddfsflow_glob",nngl,nngbl,.false.,ittot_glob)
!            if (rank == 0) then
!              write(*,*) "-->read hydraulic head and temperature change from external"
!            end if
!
!            ifile = lun_get()
!            write(strfile, *) ittot_glob
!            strfile = "x_ddfsflow_glob_"//trim(adjustl(strfile))//"_natgbl.txt"
!            open(ifile,file=trim(strfile),status='old',form='formatted')
!            uglob = 0.0d0
!            nskip = 0
!            read(ifile,*) strdummy
!            do ivol = 1, nngl
!#ifdef PETSC
!              do iskip = 1, node_idx_lg2g(ivol) - nskip -1
!                read(ifile,*) idummy
!              end do
!              nskip = node_idx_lg2g(ivol)
!#endif
!              read(ifile,*) idummy,uglob(ivol)
!            end do
!
!            rewind(ifile)
!            read(ifile,*) strdummy
!            do ivol = 1, nngbl
!              read(ifile,*) idummy
!            end do
!
!            nskip = 0
!            do ivol = 1, nngl
!#ifdef PETSC
!              do iskip = 1, node_idx_lg2g(ivol) - nskip -1
!                read(ifile,*) idummy
!              end do
!              nskip = node_idx_lg2g(ivol)
!#endif
!              read(ifile,*) idummy,uglob(ivol+nngl)
!            end do
!
!            call lun_free(ifile)
!          end if
!cdsu  debug part, use external solution (written in sequential order) to test, end
!#endif

          if (.not.transient_flow.and.itsolv.eq.msolvit_glob) then
          
            if(rank == 0 .and. b_enable_output) then
              write(*,'(a/a)')                                         &
                   'maximum number of inner iterations exceeded',      &
                   'steady state flow solution non-convergent'
            
              write(ilog,'(a/a)')                                      &
                 'maximum number of inner iterations exceeded',        &
                 'steady state flow solution non-convergent' 
            end if  
             
#ifdef PETSC
            call petsc_mpi_finalize
#endif
            stop
          end if
        
        else
!cprovi--------------------------------------------
!cprovi Only flow  
!cprovi--------------------------------------------         
          iter_vs = iter_vs+1             !iteration counter (current)
          ittot_vs = ittot_vs+1           !iteration counter (total)

          if (idetail_vs.eq.2 .and. rank == 0 .and. b_enable_output) then
            write(ilog,'(/a,i3,a)') 'Newton iteration ',iter_vs,':'
            write(ilog,'(a)') '---------------------'
          end if
!c  clear arrays
 
          call zero_r8(avs, size(avs, 1), 1, 1) 
          call zero_r8(bvs, size(bvs, 1), 1, 1) 
          call zero_r8(uvs, size(uvs, 1), 1, 1)
 
          if (i_solver_type_flow == 0) then
            call zero_r8(afvs, size(afvs, 1), 1, 1)
          end if
           
          prt_flow_jac = cputime()
           
!c  compute influence coefficients in terms of conductivities
          if (gas_advection .or. dgm .or. maxwell) then
            call infcvs_cp
          end if 

!c Compute the jacobian and residual
          call jacddfs
 
!c  adjust matrix and rhs vector for boundary conditions 
          call jacbvs

          !Export sparse matrix dataset and right hand side. For test only, dsu.   
          if((b_output_matrix.or.itimestep_output_matrix == mtime .or. &
              (itimestep_output_matrix > 0 .and. mtime == 0)).and.     &
              b_enable_output) then
            if(itype_matrix_format == 0) then
              call export_arrays1d(nngl, njavs, iavs, javs,            &
                          avs, bvs, uvs, .true., .true., .false.,      &
                          "ddfsflow_vs", ittot_vs)
            else if(itype_matrix_format == 1) then
              call export_mmformat(nngl, njavs, iavs, javs,            &
                          avs, bvs, uvs, .true., .true., .false.,      &
                          "ddfsflow_vs", ittot_vs)
#ifdef PETSC
              call export_mmformat_gbl(nn,nngl,njavs,iavs,javs,        &
                          avs,bvs,uvs,.true.,.true.,.false.,           &
                          "ddfsflow_vs",nngl,nngbl,.false.,ittot_vs)
#endif
            end if
          end if

          prt_flow_jac = cputime() - prt_flow_jac
           
!cprovi--------------------------------------------------------------------
!cprovi Estimate condition number for the current matrix. 
!cprovi This is used for testing when newton iteration failed.
!cprovi-------------------------------------------------------------------- 
#ifdef CONDITION_NUMBER

          if(b_output_condition_number) then
            call cond_num_cal(nngl, njavs, iavs, javs, avs,            &
                 condition_number, condition_number_info) 
            
            if (rank == 0 .and. b_enable_output) then                
              if (condition_number_info(1) .ge. 0) then
                write(*,"(2(a, 1pe15.6e3, 1x))")                       &
                      " classical cond. num. ", condition_number(1),   &
                      " skeel cond. num. ", condition_number(2)
                write(ilog,"(2(a, 1pe15.6e3, 1x))")                    &
                      " classical cond. num. ", condition_number(1),   &
                      " skeel cond. num. ", condition_number(2)
              else
                write(*,*)                                             &
                    ' error in estimating condition number, info(1) ', &
                    condition_number_info(1)
                write(ilog,*)                                          &
                    ' error in estimating condition number, info(1) ', &
                    condition_number_info(1)
              endif             
            
              if(i_solver_type_flow == 1) then
                if(condition_number(1) > 1.0e10 .and.                  &
                        condition_number(2) > 1.0e10) then
                  write(*,"(a)")                                       &
                        " Warning: matrix is ill-conditioned."
                  write(ilog,"(a)")                                    &
                        " Warning: matrix is ill-conditioned."
                end if
              end if            
            end if            
          end if
         
#endif           
          prt_flow_solver = 0.0d0
          !! use ws209 solver
          if (i_solver_type_flow == 0) then
#ifdef PARDISO
               
            if (b_solver_test_pardiso) then
    !$omp parallel                                                    &
    !$omp if (njavs > numofloops_thred_global)                        &
    !$omp num_threads(numofthreads_global)                            &
    !$omp default(shared)                                             &
    !$omp private (i)  
    !$omp do schedule(static)
              do i = 1, njavs
                avs_std(i) = avs(imapvs_std(i))
              end do
    !$omp end do
    !$omp end parallel          
           
              b_redo_symbfac = .true.
300           prt_flow_symbfac_comp = cputime()
              if(bsymbolicfactor_vs .or.                               &
                 i_symfactor_type_flow == 1) then
                call pardiso_symbolicfactorization(iparm_vs, ptvs,     &
                            nngl, njavs, iavs, javs_std, avs_std)
                n_unknown_vs = nngl
                bsymbolicfactor_vs = .false.
              end if 
              prt_flow_symbfac_comp=cputime()-prt_flow_symbfac_comp

              prt_flow_fac_comp = cputime()
              call pardiso_numfactorization(iparm_vs, ptvs, nngl,      &
                          njavs, iavs, javs_std, avs_std)
              prt_flow_fac_comp = cputime() - prt_flow_fac_comp

              prt_flow_sub_comp = cputime()
              call pardiso_substitution(ilog, msolvit_vs, itsolv,      &
                      idetail_vs, res_vs, restol_vs, deltol_vs,        &
                      over_flow, rnorm, rmupdate,iparm_vs, ptvs, nngl, &
                      njavs, iavs, javs_std, avs_std, bvs, uvs_std) 
              prt_flow_sub_comp = cputime() - prt_flow_sub_comp
              
              if (b_redo_symbfac .and.                                 &
                 (itsolv > n_max_iteration_flow .or.                   &
                rnorm > r_max_residual_flow .or. over_flow)) then
                bsymbolicfactor_vs = .true. 
                b_redo_symbfac = .false.
                goto 300
              end if                
            end if
#endif 

#ifdef PETSC
            if(b_solver_test_petsc) then
             !only solver the local part, update the ghost value
              call solver_dd_snes_solve_flow_heat(ilog,.true.,         &
                        idetail_vs,avs,bvs,uvs_std,                    &
                        iavs,javs,nngl,itsolv,over_flow,               &
                        rnorm,row_idx_l2pg_vs,col_idx_l2pg_vs,.false.)
              over_flow_vs = over_flow
#ifdef DEBUG
              if(rank == 0 .and. b_enable_output) then 
               write(*,'(a, 2(1x, i5), 1x, l1, 1x, 1pe15.6e3)')        &
                   "ddfsflow-C: rank, iteration, over_flow, rnorm ",   &
                   rank, itsolv, over_flow, rnorm   
              end if 
#endif
            end if
#endif

#ifdef LIS     
            if(b_solver_test_lis) then
              !only solver the local part, update the ghost value
              call solver_lis_solve_flow(ilog,idetail_vs,avs,bvs,      &
                      uvs_std,iavs,javs,nngl,nn,1,itsolv,over_flow,    &
                      rnorm,row_idx_l2pg_vs,col_idx_l2pg_vs,.false.)
              over_flow_vs = over_flow
#ifdef DEBUG
              if(rank == 0 .and. b_enable_output) then
                write(*,'(a, 2(1x, i5), 1x, l1, 1x, 1pe15.6e3)')       &
                    "ddfsflow-C2: rank, iteration, over_flow, rnorm ", &
                    rank, itsolv, over_flow, rnorm
              end if  
#endif
            end if
#endif
           
!c  Scale [avs] and {bvs} to produce unit diagonal
!c  Generate re-ordered preconditioner [af]

            ilist = 1
            prt_flow_fac = cputime()

            call incompletefactorization (nngl,njavs,njafvs,bvs,avs,afvs,    &
                                          rwork_max,iavs,javs,iafvs,iafdvs,  &
                                          jafvs,iwork_max(ilist),lordervs,   &
                                          invordvs,numofthreads_ws209)

            prt_flow_fac = cputime() - prt_flow_fac
 
!c  solve [avs] * {uvs} = {bvs}
            prt_flow_sub = cputime()

            call ws209(ilog,nngl,msolvit_vs,itsolv,idetail_vs,iavs,javs,iafvs,&
                      iafdvs,jafvs,lordervs,avs,afvs,uvs,bvs,res_vs,rwork_max,&
                      restol_vs,deltol_vs,njavs,njafvs,over_flow,rnorm,       &
                      rmupdate,numofthreads_ws209,rank,b_enable_output)

            prt_flow_sub = cputime() - prt_flow_sub
#ifdef PARDISO
            if (b_solver_test_pardiso) then       
              call solver_results_check_output(ittot_vs, nngl, uvs, uvs_std, &
                                               "ddfsflow_vs") 
            end if  
#endif 

#ifdef PETSC 
            if (b_solver_test_petsc) then        
              call solver_results_check_output(ittot_vs, nngl, uvs,    &
                       uvs_std, "ddfsflow_vs_petsc") 
            end if          
#endif     

#ifdef LIS
            if (b_solver_test_lis) then        
              call solver_results_check_output(ittot_vs, nngl, uvs,    &
                       uvs_std, "ddfsflow_vs_lis") 
            end if          
#endif

            prt_flow_solver=prt_flow_symbfac+prt_flow_fac+prt_flow_sub
          !! use pardiso solver
          else if (i_solver_type_flow == 1) then
#ifdef PARDISO   
    !$omp parallel                                                    &
    !$omp if (njavs > numofloops_thred_global)                        &
    !$omp num_threads(numofthreads_global)                            &
    !$omp default(shared)                                             &
    !$omp private (i)  
    !$omp do schedule(static)
            do i = 1, njavs
              avs_std(i) = avs(imapvs_std(i))
            end do
    !$omp end do
    !$omp end parallel
           
            b_redo_symbfac = .true.
400         prt_flow_symbfac = cputime()
            if(bsymbolicfactor_vs  .or.                                &
               i_symfactor_type_flow == 1) then
              !write(idbg, *) "pardiso symbolic factorization for ddfsflow line 363"
              call pardiso_symbolicfactorization(iparm_vs, ptvs,       &
                          nngl, njavs, iavs, javs_std, avs_std)
              n_unknown_vs = nngl
              bsymbolicfactor_vs = .false.
            end if  
            prt_flow_symbfac = cputime() - prt_flow_symbfac
            
            !write(idbg, *) "pardiso numerical factorization for ddfsflow line 367"
            prt_flow_fac = cputime()
            call pardiso_numfactorization(iparm_vs, ptvs, nngl,        &
                        njavs, iavs, javs_std, avs_std)
            prt_flow_fac = cputime() - prt_flow_fac
            
            !write(idbg, *) "pardiso substitution for ddfsflow line 369"
            prt_flow_sub = cputime() 
            call pardiso_substitution(ilog, msolvit_vs, itsolv,        &
                 idetail_vs, res_vs, restol_vs, deltol_vs, over_flow,  &
                 rnorm, rmupdate, iparm_vs, ptvs, nngl, njavs, iavs,   &
                 javs_std, avs_std, bvs, uvs) 
            prt_flow_sub = cputime() - prt_flow_sub
            
            if (b_redo_symbfac .and.                                   &
               (itsolv > n_max_iteration_flow .or.                     &
               rnorm > r_max_residual_flow .or. over_flow)) then
              bsymbolicfactor_vs = .true.  
              b_redo_symbfac = .false.
              goto 400
            end if
#endif
            prt_flow_solver=prt_flow_symbfac+prt_flow_fac+prt_flow_sub
          !! use PETSc solver
          else if (i_solver_type_flow == 2) then
#ifdef PETSC     
            prt_flow_solver = cputime()
            !only solver the local part, update the ghost value
            call solver_dd_snes_solve_flow_heat(ilog,.true.,           &
                      idetail_vs,avs,bvs,uvs,                          &
                      iavs,javs,nngl,itsolv,over_flow,                 &
                      rnorm,row_idx_l2pg_vs,col_idx_l2pg_vs,.false.)
            over_flow_vs = over_flow
            prt_flow_solver = cputime() - prt_flow_solver
#ifdef DEBUG
            if(rank == 0 .and. b_enable_output) then
               write(*,'(a, 2(1x, i5), 1x, l1, 1x, 1pe15.6e3)')       &
                   "ddfsflow-D: rank, iteration, over_flow, rnorm ",  &
                   rank, itsolv, over_flow, rnorm
            end if  
#endif

#endif      
          !! use LIS solver  
          else if (i_solver_type_flow == 3) then
#ifdef LIS     
            prt_flow_solver = cputime()
            !only solver the local part, update the ghost value
            call solver_lis_solve_flow(ilog,idetail_vs,avs,bvs,        &
                      uvs,iavs,javs,nngl,nn,1,itsolv,over_flow,rnorm,  &
                      row_idx_l2pg_vs,col_idx_l2pg_vs,.false.)
            over_flow_vs = over_flow
            prt_flow_solver = cputime() - prt_flow_solver
#ifdef DEBUG
            if(rank == 0 .and. b_enable_output) then
              write(*,'(a, 2(1x, i5), 1x, l1, 1x, 1pe15.6e3)')         &
                    "ddfsflow-D2: rank, iteration, over_flow, rnorm ", &
                    rank, itsolv, over_flow, rnorm
            end if  
#endif
#endif     
          end if

          !Export sparse matrix dataset and right hand side. For test only, dsu.
          if((b_output_matrix.or.itimestep_output_matrix == mtime .or. &
              (itimestep_output_matrix > 0 .and. mtime == 0)).and.     &
              b_enable_output) then
            if(itype_matrix_format == 0) then
              call export_arrays1d(nngl, njavs, iavs, javs, avs,       &
                          bvs, uvs, .false., .false., .true.,          &
                          "ddfsflow_vs", ittot_vs)
            else if(itype_matrix_format == 1) then
              call export_mmformat(nngl, njavs, iavs, javs, avs,       &
                          bvs, uvs, .false., .false., .true.,          &
                          "ddfsflow_vs", ittot_vs)
#ifdef PETSC
              call export_mmformat_gbl(nn,nngl,njavs,iavs,javs,        &
                          avs,bvs,uvs,.false.,.false.,.true.,          &
                          "ddfsflow_vs",nngl,nngbl,.false.,ittot_vs)
#endif
            end if
          end if

!cdsu  debug part, use external solution (written in sequential order) to test
!         if (mtime == 29) then
!                    call export_mmformat_gbl(nn,nngl,njavs,iavs,javs, &
!                         avs,bvs,uvs,.false.,.false.,.true.,          &
!                         "ddfsflow_vs_check",nngl,nngbl,.false.,ittot_vs)
!         end if
!
!         if (rank == 0) then
!           write(*,*) "-->read hydraulic head and temperature change from external"
!         end if
!
!         ifile = lun_get()
!         write(strfile, *) ittot_vs
!         strfile = "x_ddfsflow_vs_"//trim(adjustl(strfile))//"_natgbl.txt"
!         open(ifile,file=trim(strfile),status='old',form='formatted')
!         uvs = 0.0d0
!         nskip = 0
!         read(ifile,*) strdummy
!         do ivol = 1, nngl
!#ifdef PETSC
!           do iskip = 1, node_idx_lg2g(ivol) - nskip -1
!             read(ifile,*) idummy
!           end do
!           nskip = node_idx_lg2g(ivol)
!#endif
!           read(ifile,*) idummy,uvs(ivol)
!         end do
!
!         call lun_free(ifile)
!cdsu  debug part, use external solution (written in sequential order) to test, end

          if (.not.transient_flow.and.itsolv.eq.msolvit_vs) then

            if(rank == 0 .and. b_enable_output) then          !if MPI rank 0     
              write(*,'(a/a)')                                         &
                'maximum number of inner iterations exceeded',         &
                'steady state flow solution non-convergent'
              write(ilog,'(a/a)')                                      &
                'maximum number of inner iterations exceeded',         &
                'steady state flow solution non-convergent' 
            end if                      !end if MPI rank 0

#ifdef PETSC
            call petsc_mpi_finalize
#endif
            stop
          end if

        end if ! energy balance 
!cprovi---------------------------------------------------------------------------------------------------
!cprovi End build jacobian and residual 
!cprovi---------------------------------------------------------------------------------------------------      
!c  total number of solver iterations

        itsolvtot_vs = itsolvtot_vs + itsolv  
      
        if (.not.over_flow) then
!cprovi---------------------------------------------------------------------------------------------------
!cprovi Update the solution 
!cprovi Parallelized, OpenMP, DSU
!cprovi---------------------------------------------------------------------------------------------------           
          if (heat_transport .and. decoupled_type_vs_heat <= 1) then
            call updatedd_energybalance 

            iter=iter_glob
            maxiter=maxit_glob
          else
            call updatedd

            iter=iter_vs
            maxiter=maxit_vs
          end if

!cdsu ---------------------------------------------------------------------------------------------------
!cdsu check if newton iteration is diverged based on the saved maximum update value in newton iteration
!cdsu ---------------------------------------------------------------------------------------------------
          if (b_check_div_vs .and. iter >= 5) then
            iter_div = mod(iter,5)
            if (iter_div == 0) then
              if (div_vs(5) > div_vs(4)*1.1d0 .and.                  &
                  div_vs(4) > div_vs(3)*1.1d0 .and.                  &
                  div_vs(3) > div_vs(2)*1.1d0 .and.                  &
                  div_vs(2) > div_vs(1)*1.1d0) then
                reduce_timestep = .true.
              end if
            else if (iter_div == 1) then
              if (div_vs(1) > div_vs(5)*1.1d0 .and.                  &
                  div_vs(5) > div_vs(4)*1.1d0 .and.                  &
                  div_vs(4) > div_vs(3)*1.1d0 .and.                  &
                  div_vs(3) > div_vs(2)*1.1d0) then
                reduce_timestep = .true.
              end if
            else if (iter_div == 2) then
              if (div_vs(2) > div_vs(1)*1.1d0 .and.                  &
                  div_vs(1) > div_vs(5)*1.1d0 .and.                  &
                  div_vs(5) > div_vs(4)*1.1d0 .and.                  &
                  div_vs(4) > div_vs(3)*1.1d0) then
                reduce_timestep = .true.
              end if
            else if (iter_div == 3) then
              if (div_vs(3) > div_vs(2)*1.1d0 .and.                  &
                  div_vs(2) > div_vs(1)*1.1d0 .and.                  &
                  div_vs(1) > div_vs(5)*1.1d0 .and.                  &
                  div_vs(5) > div_vs(4)*1.1d0) then
                reduce_timestep = .true.
              end if
            else if (iter_div == 4) then
              if (div_vs(4) > div_vs(3)*1.1d0 .and.                  &
                  div_vs(3) > div_vs(2)*1.1d0 .and.                  &
                  div_vs(2) > div_vs(1)*1.1d0 .and.                  &
                  div_vs(1) > div_vs(5)*1.1d0) then
                reduce_timestep = .true.
              end if
            end if
            if (reduce_timestep) then
              if (steady_flow) then
                if (rank == 0) then
                  write(ilog,*)                                      &
                  '-------------------------------------------'
                  write(ilog,*)                                      &
                  '   terminated in routine ddfsflow          '
                  write(ilog,*)                                      &
                  '   newton solver diverges                  '
                  write(ilog,*)                                      &
                  '   bye now ...                             '
                  write(ilog,*)                                      &
                  '-------------------------------------------'
                end if
#ifdef PETSC
                call petsc_mpi_finalize
#endif
                stop
              else
                if (rank == 0 .and. b_enable_output .and. idetail_vs.gt.0) then
                  write(*,*) 'reduce time step: newton iteration diverged'
                  write(ilog,*) 'reduce time step: newton iteration diverged'
                end if
              end if
            end if
          end if
            
!cprovi---------------------------------------------------------------------------------------------------           
!cprovi---------------------------------------------------------------------------------------------------                 
!cprovi---------------------------------------------------------------------------------------------------            
                        
!c  max. number of iterations is exceeded and convergence tolerance
!c  not satisfied
!c  steady state problem -> terminate execution

          if ((iter==maxiter).and.(not_converged)) then
            if (steady_flow) then
              if (rank == 0 .and. b_enable_output) then              
                write(ilog,*)                                     &
                '-------------------------------------------'
                write(ilog,*)                                     &
                '   terminated in routine ddfsflow          '
                write(ilog,*)                                     &
                '   maximum number of iterations exceeded   '
                write(ilog,*)                                     &
                '   bye now ...                             '
                write(ilog,*)                                     &
                '-------------------------------------------'
                if (b_enable_output_gen) then
                  write(igen,*)                                   &
                  '-------------------------------------------'
                  write(igen,*)                                   &
                  '   terminated in routine ddfsflow          '
                  write(igen,*)                                   &
                  '   maximum number of iterations exceeded   '
                  write(igen,*)                                   &
                  '   bye now ...                             '
                  write(igen,*)                                   &
                  '-------------------------------------------'
                end if
              end if
#ifdef PETSC
              call petsc_mpi_finalize
#endif    
              stop

!c  transient problem -> reduce time step

            elseif (transient_flow) then 
                  
              if(rank == 0 .and. b_enable_output)  then
                write(ilog,*)                                   & 
                '-------------------------------------------'
                write(ilog,*)                                   & 
                '   maximum number of iterations exceeded   '
                write(ilog,*)                                   & 
                '             reducing time step            '
                write(ilog,*)                                   & 
                '-------------------------------------------'
              end if
                
              reduce_timestep = .true.
            end if
          end if

!c  overflow occurred
!c  steady state problem -> terminate execution

        elseif (over_flow) then
          if (steady_flow) then
            if (rank == 0 .and. b_enable_output) then
              write(ilog,*)'-------------------------------------------'
              write(ilog,*)'   failure in solver - overflow occurred in ddfsflow '
              write(ilog,*)'   bye now ...                             '
              write(ilog,*)'-------------------------------------------'
              
              if (b_enable_output_gen) then
                write(igen,*)'-------------------------------------------'
                write(igen,*)'   failure in solver - overflow occurred in ddfsflow '
                write(igen,*)'   bye now ...                             '
                write(igen,*)'-------------------------------------------'
              end if
            end if

            reduce_timestep = .true.

!c  transient problem -> reduce time step

          elseif (transient_flow) then

            if(rank == 0 .and. b_enable_output)  then  
                
              write(ilog,*)'-------------------------------------------'
              write(ilog,*)'   failure in solver - overflow occurred in ddfsflow '
              write(ilog,*)'             reducing time step            '
              write(ilog,*)'-------------------------------------------'
              
            end if

            reduce_timestep = .true.
          end if
        end if                      !(over_flow)
          
        prt_flow_tot = cputime() - prt_flow_tot

!c  write runtime to file
        if(rank == 0 .and. b_prtfile) then
          if (heat_transport .and. decoupled_type_vs_heat <= 1) then
            write(iprt_flow, "(i8,1x,3(i3, 1x),i8,1x,7(1pe15.6e3,2x))")&
                  mtime, iter_sia, iter_seep, iter_glob, ittot_glob,   &
                  prt_flow_jac, prt_flow_symbfac, prt_flow_fac,        &
                  prt_flow_sub, prt_flow_solver,                       &
                  (prt_flow_tot - prt_flow_jac - prt_flow_solver),     &
                  prt_flow_tot
                
            if(b_solver_test_pardiso) then
              write(iprt_flow_comp,                                    &
                    "(i8,1x,3(i3, 1x),i8,1x,5(1pe15.6e3,2x))")         &
                    mtime, iter_sia, iter_seep, iter_glob,             &
                    ittot_glob, prt_flow_fac, prt_flow_sub,            &
                    prt_flow_symbfac_comp, prt_flow_fac_comp,          &
                    prt_flow_sub_comp
            end if            
          else
            write(iprt_flow, "(i8,1x,3(i3, 1x),i8,1x,7(1pe15.6e3,2x))")&
                  mtime, iter_sia, iter_seep, iter_vs, ittot_vs,       &
                  prt_flow_jac, prt_flow_symbfac, prt_flow_fac,        &
                  prt_flow_sub, prt_flow_solver,                       &
                  (prt_flow_tot - prt_flow_jac - prt_flow_solver),     &
                  prt_flow_tot
                
            if(b_solver_test_pardiso) then
              write(iprt_flow_comp,                                    &
                    "(i8,1x,3(i3, 1x),i8,1x,5(1pe15.6e3,2x))")         &
                    mtime, iter_sia, iter_seep, iter_vs,               &
                    ittot_vs, prt_flow_fac, prt_flow_sub,              &
                    prt_flow_symbfac_comp, prt_flow_fac_comp,          &
                    prt_flow_sub_comp
            end if
          end if
        end if
        
!c  reset primary and secondary unknowns for reduced time step
        if (reduce_timestep) then
!c  return and start over with reduced time step
          exit            
        end if
           
        !c iterative_solver_flow = .false. treated unconverged newton iteration
        !c as converged iteration. This is weird here, Why? By default, iterative solver
        !c for density dependent flow is not used. As a result, newton iteration always
        !c stops at first iteration. This problem was first reported by DSU on 2016-10-17.
        !c
        !c Converge check was enforced by DSU on 2019-08-29 by commenting the following code.
        !c This change affects all the density dependent flow problem with full saturated condition.
        !if (.not.iterative_solver_flow) then
        !  not_converged=.false.
        !  exit
        !end if
      end do          !newton iteration loop

!c  pass the iteration information back since iter_vs is used 
!c  in reporting the solver information
      iter_vs = iter
        
!   10 continue     
   
! If porosity is not updated according to flow

      if (.not. reduce_timestep) then   
        if (.not. update_porosity_flow) then
           pornew=porold
        end if
      end if

!c check if boundary nodes with influx is ponding
      if (b_water_freezing .and. b_freezing_no_pond .and.        &
          .not.reduce_timestep) then
        b_freezing_pond = .false.

        uvsmax = r0
        maxvol = i0

        do ibvs = 1, nbvs          
          ivol = iabvs(ibvs)
          if (ivol < 0) then
            cycle  
          end if
          if ((btypevs(ibvs).eq.'second' .or. btypevs(ibvs).eq.'point' .or. &
               btypevs(ibvs).eq.'seepage-second') .and. &
               uvsnew(ivol) > tol_freezing_pond(ibvs)) then
            bcondvs(ibvs) = r0    
            b_freezing_pond = .true.
            reduce_timestep = .true.

            if (uvsnew(ivol) > uvsmax) then
              uvsmax = uvsnew(ivol)
              maxvol = ivol
            end if
          end if
        end do
#ifdef PETSC
        call MPI_Allreduce(b_freezing_pond,reduce_timestep,1,    &
                 MPI_LOGICAL,MPI_LOR,Petsc_Comm_World,ierrcode)
        CHKERRQ(ierrcode)

        if (reduce_timestep) then
          mpireduce_in(1) = uvsmax      !returns the reduced value
          mpireduce_in(2) = rank        !returns the rank of process that owns it
          call MPI_Allreduce(mpireduce_in, mpireduce_out, 1,               &
                             MPI_2DOUBLE_PRECISION,MPI_MAXLOC,             &
                             Petsc_Comm_World,ierrcode)
          CHKERRQ(ierrcode)
          uvsmax = mpireduce_out(1)
          mpireduce_irank = int(mpireduce_out(2))
                
          call MPI_BCAST(maxvol, 1, MPI_INTEGER4, mpireduce_irank,         &
                         Petsc_Comm_World, ierrcode) 
          CHKERRQ(ierrcode)                
        end if              
#endif
    
        if (reduce_timestep) then
          if (rank == 0 .and. b_enable_output .and. idetail_vs.gt.0) then
#ifdef PETSC
            write(*,'(1x,a,1x,1pe11.4,1x,a,1x,i9,1x,a,1x,i6)')             &
                 'Reduce time step for boundary nodes with pond, max',     &
                  uvsmax,'Pa, vol',maxvol,'rank',mpireduce_irank
            write(ilog,'(1x,a,1x,1pe11.4,1x,a,1x,i9,1x,a,1x,i6)')          &
                 'Reduce time step for boundary nodes with pond, max',     &
                  uvsmax,'Pa, vol',maxvol,'rank',mpireduce_irank
#else
            write(*,'(1x,a,1x,1pe11.4,1x,a,1x,i9)')                        &
                 'Reduce time step for boundary nodes with pond, max',     &
                  uvsmax,'Pa, vol',maxvol
            write(ilog,'(1x,a,1x,1pe11.4,1x,a,1x,i9)')                     &
                 'Reduce time step for boundary nodes with pond, max',     &
                  uvsmax,'Pa, vol',maxvol
#endif
          end if
        end if
      end if

!c  reset primary and secondary unknowns for reduced time step

      if (reduce_timestep) then
        uvsnew = uvsold
        pornew = porold

        density = densold2
        densold = densold2
        !densold1 = densold
        tds_new = tds_old2
        tds_old = tds_old2

        if (heat_transport .and. decoupled_type_vs_heat <= 1) then
          tempnew = tempold
          if (update_viscosity_temp) then 
            viscosity = viscoold
          end if 
          if (ispitzerdens) then
            density_pitzer = densold2_pitzer
            densold_pitzer = densold2_pitzer
          end if
        end if
      end if
   
!cprov-------------------------------------------------------------------------      
!cprov Deallocate local variables
!cprov------------------------------------------------------------------------- 
      
      if (heat_transport .and. decoupled_type_vs_heat <= 1) then
        if (b_dynamic_memory) then
            call memory_monitor(-sizeof(aglob),'aglob',.true.)
            deallocate (aglob, stat = ierr)
            call checkerr(ierr,'aglob',ilog)
            
            if (i_solver_type_flow == 0) then
                call memory_monitor(-sizeof(afglob),'afglob',.true.)
                deallocate (afglob, stat = ierr)
                call checkerr(ierr,'afglob',ilog)
            end if
        end if
      else  
        if (b_dynamic_memory) then
            call memory_monitor(-sizeof(avs),'avs',.true.)
            deallocate (avs, stat = ierr)
            call checkerr(ierr,'avs',ilog) 
            
            if (i_solver_type_flow == 0) then
                call memory_monitor(-sizeof(afvs),'afvs',.true.)
                deallocate (afvs, stat = ierr)
                call checkerr(ierr,'afvs',ilog) 
            end if
        end if
      end if

!cdsu estimate time increment
      if (transient_flow .and. .not.reduce_timestep) then
        call tstepfs
      end if

      return
      end
