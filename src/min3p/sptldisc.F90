!*****Revision Informations Automatically Generated by VisualSVN*****!
!---------------------------------------------------------------------
!> $ID:$
!> $Revision: 877 $
!> $Author: dsu $
!> $Date: 2024-02-08 21:51:08 -0800 (Thu, 08 Feb 2024) $
!> $URL: https://min3psvn.ubc.ca/svn/min3p_thcm/branches/dsu_new_add_2024Jan/src/min3p/sptldisc.F90 $
!---------------------------------------------------------------------
!********************************************************************!

!c ----------------------------------------------------------------------
!c subroutine sptldisc
!c -------------------
!c
!c read spatial discretization parameters,
!c discretize solution domain and compute nodal volumes
!c
!c written by:      Uli Mayer - May 11, 96
!c
!c last modified:   Uli Mayer - November 25, 96
!c
!c                  Danyang Su - Sept. 10, 2018
!c                  Unstructured grid and HPC capabilities
!c
!c             Celine Blitz Frayret (CBF) for Frederic Gerard, December 14, 2018 :
!c            addition of toparea
!c
!c definition of variables:
!c
!c I --> on input   * arbitrary  - initialized  + entries expected
!c O --> on output  * arbitrary  - unaltered    + altered
!c 
!c                                                                    I O
!c passed:   -
!c
!c common:   
!c parm.f:   -
!c
!c gen.f:    real*8:
!c           -------
!c           xmax(nxx)          = max. coordinate of discretization   * +
!c                                interval in x-direction
!c           xmin(nxx)          = min. coordinate of discretization   * +
!c                                interval in x-direction
!c           ymax(nyy)          = max. coordinate of discretization   * +
!c                                interval in y-direction
!c           ymin(nyy)          = min. extent of solution domain      * +
!c                                in y-direction
!c           zmax(nzz)          = max. extent of solution domain      * +
!c                                in z-direction
!c           zmin(nzz)          = min. extent of solution domain      * +
!c                                in z-direction
!c           cvol(nn)           = nodal volumes                       * +
!c           dimcv(3,nn)        = dimension of control volumes        * +
!c           xg(nn)             = spatial coordinates in x-direction  * +
!c           yg(nn)             = spatial coordinates in y-direction  * +
!c           zg(nn)             = spatial coordinates in z-direction  * +
!c           tkel(nn)           = nodal temperatures in Kelvin        * +
!c           delx(nvx)          = spatial increment in x-direction    * +
!c           dely(nvy)          = spatial increment in y-direction    * +
!c           delz(nvz)          = spatial increment in z-direction    * +
!c
!c           integer*4:
!c           ----------
!c           idat               = unit number, run specific inputs    + -
!c                                             file
!c           igen               = unit number, generic output file    + -
!c           ilog               = unit number, log file               + -
!c           itmp               = unit number, temporary storage      + -
!c           nxx                = number of intervals in x-direction  * +
!c           nyy                = number of intervals in y-direction  * +
!c           nzz                = number of intervals in z-direction  * +
!c           nvix(nxx)          = number of control volumes in        * +
!c                                x-direction (intervals)
!c           nviy(nyy)          = number of control volumes in        * +
!c                                y-direction (intervals)
!c           nviz(nzz)          = number of control volumes in        * +
!c                                z-direction (intervals)
!c           nvx                = number of control volumes in        * +
!c                                x-direction
!c           nvy                = number of control volumes in        * +
!c                                y-direction
!c           nvz                = number of control volumes in        * +
!c                                z-direction
!c           nn                 = total number of control volumes     * +
!c
!c           logical:
!c           --------
!c           half_cells         = .true.  -> half cells on boundary   * +
!c
!c           character:
!c           ----------
!c           section_header     = section header                      * *
!c
!c local:    integer*4:
!c           ----------
!c           ierr               = 0 -> memory allocation successful
!c           ixx                = counter (intervals in x-direction)
!c           iyy                = counter (intervals in y-direction)
!c           izz                = counter (intervals in z-direction)
!c           l_string           = length of text string
!c
!c           real*8:
!c           -------
!c           xtot               = total dimension in x-direction  CBF 
!c           ytot               = total dimension in y-direction  CBF 
!c
!c           logical:
!c           --------
!c           found_section      = .true.  -> section header was
!c                                           found in input file
!c
!c external: checkerr  = check for error during memory allocation 
!c           cvolume   = compute volumes on a nodal basis
!c           readbloc  = read section of input file and write to
!c                       temporary file
!c           xyzcoord  = compute spatial increments for x,y and 
!c                       z-direction and calculate xyz-coordinates 
!c                       of control volume centroids
!c---------------------------------------------------------------------- 
!c WARNING
!c---------------------------------------------------------------------- 
!c All allocated variables are initialized to some value. This depends  
!c of its type:
!c 
!c real*8      => 0.0d0
!c integer     => 0 
!c character   => ' '    
!c logical     => .false. 
!c 
!c Sergio Andres Bea Jofre (2009)
!c
!c ----------------------------------------------------------------------
 
      subroutine sptldisc

#ifdef PETSC
#include <petscversion.h>
#if (PETSC_VERSION_MAJOR >= 3 && PETSC_VERSION_MINOR >= 8)
#include <petsc/finclude/petscsys.h>
      use petscsys
#endif
#endif

      use parm
      use gen
      use phys, only : is_cell_based_perm_cond,                        &
                       is_cell_based_relp,                             &
                       is_cell_based_disp_heat,                        &
                       is_cell_based_disp_rt
      use file_unit, only : lun_get, lun_free
#ifdef PETSC
      use solver_dd, only : solver_dd_DMDACreate_flow_heat,            &
                            solver_dd_create_dmplex,                   &
                            solver_dd_mapping_set_dmplex
      use solver_snes_common, only: dmda_flow
      use petsc_mpi_common, only : petsc_mpi_finalize, petsc_mpi_barrier
#endif      

#ifdef USG
      use usg_mesh_generation
      use geometry
      use usg_mesh_data, only : num_nodes, num_cells, nodes, cells,    &
#ifdef PETSC
                                nodes_gbl, cells_gbl,                  &
                                num_nodes_gbl, num_cells_gbl,          &
                                num_nodes_loc, num_cells_loc,          &
                                num_edge_maxcells,                     &
#endif
                                cvol_method, usg_mesh_data_input_vtk,  &
                                write_mesh_data_vtk_ascii,             &
                                cal_cell_node_inverse_dist,            &
                                usg_mesh_data_input_gid_mesh,          &
                                usg_mesh_data_export_pflotran,         &
                                usg_mesh_data_build, cell_type,        &
                                b_renumber_node_id, cell_projection,   &
                                b_use_node_matids, b_use_cell_matids,  &
                                b_error_flag, num_nodes_per_cell,      &
                                num_node_layers, num_nodes_per_layer,  &
                                num_cell_layers, num_cells_per_layer,  &                                
                                is_boundary_node_gbl,                  &
                                is_boundary_cell_gbl,                  &
                                layer_nodes_top, layer_nodes_bottom,   &
                                allow_obtuse_cells, grad_method,       &
                                grad_spatial_weighting,                &
                                grad_average_weighting,                &
                                grad_spatial_factor,                   &
                                b_cell_based_grad_itpl,                &
                                b_mesh_output_scale, b_anchor_coord,   &
                                anchor_coord_old, anchor_coord_new,    &
                                mesh_output_scale,                     &
                                usg_mesh_data_reorder,                 &
                                b_reverse_cell_node,                   &
                                b_reorder_cell_node,                   &
                                b_export_mesh_pflotran,                &
                                b_use_face_based_flux
#endif

      implicit none
#ifdef PETSC
#if (PETSC_VERSION_MAJOR >= 3 && PETSC_VERSION_MINOR >= 6 && PETSC_VERSION_MINOR < 8)
#include <petsc/finclude/petscsys.h>
#elif (PETSC_VERSION_MAJOR >= 3 && PETSC_VERSION_MINOR < 6)
#include <finclude/petscsys.h>
#endif
#endif

#ifdef PETSC
      PetscErrorCode :: ierrcode
#endif

      external checkerr, cvolume, readbloc, xyzcoord

#ifdef USG
      integer :: bandwidth1, bandwidth2
      external :: iajavs_usg, rcmordering
      integer :: adj_bandwidth, adj_rcm_bandwidth
      external :: adj_bandwidth, adj_rcm_bandwidth
#endif

      logical found_section,found_subsection,bflag

      character*72 subsection, strbuffer
      character*256 :: str_usg_generate_file
      
      integer :: i, j, ierr, ixx, iyy, izz, ivtk, l_string, ierrcd
      integer :: nvx_inter, nvy_inter, nvz_inter
      real*8 :: xtot, ytot ! CBF

      ierrcd = 0

!c  choose spatial discretization type, the default is strucutre grid
      discretization_type = 0
      is_cell_based_perm_cond = .false.
      is_cell_based_relp = .false.
      is_cell_based_disp_heat = .false.
      is_cell_based_disp_rt = .false.
      usg_mesh_ordering = 0

!c  flag to indicate the hdf5 global to natural ordering is not set
      b_init_hdf5_lg2g = .false.

!c  radial coordinates option
      radial_coord = .false.  

!c  read spatial discretization parameters and write to temporary file   
   
      section_header = 'spatial discretization'
      call readbloc (idat,itmp,section_header,found_section,.true.)

!c  define length of section header

      l_string = index(section_header,'  ')-1
      if (l_string.eq.-1.or.l_string.gt.72) then
         l_string=72
      end if

!c  terminate program if section header not found

      if (.not.found_section) then
        if (rank == 0) then  
          write(ilog,*) 'error reading input file'
          write(ilog,*) 'section "',section_header(:l_string),'" missing'
          close(ilog)
        end if
#ifdef PETSC
        call petsc_mpi_finalize
#endif
        stop
      end if

        !c  optional use of radial coordinates
        subsection = 'radial coordinates'
        call findstrg(subsection,itmp,found_subsection)
        if (found_subsection) then
          radial_coord = .true.
        end if

        half_cells = .true.
        subsection = 'full cells'
        call findstrg(subsection,itmp,found_subsection)
        if (found_subsection) then
          half_cells = .false.
        end if

#ifdef USG
      !c generate unstructured grid from structured grid discretization parameters
      subsection = 'generate unstructured grid from structured parameters'
      call findstrg(subsection,itmp,found_subsection)
      if (found_subsection) then
        discretization_type = 1
        goto 100
      end if

      subsection = 'generate unstructured grid from structured boundary'
      call findstrg(subsection,itmp,found_subsection)
      if (found_subsection) then
        discretization_type = 2
        goto 100
      end if

      subsection = 'generate unstructured grid from file'
      call findstrg(subsection,itmp,found_subsection)
      if (found_subsection) then
        discretization_type = 3
        ierrcd = 1
        !read(itmp,*,err=999,end=999) str_usg_generate_file
        goto 100
      end if

      subsection = 'generate unstructured grid from scalar points'
      call findstrg(subsection,itmp,found_subsection)
      if (found_subsection) then
        discretization_type = 4
        goto 100
      end if

      subsection = 'use unstructured grid method'
      call findstrg(subsection,itmp,found_subsection)
      if (found_subsection) then
        discretization_type = 5
        goto 100
      end if

      subsection = 'read unstructured grid from file'
      call findstrg(subsection,itmp,found_subsection)
      if (found_subsection) then
        discretization_type = 10
        goto 100
      end if

      subsection = 'read unstructured grid from vtk mesh file'
      call findstrg(subsection,itmp,found_subsection)
      if (found_subsection) then
        discretization_type = 10
        goto 100
      end if

      subsection = 'read unstructured grid from gid mesh file'
      call findstrg(subsection,itmp,found_subsection)
      if (found_subsection) then
        discretization_type = 11
        goto 100
      end if

      rewind(itmp)
#endif

100   continue

#ifdef USG
      if (discretization_type > 0) then

        ncon_usg_est = 60
        subsection = 'maximum number of node connection'
        call findstrg(subsection,itmp,found_subsection)
        if (found_subsection) then
          ierrcd = 2
          read(itmp,*,err=999,end=999) ncon_usg_est
        end if

        subsection = 'use legacy vtk format'
        call findstrg(subsection,itmp,found_subsection)
        if (found_subsection) then
          b_legacy_vtk = .true.
        else
          b_legacy_vtk = .false.
        end if

        subsection = 'renumber node from file'
        call findstrg(subsection,itmp,found_subsection)
        if (found_subsection) then
          b_renumber_node_id = .true.
        else
          b_renumber_node_id = .false.
        end if

        subsection = 'reverse cell-node ordering'
        call findstrg(subsection,itmp,found_subsection)
        if (found_subsection) then
          b_reverse_cell_node = .true.
        else
          b_reverse_cell_node = .false.
        end if

        b_reorder_cell_node = .true.
        subsection = 'disable automatic cell-node ordering'
        call findstrg(subsection,itmp,found_subsection)
        if (found_subsection) then
          b_reorder_cell_node = .false.
        end if


        usg_mesh_ordering = 0
        subsection = 'rcm mesh ordering'
        call findstrg(subsection,itmp,found_subsection)
        if (found_subsection) then
          !rcm ordering = ibset(usg_mesh_ordering,0) + ibset(usg_mesh_ordering,2)
          usg_mesh_ordering = 5
        end if

        subsection = 'export mesh to pflotran mesh format'
        call findstrg(subsection,itmp,found_subsection)
        if (found_subsection) then
          b_export_mesh_pflotran = .true.
        else
          b_export_mesh_pflotran = .false.
        end if

        subsection = 'control volume method'
        call findstrg(subsection,itmp,found_subsection)
        if (found_subsection) then
          ierrcd = 3
          read(itmp,*,err=999,end=999) subsection
          if (trim(subsection) == 'median dual') then
            cvol_method = cvol_method_md
            if (b_enable_output .and. b_enable_output_gen) then
              write(igen,'(a)') "Control volume method: median dual"
            end if
          else if (trim(subsection) == 'voronoi diagram' .or. &
                   trim(subsection) == 'voronoi dual' .or. &
                   trim(subsection) == 'containment dual') then
            cvol_method = cvol_method_vd
            if (b_enable_output .and. b_enable_output_gen) then
              write(igen,'(2a)') "Control volume method: ",&
                    "voronoi diagram/voronoi dual/containment dual"
            end if
          else if (trim(subsection) == 'cell center' .or. &
                   trim(subsection) == 'center dual') then
            cvol_method = cvol_method_cc
            if (b_enable_output .and. b_enable_output_gen) then
              write(igen,'(a)') "Control volume method: cell center/center dual"
            end if
          else
            if (rank == 0) then
              write(*,'(a)') "Error: Unknown control volume method"
              write(*,'(a)') "This value should be median dual or voronoi dual"
              write(ilog,'(a)') "Error: Unknown control volume method"
              write(ilog,'(a)') "This value should be median dual or voronoi dual"
            end if
            ierrcd = 4
            goto 999
          end if
        else
          cvol_method = cvol_method_md
          if (b_enable_output .and. b_enable_output_gen) then
            write(igen,'(a)') "Control volume method: median dual (default)"
          end if
        end if
        
        if (cvol_method == cvol_method_vd) then
          subsection = 'allow obtuse cells'
          call findstrg(subsection,itmp,found_subsection)
          if (found_subsection) then
            allow_obtuse_cells = .true.
          else
            allow_obtuse_cells = .false.  
          end if
        end if

        subsection = 'gradient reconstruction method'
        call findstrg(subsection,itmp,found_subsection)
        if (found_subsection) then
          ierrcd = 5
          read(itmp,*,err=999,end=999) subsection
          if (trim(subsection) == 'cell based green gauss' .or.        &
              trim(subsection) == 'cell based gauss green') then
            grad_method = grad_method_cgg
            if (b_enable_output .and. b_enable_output_gen) then
              write(igen,'(a)') "Gradient reconstruction method: cell based Green-Gauss"
            end if
          else if (trim(subsection) == 'green gauss' .or.              &
              trim(subsection) == 'gauss green') then
            grad_method = grad_method_gg
            if (b_enable_output .and. b_enable_output_gen) then
              write(igen,'(a)') "Gradient reconstruction method: Green-Gauss"
            end if
          else if (trim(subsection) == 'least square' .or.             &
                   trim(subsection) == 'least square level 1') then
            grad_method = grad_method_ls
            if (b_enable_output .and. b_enable_output_gen) then
              write(igen,'(2a)') "Gradient reconstruction method: ",trim(subsection)
            end if
          else if (trim(subsection) == 'least square second order') then
            grad_method = grad_method_ls2
            if (b_enable_output .and. b_enable_output_gen) then
              write(igen,'(2a)') "Gradient reconstruction method: ",trim(subsection)
            end if
          else if (trim(subsection) == 'least square third order') then
            grad_method = grad_method_ls3
            if (b_enable_output .and. b_enable_output_gen) then
              write(igen,'(2a)') "Gradient reconstruction method: ",trim(subsection)
            end if
          else if (trim(subsection) == 'least square fourth order') then
            grad_method = grad_method_ls4
            if (b_enable_output .and. b_enable_output_gen) then
              write(igen,'(2a)') "Gradient reconstruction method: ",trim(subsection)
            end if
          else
            if (rank == 0) then
              write(*,'(a)') "Error: Unknown gradient reconstruction method"
              write(*,'(a)') "This value should be Green Gauss or least square [second/third/fourth order]"
              write(ilog,'(a)') "Error: Unknown gradient reconstruction method"
              write(ilog,'(a)') "This value should be Green Gauss or least square [second/third/fourth order]"
            end if
            ierrcd = 6
            goto 999
          end if

          subsection = 'gradient reconstruction spatial weighting'
          call findstrg(subsection,itmp,found_subsection)
          if (found_subsection) then
            grad_spatial_weighting = .true.
          else
            grad_spatial_weighting = .false.
          end if

          subsection = 'gradient average spatial weighting'
          call findstrg(subsection,itmp,found_subsection)
          if (found_subsection) then
            ierrcd = 7
            read(itmp,*,err=999,end=999) strbuffer
            if (trim(strbuffer) == 'centered') then
              grad_average_weighting = 0
            else if (trim(strbuffer) == 'harmonic') then
              grad_average_weighting = 1
            else if (trim(strbuffer) == 'minimum') then
              grad_average_weighting = 2
            else
              grad_average_weighting = 0
            end if
          else
            grad_average_weighting = 0
          end if

          subsection = 'inverse distance weighting factor'
          call findstrg(subsection,itmp,found_subsection)
          if (found_subsection) then
            ierrcd = 8
            read(itmp,*,err=999,end=999) grad_spatial_factor
          else
            grad_spatial_factor = 1.0
          end if

        else
          grad_method = grad_method_ls2
          if (b_enable_output .and. b_enable_output_gen) then
            write(igen,'(2a)') "Default gradient reconstruction method",&
                               ": least square second order"
          end if
        end if

        b_cell_based_grad_itpl = .true.
        subsection = 'disable cell based gradient interpolation'
        call findstrg(subsection,itmp,found_subsection)
        if (found_subsection) then
          b_cell_based_grad_itpl = .false.
        end if

        !c use two-point flux approximation when use structured mesh;
        !c use multi-point flux approximation when use unstructured mesh;

        !c by default, include cross diffusion term flow problem, 
        !c include cross diffusion term in advection and 
        !c exlcude it in diffusion term for heat and reactive transport.
        !c 
        b_use_cross_diffusion_flow = .true.
        b_use_cross_diffusion_heat = .true.
        b_use_cross_diffusion_react = .true.

        subsection = 'multi-point flux approximation'
        call findstrg(subsection,itmp,found_subsection)
        if (found_subsection) then
          b_use_cross_diffusion_flow = .true.
          b_use_cross_diffusion_heat = .true.
          b_use_cross_diffusion_react = .true.
        else
          subsection = 'include cross diffusion term'
          call findstrg(subsection,itmp,found_subsection)
          if (found_subsection) then
            b_use_cross_diffusion_flow = .true.
            b_use_cross_diffusion_heat = .true.
            b_use_cross_diffusion_react = .true.
          end if
        end if

        subsection = 'two-point flux approximation'
        call findstrg(subsection,itmp,found_subsection)
        if (found_subsection) then
          b_use_cross_diffusion_flow = .false.
          b_use_cross_diffusion_heat = .false.
          b_use_cross_diffusion_react = .false.
        else
          subsection = 'exclude cross diffusion term'
          call findstrg(subsection,itmp,found_subsection)
          if (found_subsection) then
            b_use_cross_diffusion_flow = .false.
            b_use_cross_diffusion_heat = .false.
            b_use_cross_diffusion_react = .false.
          end if
        end if

        subsection = 'multi-point flux approximation for flow'
        call findstrg(subsection,itmp,found_subsection)
        if (found_subsection) then
          b_use_cross_diffusion_flow = .true.
        else
          subsection = 'include cross diffusion term for flow'
          call findstrg(subsection,itmp,found_subsection)
          if (found_subsection) then
            b_use_cross_diffusion_flow = .true.
          end if
        end if

        subsection = 'two-point flux approximation for flow'
        call findstrg(subsection,itmp,found_subsection)
        if (found_subsection) then
          b_use_cross_diffusion_flow = .false.
        else
          subsection = 'exclude cross diffusion term for flow'
          call findstrg(subsection,itmp,found_subsection)
          if (found_subsection) then
            b_use_cross_diffusion_flow = .false.
          end if
        end if

        subsection = 'multi-point flux approximation for heat transport'
        call findstrg(subsection,itmp,found_subsection)
        if (found_subsection) then
          b_use_cross_diffusion_heat = .true.
        else
          subsection = 'include cross diffusion term for heat transport'
          call findstrg(subsection,itmp,found_subsection)
          if (found_subsection) then
            b_use_cross_diffusion_heat = .true.
          end if
        end if

        subsection = 'two-point flux approximation for heat transport'
        call findstrg(subsection,itmp,found_subsection)
        if (found_subsection) then
          b_use_cross_diffusion_heat = .false.
        else
          subsection = 'exclude cross diffusion term for heat transport'
          call findstrg(subsection,itmp,found_subsection)
          if (found_subsection) then
            b_use_cross_diffusion_heat = .false.
          end if
        end if

        subsection = 'multi-point flux approximation for reactive transport'
        call findstrg(subsection,itmp,found_subsection)
        if (found_subsection) then
          b_use_cross_diffusion_react = .true.
        else
          subsection = 'include cross diffusion term for reactive transport'
          call findstrg(subsection,itmp,found_subsection)
          if (found_subsection) then
            b_use_cross_diffusion_react = .true.
          end if
        end if

        subsection = 'two-point flux approximation for reactive transport'
        call findstrg(subsection,itmp,found_subsection)
        if (found_subsection) then
          b_use_cross_diffusion_react = .false.
        else
          subsection = 'exclude cross diffusion term for reactive transport'
          call findstrg(subsection,itmp,found_subsection)
          if (found_subsection) then
            b_use_cross_diffusion_react = .false.
          end if
        end if

        b_use_hls_correction = .false.
        subsection = 'include high order flux correction term'
        call findstrg(subsection,itmp,found_subsection)
        if (found_subsection) then
          if (grad_method >= grad_method_ls2) then
            b_use_hls_correction = .true.
          end if
        end if

        subsection = 'exclude high order flux correction term'
        call findstrg(subsection,itmp,found_subsection)
        if (found_subsection) then
          b_use_hls_correction = .false.
        end if

        !c define edge-based or face-based flux evaluation
        b_use_face_based_flux = .false.
        subsection = 'face-based flux approximation'
        call findstrg(subsection,itmp,found_subsection)
        if (found_subsection) then
          b_use_face_based_flux = .true.
        end if

        if (.not. found_subsection) then
          subsection = 'use separated interface centers'
          call findstrg(subsection,itmp,found_subsection)
          if (found_subsection) then
            b_use_face_based_flux = .true.
          end if
        end if

        if (.not. found_subsection) then
          subsection = 'use control volume half face center'
          call findstrg(subsection,itmp,found_subsection)
          if (found_subsection) then
            b_use_face_based_flux = .true.
          end if
        end if

        b_grad_interpolate_cell = .false.
        subsection = 'gradient interpolation based on neighbour cells'
        call findstrg(subsection,itmp,found_subsection)
        if (found_subsection) then
          b_grad_interpolate_cell = .true.
        end if

        tol_vc_vel_ave = 0.5d0
        subsection = 'variation coefficient for velocity average'
        call findstrg(subsection,itmp,found_subsection)
        if (found_subsection) then
          ierrcd = 9
          read(itmp,*,err=999,end=999) tol_vc_vel_ave
        end if

!c  check if Perot's method or avreaged method is used for velocity reconstruction during output.
!c  Before revision 844, averaged method, which is estimated based averaged gradient and conductivity, 
!c  is used for velocity reconstruction.After revision 845, Perot's method is set to default since 
!c  this method works better for heterogeneous problem. 

!c  Revision before 844 requires keyword 'use perot velocity reconstruction' for Perot's method.
!c  To use the averaged velocity reconstruction method, please specify keyword 
!c  'use averaged velocity reconstruction'.

        b_use_perot_flow_vel = .true.

        !cdsu deprecated
        !subsection = 'use averaged velocity reconstruction'
        !call findstrg(subsection,itmp,found_subsection)
        !if (found_subsection) then
        !  b_use_perot_flow_vel = .false.
        !end if

!c  to be checked in the future, currently deactivate.
!c  check if velocity output is cell based
!c  The velocity is calculated based on darcy flux using
!c  the gradient at the cell center based on nodal values 
!c  belong to the cell. However, it seems not accurate enough.
        b_use_cell_vel = .false.

        !subsection = 'cell based velocity output'
        !call findstrg(subsection,itmp,found_subsection)
        !if (found_subsection) then
        !  b_use_cell_vel = .true.
        !end if

!c  check if permeability is node based or cell based
        subsection = 'cell based hydraulic conductivity'
        call findstrg(subsection,itmp,found_subsection)
        if (found_subsection) then
          is_cell_based_perm_cond = .true.
        else
          subsection = 'cell based permeability'
          call findstrg(subsection,itmp,found_subsection)
          if (found_subsection) then
            is_cell_based_perm_cond = .true.
          end if
        end if

        !c likely to cause convergence problem when use cell based
        !c relative permeability, DSU, 2018-09-06
        subsection = 'cell based relative permeability'
        call findstrg(subsection,itmp,found_subsection)
        if (found_subsection) then
          is_cell_based_relp = .true.
        end if

!c  check if dispersivity is node based or cell based
        subsection = 'cell based dispersivity'
        call findstrg(subsection,itmp,found_subsection)
        if (found_subsection) then
          is_cell_based_disp_rt = .true.
          is_cell_based_disp_heat = .true.
        end if

!c  check if dispersivity is node based or cell based
        subsection = 'cell based dispersivity for heat transport'
        call findstrg(subsection,itmp,found_subsection)
        if (found_subsection) then
          is_cell_based_disp_heat = .true.
        end if

!c  check if dispersivity is node based or cell based
        subsection = 'cell based dispersivity for reactive transport'
        call findstrg(subsection,itmp,found_subsection)
        if (found_subsection) then
          is_cell_based_disp_rt = .true.
        end if

!c  polygon selection buffer
        subsection = 'polygon zone selection buffer'
        zone_buffer = 1.0d-4
        call findstrg(subsection,itmp,found_subsection)
        if (found_subsection) then
          ierrcd = 10
          read(itmp,*,err=999,end=999) zone_buffer
        end if

!c  check if layered nodes/cells are used to help setting initial conditions
        b_use_layered_mesh = .false.
        subsection = 'use layered mesh structure'
        call findstrg(subsection,itmp,found_subsection)
        if (found_subsection) then
          b_use_layered_mesh = .true.
        end if

!c  check if number of node layers is specified and type of method
!c  available method: 'material id', 'coordinate' and 'user specified'
        ctype_num_node_layers = 'material id'
        num_node_layers_inp = 0
        if (b_use_layered_mesh) then
          subsection = 'method to calculate number of node layers'
          call findstrg(subsection,itmp,found_subsection)
          if (found_subsection) then
            ierrcd = 11
            read(itmp,*,err=999,end=999) ctype_num_node_layers
            if (trim(ctype_num_node_layers) == 'user specified') then
              backspace(itmp)
              ierrcd = 12
              read(itmp,*,err=999,end=999) ctype_num_node_layers,      &
                                           num_node_layers_inp
              if (num_node_layers_inp < 1) then
                if (rank == 0) then  
                  write(*,*) 'error in specifying number of node layers'
                  write(ilog,*) 'error in specifying number of node layers'
                  close(ilog)
                end if
#ifdef PETSC
                call petsc_mpi_finalize
#endif
                stop
              end if
            end if
          end if
        end if

!c  check scale factor for mesh transforming during output
        subsection = 'scale factor for mesh output'
        call findstrg(subsection,itmp,found_subsection)
        if (found_subsection) then
          b_mesh_output_scale = .true.
          ierrcd = 13
          read(itmp,*,err=999,end=999) mesh_output_scale%x,            &
                                       mesh_output_scale%y,            &
                                       mesh_output_scale%z
        else
          b_mesh_output_scale = .false.
          mesh_output_scale%x = 1.0d0
          mesh_output_scale%y = 1.0d0
          mesh_output_scale%z = 1.0d0
        end if

!c  check if 2D plane in 3D space mapping is specified
        subsection = 'pairs of anchor point coordinates'
        call findstrg(subsection,itmp,found_subsection)
        if (found_subsection) then
          b_anchor_coord = .true.
          ierrcd = 14
          read(itmp,*,err=999,end=999) anchor_coord_old(1)%x,            &
                                       anchor_coord_old(1)%y,            &
                                       anchor_coord_old(1)%z
          read(itmp,*,err=999,end=999) anchor_coord_old(2)%x,            &
                                       anchor_coord_old(2)%y,            &
                                       anchor_coord_old(2)%z 
          read(itmp,*,err=999,end=999) anchor_coord_new(1)%x,            &
                                       anchor_coord_new(1)%y,            &
                                       anchor_coord_new(1)%z
          read(itmp,*,err=999,end=999) anchor_coord_new(2)%x,            &
                                       anchor_coord_new(2)%y,            &
                                       anchor_coord_new(2)%z                             
        else
          b_anchor_coord = .false.
        end if

        !c for the discretization type 1 or 2, additional header
        !c "structured spatial discretization" is needed before the discretization data
        if (discretization_type == 1 .or. discretization_type == 2 .or.&
            discretization_type == 5) then
          rewind(itmp)
          subsection = 'structured spatial discretization'
          call findstrg(subsection,itmp,found_subsection)
          if (.not. found_subsection) then
            if (rank == 0) then
              write(*,'(a)') "Error: 'structured spatial discretization' is missing"
              write(ilog,'(a)') "Error: 'structured spatial discretization' is missing"
            end if
            ierrcd = 15
            goto 999
          end if
        end if
      end if
#endif

      if (discretization_type == 0) then

!c  read spatial discretization parameters and allocate memory

!c  x-direction

        nvxgbl = 0
        ierrcd = 16
        read(itmp,*,err=999,end=999) nxx

        allocate (xmax(nxx), stat = ierr)
        xmax=0.0d0
        call checkerr(ierr,'xmax',ilog)
        call memory_monitor(sizeof(xmax),'xmax',.true.)

        allocate (xmin(nxx), stat = ierr)
        xmin=0.0d0
        call checkerr(ierr,'xmin',ilog)
        call memory_monitor(sizeof(xmin),'xmin',.true.)

        allocate (nvix(nxx), stat = ierr)
        nvix=0
        call checkerr(ierr,'nvix',ilog)
        call memory_monitor(sizeof(nvix),'nvix',.true.)

        do ixx = 1,nxx
          ierrcd = 17
          read(itmp,*,err=999,end=999) nvix(ixx)
          read(itmp,*,err=999,end=999) xmin(ixx), xmax(ixx)
          nvxgbl = nvxgbl+nvix(ixx)
          xtot=xmax(ixx)-xmin(ixx) ! CBF
        end do

        allocate (delx(nvxgbl), stat = ierr)
        delx=0.0d0
        call checkerr(ierr,'delx',ilog)
        call memory_monitor(sizeof(delx),'delx',.true.)

        allocate (xglat(nvxgbl), stat = ierr)
        xglat=0.0d0
        call checkerr(ierr,'xglat',ilog)
        call memory_monitor(sizeof(xglat),'xglat',.true.)

!c  y-direction

        nvygbl = 0
        read(itmp,*) nyy

        allocate (ymax(nyy), stat = ierr)
        ymax=0.0d0
        call checkerr(ierr,'ymax',ilog)
        call memory_monitor(sizeof(ymax),'ymax',.true.)

        allocate (ymin(nyy), stat = ierr)
        ymin=0.0d0
        call checkerr(ierr,'ymin',ilog)
        call memory_monitor(sizeof(ymin),'ymin',.true.)

        allocate (nviy(nyy), stat = ierr)
        nviy=0
        call checkerr(ierr,'nviy',ilog)
        call memory_monitor(sizeof(nviy),'nviy',.true.)

        do iyy = 1,nyy
          ierrcd = 18
          read(itmp,*,err=999,end=999) nviy(iyy)
          read(itmp,*,err=999,end=999) ymin(iyy), ymax(iyy)
          nvygbl = nvygbl+nviy(iyy)
          ytot=ymax(iyy)-ymin(iyy) ! CBF
        end do

        allocate (dely(nvygbl), stat = ierr)
        dely=0.0d0
        call checkerr(ierr,'dely',ilog)
        call memory_monitor(sizeof(dely),'dely',.true.)

        allocate (yglat(nvygbl), stat = ierr)
        yglat=0.0d0
        call checkerr(ierr,'yglat',ilog)
        call memory_monitor(sizeof(yglat),'yglat',.true.)

!c  z-direction

        nvzgbl = 0
        ierrcd = 19
        read(itmp,*,err=999,end=999) nzz

        allocate (zmax(nzz), stat = ierr)
        zmax=0.0d0
        call checkerr(ierr,'zmax',ilog)
        call memory_monitor(sizeof(zmax),'zmax',.true.)

        allocate (zmin(nzz), stat = ierr)
        zmin=0.0d0
        call checkerr(ierr,'zmin',ilog)
        call memory_monitor(sizeof(zmin),'zmin',.true.)

        allocate (nviz(nzz), stat = ierr)
        nviz=0
        call checkerr(ierr,'nviz',ilog)
        call memory_monitor(sizeof(nviz),'nviz',.true.)

        do izz = 1,nzz
          ierrcd = 20
          read(itmp,*,err=999,end=999) nviz(izz)
          read(itmp,*,err=999,end=999) zmin(izz), zmax(izz)
          nvzgbl = nvzgbl+nviz(izz)
        end do

        allocate (delz(nvzgbl), stat = ierr)
        delz=0.0d0
        call checkerr(ierr,'delz',ilog)
        call memory_monitor(sizeof(delz),'delz',.true.)

        allocate (zglat(nvzgbl), stat = ierr)
        zglat=0.0d0
        call checkerr(ierr,'zglat',ilog)
        call memory_monitor(sizeof(zglat),'zglat',.true.)

!c  calculate total number of control volumes
        nngbl = nvxgbl*nvygbl*nvzgbl

!c  calculate profil area exposed to atmosphere - CBF 

        toparea = xtot*ytot
    

        if (nvxgbl > 1 .and. nvygbl > 1 .and. nvzgbl > 1) then
          cell_projection = projection_xyz
          ndim_sys = 3
        else if (nvxgbl > 1 .and. nvygbl > 1) then
          cell_projection = projection_xy
          ndim_sys = 2
        else if (nvygbl > 1 .and. nvzgbl > 1) then
          cell_projection = projection_yz
          ndim_sys = 2
        else if (nvxgbl > 1 .and. nvzgbl > 1) then
          cell_projection = projection_xz
          ndim_sys = 2
        else if (nvxgbl > 1) then
          cell_projection = projection_x
          ndim_sys = 1
        else if (nvygbl > 1) then
          cell_projection = projection_y
          ndim_sys = 1
        else if (nvzgbl > 1) then
          cell_projection = projection_z 
          ndim_sys = 1 
        end if

#ifdef USG
        num_nodes = nngbl
#endif

#ifdef PETSC
        if (nngbl < nprcs*2) then
          if (rank == 0) then
            write(ilog,'(a)')                                          &
                'Error: Required minimum subdomain discretization is 2.'
            write(ilog,'(a)')                                          &
                '       Increase discretization or use less processors.'
            write(*,'(a)')                                             &
                'Error: Required minimum subdomain discretization is 2.'
            write(*,'(a)')                                             &
                '       Increase discretization or use less processors.'
          end if
          call petsc_mpi_finalize
          stop
        end if
#endif
      
!c  domain decomposition
#ifdef PETSC 
        if (varsat_flow) then
          call solver_dd_DMDACreate_flow_heat(.true.)
        end if

        if (heat_transport .and. decoupled_type_vs_heat > 1) then
          call solver_dd_DMDACreate_flow_heat(.false.)
        end if

        nvx = dmda_flow%xl
        nvy = dmda_flow%yl
        nvz = dmda_flow%zl
        nvxgl = dmda_flow%gxl
        nvygl = dmda_flow%gyl
        nvzgl = dmda_flow%gzl
        nvxls = dmda_flow%xs
        nvxle = dmda_flow%xe
        nvyls = dmda_flow%ys
        nvyle = dmda_flow%ye
        nvzls = dmda_flow%zs
        nvzle = dmda_flow%ze
        nvxgls = dmda_flow%gxs
        nvxgle = dmda_flow%gxe
        nvygls = dmda_flow%gys
        nvygle = dmda_flow%gye
        nvzgls = dmda_flow%gzs
        nvzgle = dmda_flow%gze
#else
        nvx = nvxgbl
        nvy = nvygbl
        nvz = nvzgbl
        nvxgl = nvxgbl
        nvygl = nvygbl
        nvzgl = nvzgbl
        nvxls = 1
        nvxle = nvxgbl
        nvyls = 1
        nvyle = nvygbl
        nvzls = 1
        nvzle = nvzgbl
        nvxgls = 1
        nvxgle = nvxgbl
        nvygls = 1
        nvygle = nvygbl
        nvzgls = 1
        nvzgle = nvzgbl
#endif

        nn = nvx*nvy*nvz
        nngl = nvxgl*nvygl*nvzgl

        if(nvx > 1) then
          nvx_inter = nvx-1+nvxls-nvxgls
        else
          nvx_inter = 1
        end if

        if(nvy > 1) then
          nvy_inter = nvy-1+nvyls-nvygls
        else
          nvy_inter = 1
        end if

        if(nvz > 1) then
          nvz_inter = nvz-1+nvzls-nvzgls
        else
          nvz_inter = 1
        end if

        nn_interfacial = nvx_inter*nvy_inter*nvz_inter
      
      
#ifdef PETSC
        allocate(nn_ranks(nprcs), stat = ierr)
        call checkerr(ierr,'nn_ranks',ilog)
        nn_ranks = 0
        call memory_monitor(sizeof(nn_ranks),'nn_ranks',.true.)

        allocate(nngl_ranks(nprcs), stat = ierr)
        call checkerr(ierr,'nngl_ranks',ilog)
        nngl_ranks = 0
        call memory_monitor(sizeof(nngl_ranks),'nngl_ranks',.true.)

        allocate(nn_interfacial_ranks(nprcs), stat = ierr)
        call checkerr(ierr,'nn_interfacial_ranks',ilog)
        nn_interfacial_ranks = 0
        call memory_monitor(sizeof(nn_interfacial_ranks),'nn_interfacial_ranks',.true.)

        call MPI_Allgather(nn, 1, MPI_INTEGER4, nn_ranks, 1,           &
                           MPI_INTEGER4, Petsc_Comm_World, ierrcode)
        CHKERRQ(ierrcode)

        call MPI_Allgather(nngl, 1, MPI_INTEGER4, nngl_ranks, 1,       &
                           MPI_INTEGER4, Petsc_Comm_World, ierrcode)
        CHKERRQ(ierrcode)

        call MPI_Allgather(nn_interfacial, 1, MPI_INTEGER4,            &
                           nn_interfacial_ranks, 1, MPI_INTEGER4,      &
                           Petsc_Comm_World, ierrcode)
        CHKERRQ(ierrcode)

        nn_offset = 0
        nngl_offset = 0
        nn_interfacial_offset = 0
        do i = 1, rank
          nn_offset = nn_offset + nn_ranks(i)
          nngl_offset = nngl_offset + nngl_ranks(i)
          nn_interfacial_offset = nn_interfacial_offset +              &
                                  nn_interfacial_ranks(i)
        end do
#endif


!c  allocate remaining arrays for spatial discretization 
        allocate (cvol(nngl), stat = ierr)
        cvol=0.0d0
        call checkerr(ierr,'cvol',ilog)
        call memory_monitor(sizeof(cvol),'cvol',.true.)

    
        allocate (xg(nngl), stat = ierr)
        xg=0.0d0
        call checkerr(ierr,'xg',ilog)
        call memory_monitor(sizeof(xg),'xg',.true.)


        allocate (yg(nngl), stat = ierr)
        yg=0.0d0
        call checkerr(ierr,'yg',ilog)
        call memory_monitor(sizeof(yg),'yg',.true.)


        allocate (zg(nngl), stat = ierr)
        zg=0.0d0
        call checkerr(ierr,'zg',ilog)
        call memory_monitor(sizeof(zg),'zg',.true.)


        allocate (dimcv(3,nngl), stat = ierr)
        dimcv=0.0d0
        call checkerr(ierr,'dimcv',ilog)
        call memory_monitor(sizeof(dimcv),'dimcv',.true.)

!c  calculate spatial increments and coordinates of for centroids of 
!c  control volumes
!c  Parallelized, OpenMP, DSU

        call xyzcoord(discretization_type)

        xlmin = minval(xg)
        xlmax = maxval(xg)
        ylmin = minval(yg)
        ylmax = maxval(yg)
        zlmin = minval(zg)
        zlmax = maxval(zg)

#ifdef PETSC
        call MPI_Allreduce(xlmin,xlmingbl,1,MPI_REAL8,MPI_MIN,         &
                           Petsc_Comm_World,ierr) 
        CHKERRQ(ierr)
        call MPI_Allreduce(xlmax,xlmaxgbl,1,MPI_REAL8,MPI_MAX,         &
                           Petsc_Comm_World,ierr) 
        CHKERRQ(ierr)
        call MPI_Allreduce(ylmin,ylmingbl,1,MPI_REAL8,MPI_MIN,         &
                           Petsc_Comm_World,ierr) 
        CHKERRQ(ierr) 
        call MPI_Allreduce(ylmax,ylmaxgbl,1,MPI_REAL8,MPI_MAX,         &
                           Petsc_Comm_World,ierr) 
        CHKERRQ(ierr)
        call MPI_Allreduce(zlmin,zlmingbl,1,MPI_REAL8,MPI_MIN,         &
                           Petsc_Comm_World,ierr) 
        CHKERRQ(ierr) 
        call MPI_Allreduce(zlmax,zlmaxgbl,1,MPI_REAL8,MPI_MAX,         &
                           Petsc_Comm_World,ierr) 
        CHKERRQ(ierr)  
#else
        xlmingbl = xlmin
        xlmaxgbl = xlmax
        ylmingbl = ylmin
        ylmaxgbl = ylmax
        zlmingbl = zlmin
        zlmaxgbl = zlmax
#endif

        if (xglat(nvxgls)< xglat(nvxgle)) then
          gxlmin = xglat(nvxgls)
          gxlmax = xglat(nvxgle)
        else
          gxlmin = xglat(nvxgle)
          gxlmax = xglat(nvxgls)
        end if

        if (yglat(nvygls)< yglat(nvygle)) then
          gylmin = yglat(nvygls)
          gylmax = yglat(nvygle)
        else
          gylmin = yglat(nvygle)
          gylmax = yglat(nvygls)
        end if

        if (zglat(nvzgls)< zglat(nvzgle)) then
          gzlmin = zglat(nvzgls)
          gzlmax = zglat(nvzgle)
        else
          gzlmin = zglat(nvzgle)
          gzlmax = zglat(nvzgls)
        end if

!c  calculate volumes
!c  Parallelized, OpenMP, DSU

        call cvolume

        !c set node flags, used for node selection
        allocate(node_iflags(nngl), stat = ierr)
        call checkerr(ierr,'node_iflags',ilog)
        node_iflags = -1
        call memory_monitor(sizeof(node_iflags),'node_iflags',.true.)

#ifdef USG
        !c set node coordinates, used for node selection
        allocate(nodes(nngl), stat = ierr)
        call checkerr(ierr,'nodes',ilog)
        call memory_monitor(sizeof(nodes),'nodes',.true.)

        do i = 1, nngl
          nodes(i)%x = xg(i)
          nodes(i)%y = yg(i)
          nodes(i)%z = zg(i)
        end do
#endif

      else

#ifdef USG

        if (discretization_type == 1 .or. discretization_type == 2 .or.&
            discretization_type == 5) then
!c  read spatial discretization parameters and allocate memory

!c  x-direction

          nvxgbl = 0
          ierrcd = 21
          read(itmp,*,err=999,end=999) nxx

          allocate (xmax(nxx), stat = ierr)
          xmax=0.0d0
          call checkerr(ierr,'xmax',ilog)
          call memory_monitor(sizeof(xmax),'xmax',.true.)

          allocate (xmin(nxx), stat = ierr)
          xmin=0.0d0
          call checkerr(ierr,'xmin',ilog)
          call memory_monitor(sizeof(xmin),'xmin',.true.)

          allocate (nvix(nxx), stat = ierr)
          nvix=0
          call checkerr(ierr,'nvix',ilog)
          call memory_monitor(sizeof(nvix),'nvix',.true.)

          do ixx = 1,nxx
            ierrcd = 22
            read(itmp,*,err=999,end=999) nvix(ixx)
            read(itmp,*,err=999,end=999) xmin(ixx), xmax(ixx)
            nvxgbl = nvxgbl+nvix(ixx)
          end do

          allocate (delx(nvxgbl), stat = ierr)
          delx=0.0d0
          call checkerr(ierr,'delx',ilog)
          call memory_monitor(sizeof(delx),'delx',.true.)

          allocate (xglat(nvxgbl), stat = ierr)
          xglat=0.0d0
          call checkerr(ierr,'xglat',ilog)
          call memory_monitor(sizeof(xglat),'xglat',.true.)

!c  y-direction

          nvygbl = 0
          read(itmp,*) nyy

          allocate (ymax(nyy), stat = ierr)
          ymax=0.0d0
          call checkerr(ierr,'ymax',ilog)
          call memory_monitor(sizeof(ymax),'ymax',.true.)

          allocate (ymin(nyy), stat = ierr)
          ymin=0.0d0
          call checkerr(ierr,'ymin',ilog)
          call memory_monitor(sizeof(ymin),'ymin',.true.)

          allocate (nviy(nyy), stat = ierr)
          nviy=0
          call checkerr(ierr,'nviy',ilog)
          call memory_monitor(sizeof(nviy),'nviy',.true.)

          do iyy = 1,nyy
            ierrcd = 23
            read(itmp,*,err=999,end=999) nviy(iyy)
            read(itmp,*,err=999,end=999) ymin(iyy), ymax(iyy)
            nvygbl = nvygbl+nviy(iyy)
          end do

          allocate (dely(nvygbl), stat = ierr)
          dely=0.0d0
          call checkerr(ierr,'dely',ilog)
          call memory_monitor(sizeof(dely),'dely',.true.)

          allocate (yglat(nvygbl), stat = ierr)
          yglat=0.0d0
          call checkerr(ierr,'yglat',ilog)
          call memory_monitor(sizeof(yglat),'yglat',.true.)

!c  z-direction

          nvzgbl = 0
          ierrcd = 24
          read(itmp,*,err=999,end=999) nzz

          allocate (zmax(nzz), stat = ierr)
          zmax=0.0d0
          call checkerr(ierr,'zmax',ilog)
          call memory_monitor(sizeof(zmax),'zmax',.true.)

          allocate (zmin(nzz), stat = ierr)
          zmin=0.0d0
          call checkerr(ierr,'zmin',ilog)
          call memory_monitor(sizeof(zmin),'zmin',.true.)

          allocate (nviz(nzz), stat = ierr)
          nviz=0
          call checkerr(ierr,'nviz',ilog)
          call memory_monitor(sizeof(nviz),'nviz',.true.)

          do izz = 1,nzz
            ierrcd = 25
            read(itmp,*,err=999,end=999) nviz(izz)
            read(itmp,*,err=999,end=999) zmin(izz), zmax(izz)
            nvzgbl = nvzgbl+nviz(izz)
          end do

          allocate (delz(nvzgbl), stat = ierr)
          delz=0.0d0
          call checkerr(ierr,'delz',ilog)
          call memory_monitor(sizeof(delz),'delz',.true.)

          allocate (zglat(nvzgbl), stat = ierr)
          zglat=0.0d0
          call checkerr(ierr,'zglat',ilog)
          call memory_monitor(sizeof(zglat),'zglat',.true.)

!c  calculate total number of control volumes
          nngbl = nvxgbl*nvygbl*nvzgbl

!c  calculate spatial increments and coordinates of for centroids of
!c  control volumes
!c  Parallelized, OpenMP, DSU

          call xyzcoord(discretization_type)

#ifdef PETSC
          if (rank == 0) then
#endif

#ifdef CGAL
            call usg_mesh_set_output_file(prefix(:l_prfx)//'.vtk', 'vtk')

            if (discretization_type == 1) then
              call usg_mesh_from_struct_grid(nvxgbl, nvygbl, nvzgbl,   &
                       xglat, yglat, zglat)
            end if

            if (discretization_type == 2) then
              call usg_mesh_from_struct_boundary(nvxgbl, nvygbl, nvzgbl, &
                       xglat, yglat, zglat)
            end if
#endif
            if (discretization_type == 5) then
              call usg_mesh_use_struct_grid(nvxgbl, nvygbl, nvzgbl,    &
                       xglat, yglat, zglat, prefix(:l_prfx)//'.vtk')
            end if

#ifdef PETSC
          end if
          call petsc_mpi_barrier
#endif

        else if (discretization_type == 3) then

#ifdef PETSC
          if (rank == 0) then
#endif
            str_usg_generate_file = prefix(:l_prfx)//'.usg'
            call usg_mesh_set_output_file(prefix(:l_prfx)//'.vtk', 'vtk')
            call usg_mesh_from_script_file(trim(str_usg_generate_file))

#ifdef PETSC
          end if
          call petsc_mpi_barrier
#endif

        else if (discretization_type == 4) then

#ifdef PETSC
          if (rank == 0) then
#endif
            str_usg_generate_file = prefix(:l_prfx)//'.usgpts'
            call usg_mesh_set_output_file(prefix(:l_prfx)//'.vtk', 'vtk')
            call usg_mesh_from_scalar_points(trim(str_usg_generate_file))
#ifdef PETSC
          end if
          call petsc_mpi_barrier
#endif
        end if

        call sleep(2)
#endif
      end if

!c  read unstructured mesh from vtk file
#ifdef USG
      if (discretization_type > 0) then

        !c read mesh data from vtk file
        !c optimized to use one processor only, do not forget to broadcast variables
        bflag = .false.
        if (rank == 0) then
          if (discretization_type <= 10) then
            call usg_mesh_data_input_vtk(prefix(:l_prfx)//'.vtk',b_legacy_vtk)
          else if (discretization_type == 11) then
            call usg_mesh_data_input_gid_mesh(prefix(:l_prfx)//'.msh')
          end if
        end if
#ifdef PETSC
        call MPI_Allreduce(b_error_flag,bflag,1,MPI_LOGICAL,           &
                           MPI_LOR,Petsc_Comm_World,ierr)
        CHKERRQ(ierr)
#endif
        if (bflag) then
          ierrcd = 26
          goto 999
        end if

#ifdef PETSC
        !c broadcast variables required by other processors
        if (nprcs > 1) then
          call petsc_mpi_barrier

          call MPI_BCAST(b_use_node_matids, 1, MPI_LOGICAL, 0,         &
                         Petsc_Comm_World, ierrcode)
          CHKERRQ(ierrcode)

          call MPI_BCAST(b_use_cell_matids, 1, MPI_LOGICAL, 0,         &
                         Petsc_Comm_World, ierrcode)
          CHKERRQ(ierrcode)

          call MPI_BCAST(b_use_layered_mesh, 1, MPI_LOGICAL, 0,        &
                         Petsc_Comm_World, ierrcode)
          CHKERRQ(ierrcode)

          call MPI_BCAST(cell_type, 1, MPI_INTEGER4, 0,                &
                         Petsc_Comm_World, ierrcode)
          CHKERRQ(ierrcode)

          call MPI_BCAST(num_nodes, 1, MPI_INTEGER4, 0,                &
                         Petsc_Comm_World, ierrcode)
          CHKERRQ(ierrcode)

          call MPI_BCAST(num_cells, 1, MPI_INTEGER4, 0,                &
                         Petsc_Comm_World, ierrcode)
          CHKERRQ(ierrcode)

          call MPI_BCAST(num_nodes_gbl, 1, MPI_INTEGER4, 0,            &
                         Petsc_Comm_World, ierrcode)
          CHKERRQ(ierrcode)

          call MPI_BCAST(num_cells_gbl, 1, MPI_INTEGER4, 0,            &
                         Petsc_Comm_World, ierrcode)
          CHKERRQ(ierrcode)

          call MPI_BCAST(num_nodes_per_cell, 1, MPI_INTEGER4, 0,       &
                         Petsc_Comm_World, ierrcode)
          CHKERRQ(ierrcode)

          call MPI_BCAST(ncon_usg, 1, MPI_INTEGER4, 0,                 &
                         Petsc_Comm_World, ierrcode)
          CHKERRQ(ierrcode)

          call MPI_BCAST(ncon_cell_usg, 1, MPI_INTEGER4, 0,            &
                         Petsc_Comm_World, ierrcode)
          CHKERRQ(ierrcode)

          call MPI_BCAST(num_edge_maxcells, 1, MPI_INTEGER4, 0,        &
                         Petsc_Comm_World, ierrcode)
          CHKERRQ(ierrcode)

          call MPI_BCAST(mnjavs, 1, MPI_INTEGER4, 0,                   &
                         Petsc_Comm_World, ierrcode)
          CHKERRQ(ierrcode)

          call MPI_BCAST(cell_projection, 1, MPI_INTEGER4, 0,          &
                         Petsc_Comm_World, ierrcode)
          CHKERRQ(ierrcode)

          call MPI_BCAST(node_min%x, 1, MPI_REAL8, 0,                  &
                         Petsc_Comm_World, ierrcode)
          CHKERRQ(ierrcode)

          call MPI_BCAST(node_min%y, 1, MPI_REAL8, 0,                  &
                         Petsc_Comm_World, ierrcode)
          CHKERRQ(ierrcode)

          call MPI_BCAST(node_min%z, 1, MPI_REAL8, 0,                  &
                         Petsc_Comm_World, ierrcode)
          CHKERRQ(ierrcode)

          call MPI_BCAST(node_max%x, 1, MPI_REAL8, 0,                  &
                         Petsc_Comm_World, ierrcode)
          CHKERRQ(ierrcode)

          call MPI_BCAST(node_max%y, 1, MPI_REAL8, 0,                  &
                         Petsc_Comm_World, ierrcode)
          CHKERRQ(ierrcode)

          call MPI_BCAST(node_max%z, 1, MPI_REAL8, 0,                  &
                         Petsc_Comm_World, ierrcode)
          CHKERRQ(ierrcode)

          if (b_use_layered_mesh) then

            call MPI_BCAST(num_cell_layers, 1, MPI_INTEGER4, 0,       &
                           Petsc_Comm_World, ierrcode)
            CHKERRQ(ierrcode)

            call MPI_BCAST(num_node_layers, 1, MPI_INTEGER4, 0,       &
                           Petsc_Comm_World, ierrcode)
            CHKERRQ(ierrcode)

            call MPI_BCAST(num_nodes_per_layer, 1, MPI_INTEGER4, 0,   &
                           Petsc_Comm_World, ierrcode)
            CHKERRQ(ierrcode)

            call MPI_BCAST(num_cells_per_layer, 1, MPI_INTEGER4, 0,   &
            Petsc_Comm_World, ierrcode)
            CHKERRQ(ierrcode)

            if (rank > 0) then
              allocate(layer_nodes_top(num_nodes_per_layer), stat = ierr)
              call checkerr(ierr,'layer_nodes_top',ilog)
              call memory_monitor(sizeof(layer_nodes_top),'layer_nodes_top',.false.)

              allocate(layer_nodes_bottom(num_nodes_per_layer), stat = ierr)
              call checkerr(ierr,'layer_nodes_bottom',ilog)
              call memory_monitor(sizeof(layer_nodes_bottom),'layer_nodes_bottom',.false.)
            end if

            !c allocate temporary variables
            allocate(realbuffer(3*num_nodes_per_layer), stat = ierr)
            call checkerr(ierr,'realbuffer',ilog)
            call memory_monitor(sizeof(realbuffer),'realbuffer',.true.)
            !c broadcast layer_nodes_top
            if(rank == 0) then
              do i = 1, num_nodes_per_layer
                realbuffer((i-1)*3+1) = layer_nodes_top(i)%x
                realbuffer((i-1)*3+2) = layer_nodes_top(i)%y
                realbuffer((i-1)*3+3) = layer_nodes_top(i)%z
              end do
            end if

            call MPI_BCAST(realbuffer, num_nodes_per_layer*3,          &
                           MPI_REAL8, 0, Petsc_Comm_World, ierrcode)
            CHKERRQ(ierrcode)

            if(rank > 0) then
              do i = 1, num_nodes_per_layer
                layer_nodes_top(i)%x = realbuffer((i-1)*3+1)
                layer_nodes_top(i)%y = realbuffer((i-1)*3+2)
                layer_nodes_top(i)%z = realbuffer((i-1)*3+3)
              end do
            end if

            !c broadcast layer_nodes_bottom
            if(rank == 0) then
              do i = 1, num_nodes_per_layer
                realbuffer((i-1)*3+1) = layer_nodes_bottom(i)%x
                realbuffer((i-1)*3+2) = layer_nodes_bottom(i)%y
                realbuffer((i-1)*3+3) = layer_nodes_bottom(i)%z
              end do
            end if

            call MPI_BCAST(realbuffer, num_nodes_per_layer*3,          &
                           MPI_REAL8, 0, Petsc_Comm_World, ierrcode)
            CHKERRQ(ierrcode)

            if(rank > 0) then
              do i = 1, num_nodes_per_layer
                layer_nodes_bottom(i)%x = realbuffer((i-1)*3+1)
                layer_nodes_bottom(i)%y = realbuffer((i-1)*3+2)
                layer_nodes_bottom(i)%z = realbuffer((i-1)*3+3)
              end do
            end if

            !c deallocate temporary variables
            call memory_monitor(-sizeof(realbuffer),'realbuffer',.true.)
            deallocate(realbuffer, stat = ierr)
            call checkerr(ierr,'realbuffer',ilog)

          end if

        end if
#endif

        !c renumber node ordering for the unstructured grid
        !c If reordering has already been applied to the mesh file,
        !c it is not necessary to apply again.
        !c Reordering help to reduce bandwidth needed in ILU filling, thus
        !c increasing convergence rate.
        if (btest(usg_mesh_ordering,0) .and. .not. btest(usg_mesh_ordering,1)) then

          nngl = num_nodes          !this part is necessary since nngl value is not set yet
          nngbl = num_nodes

          !mnjavs = 3*nngl*ncon_usg
          !use mnjavs calculated based on the input mesh

          if (rank == 0) then

            !c use mesh reordering but it is not done yet, use the following to
            !c build the variables that are needed in building graph
            call usg_mesh_data_build(.true.)

            !c allocate variables used in graph
            allocate(iavs(nngl+1), stat = ierr)
            iavs=0
            call checkerr(ierr,'iavs',ilog)
            call memory_monitor(sizeof(iavs),'iavs',.false.)

            allocate(javs(mnjavs), stat = ierr)
            javs=0
            call checkerr(ierr,'javs',ilog)
            call memory_monitor(sizeof(javs),'javs',.false.)

            !c build graph (node connection)
            call iajavs_usg

            !c variables needs to be saved for later use
            allocate (node_idx_g2g_lorder(nngl), stat = ierr)
            node_idx_g2g_lorder=0
            call checkerr(ierr,'node_idx_g2g_lorder',ilog)
            call memory_monitor(sizeof(node_idx_g2g_lorder),'node_idx_g2g_lorder',.false.)

            allocate (node_idx_g2g_invord(nngl), stat = ierr)
            node_idx_g2g_invord=0
            call checkerr(ierr,'node_idx_g2g_invord',ilog)
            call memory_monitor(sizeof(node_idx_g2g_invord),'node_idx_g2g_invord',.false.)

            !c allocate other temporary variables used in rcm ordering

            allocate(lwork(nngl), stat = ierr)
            lwork=.false.
            call checkerr(ierr,'lwork',ilog)
            call memory_monitor(sizeof(lwork),'lwork',.false.)

            allocate(iwork(nngl), stat = ierr)
            iwork=0
            call checkerr(ierr,'lwork',ilog)
            call memory_monitor(sizeof(iwork),'iwork',.false.)

            !c calculate bandwidth of the adjacency matrix
            bandwidth1 = adj_bandwidth(nngl, njavs, iavs, javs)

            !c Apply rcm ordering
            if (btest(usg_mesh_ordering,2)) then
              call rcmordering(nngl,njavs,iavs,javs,                   &
                               node_idx_g2g_lorder,                    &
                               node_idx_g2g_invord,lwork,iwork)

            end if

            !c calculate bandwidth of the permuted adjacency matrix
            bandwidth2 = adj_rcm_bandwidth(nngl, njavs, iavs, javs,    &
                             node_idx_g2g_lorder, node_idx_g2g_invord)

            if (b_enable_output) then
              write(*,'(2(a,1x,i0,1x),a)')                             &
                    "Bandwidth is changed from",bandwidth1,            &
                    "to",bandwidth2,"after RCM ordering"

              write(ilog,'(2(a,1x,i0,1x),a)')                          &
                    "Bandwidth is changed from",bandwidth1,            &
                    "to",bandwidth2,"after RCM ordering"
            end if

            !c test: check the reordered mesh
            !ivtk = lun_get()
            !open(ivtk,file=prefix(:l_prfx)//'_rcm.vtk',                &
            !     status='unknown',form='formatted')
            !
            !call write_mesh_data_vtk_ascii(ivtk,"after rcm ordering",  &
            !           node_idx_g2g_lorder,node_idx_g2g_invord)
            !
            !close(ivtk)
            !call lun_free(ivtk)
            !write(*,*) "end of mesh reordering test"
            !c end of test

            !c release temporary variables used in rcm ordering
            call memory_monitor(-sizeof(iavs),'iavs',.false.)
            deallocate(iavs, stat = ierr)
            call checkerr(ierr,'iavs',ilog)

            call memory_monitor(-sizeof(javs),'javs',.false.)
            deallocate(javs, stat = ierr)
            call checkerr(ierr,'javs',ilog)

            call memory_monitor(-sizeof(lwork),'lwork',.false.)
            deallocate(lwork, stat = ierr)
            call checkerr(ierr,'lwork',ilog)

            call memory_monitor(-sizeof(iwork),'iwork',.false.)
            deallocate(iwork, stat = ierr)
            call checkerr(ierr,'lwork',ilog)

            !c apply node reordering to the related variables
            call usg_mesh_data_reorder

            !c save the state after mesh reordering is done
            usg_mesh_ordering = ibset(usg_mesh_ordering,1)

          end if

#ifdef PETSC
          call petsc_mpi_barrier

          if (rank > 0) then
            allocate (node_idx_g2g_lorder(nngl), stat = ierr)
            node_idx_g2g_lorder=0
            call checkerr(ierr,'node_idx_g2g_lorder',ilog)
            call memory_monitor(sizeof(node_idx_g2g_lorder),'node_idx_g2g_lorder',.false.)

            allocate (node_idx_g2g_invord(nngl), stat = ierr)
            node_idx_g2g_invord=0
            call checkerr(ierr,'node_idx_g2g_invord',ilog)
            call memory_monitor(sizeof(node_idx_g2g_invord),'node_idx_g2g_invord',.false.)

          end if

          !c broadcast the state after mesh reordering is done
          call MPI_BCAST(usg_mesh_ordering, 1, MPI_INTEGER4, 0,        &
                       Petsc_Comm_World, ierrcode)
          CHKERRQ(ierrcode)

          !c broadcast the reordered map
          call MPI_BCAST(node_idx_g2g_lorder, nngl, MPI_INTEGER4, 0,   &
                       Petsc_Comm_World, ierrcode)
          CHKERRQ(ierrcode)

          call MPI_BCAST(node_idx_g2g_invord, nngl, MPI_INTEGER4, 0,   &
                       Petsc_Comm_World, ierrcode)
          CHKERRQ(ierrcode)

#endif
        end if

#ifdef PETSC
        !c optimized to use one processor only, do not forget to broadcast variables
        if (rank == 0) then
          call usg_mesh_data_build(.true.)
        end if

        !c broadcast variables required by other processors
        if (nprcs > 1) then
          if (rank > 0) then
            allocate(is_boundary_node_gbl(num_nodes), stat = ierr)
            call checkerr(ierr,'is_boundary_node_gbl',ilog)
            is_boundary_node_gbl = .false.
            call memory_monitor(sizeof(is_boundary_node_gbl),'is_boundary_node_gbl',.false.)

            allocate(is_boundary_cell_gbl(num_cells), stat = ierr)
            call checkerr(ierr,'is_boundary_cell_gbl',ilog)
            is_boundary_cell_gbl = .false.
            call memory_monitor(sizeof(is_boundary_cell_gbl),'is_boundary_cell_gbl',.false.)
          end if

          call petsc_mpi_barrier

          !c broadcast global boundary ndoes and cells
          call MPI_BCAST(is_boundary_node_gbl, num_nodes,              &
                         MPI_LOGICAL, 0, Petsc_Comm_World, ierrcode)
          CHKERRQ(ierrcode)

          call MPI_BCAST(is_boundary_cell_gbl, num_cells,              &
                         MPI_LOGICAL, 0, Petsc_Comm_World, ierrcode)
          CHKERRQ(ierrcode)
        end if

        call solver_dd_create_dmplex

        call solver_dd_mapping_set_dmplex

        !c estimate the new mnjavs after distribution based on the number of
        !c local owned nodes, increase the redundancy ratio 1.2d0 when necessary
        if (nprcs > 1) then
          mnjavs = int(1.2d0*num_nodes/num_nodes_gbl*mnjavs)
        end if
#endif
        !c check if specified control volume method can be used for the sepcified mesh
        if (cell_type /= cell_type_tri .and.                           &
            cell_type /= cell_type_tetra .and.                         &
            cell_type /= cell_type_prism .and.                         &
            cvol_method == cvol_method_vd) then
          cvol_method = cvol_method_md
          if (b_enable_output .and. b_enable_output_gen) then
            write(igen,'(a)') "Control volume method: median dual (reset)"
          end if
        end if

        if (grad_method == grad_method_cgg) then
          if (cell_type /= cell_type_tri .and.                         &
              cell_type /= cell_type_quad .and.                        &
              cell_type /= cell_type_tetra) then
            if (rank == 0) then
              write(*,'(2a)') 'Error: cell type is not supported ',    &
                    'by "cell based green gauss" gradient reconstruction'
              write(ilog,'(2a)') 'Error: cell type is not supported ', &
                    'by "cell based green gauss" gradient reconstruction'
            end if
            ierrcd = 27
            goto 998
          end if
        end if

        !c build mesh structure
        call usg_mesh_data_build(.false.)

        !c export mesh to pflotran mesh format
        if (b_export_mesh_pflotran) then
#ifdef PETSC
          if (rank == 0) then
            call usg_mesh_data_export_pflotran(                        &
                     prefix(:l_prfx)//'_pflotran.mesh',                &
                     num_nodes_gbl, num_cells_gbl, nodes_gbl, cells_gbl)
          end if
#else
          call usg_mesh_data_export_pflotran(                          &
                   prefix(:l_prfx)//'_pflotran.mesh',                  &
                   num_nodes, num_cells, nodes, cells)
#endif
        end if

!cdsu free unnecessary memory after final use in initopgs
#ifdef PETSC
        if (allocated(nodes_gbl)) then
          call memory_monitor(-sizeof(nodes_gbl),'nodes_gbl',.false.)
          deallocate(nodes_gbl)
        end if
        if (allocated(cells_gbl)) then
          call memory_monitor(-sizeof(cells_gbl),'cells_gbl',.false.)
          deallocate(cells_gbl)
        end if
#endif

        !c *******************************************************************
        !c !!! IMPORTNANT NOTE: GREEN GAUSS METHOD FOR TETRA IS NOT STRICT !!!
        !c !!!     IN THE CURRENT CODE, FURTHER IMPROVEMENT IS NEEDED      !!!
        !c *******************************************************************
        if (grad_method == grad_method_gg .and. cell_type /= cell_type_tri) then
          if (rank == 0) then
            write(*,'(a)') "Warning: Green-Gauss is not strict for current mesh"
            write(ilog,'(a)') "Warning: Green-Gauss is not strict for current mesh"
          end if
        end if

        !c need modification for parallel codes
        nngl = num_nodes
#ifdef PETSC
        nn = num_nodes_loc
        nngbl = num_nodes_gbl
#else
        nn = num_nodes
        nngbl = num_nodes
#endif

!c  allocate remaining arrays for spatial discretization
        allocate (cvol(nngl), stat = ierr)
        cvol=0.0d0
        call checkerr(ierr,'cvol',ilog)
        call memory_monitor(sizeof(cvol),'cvol',.true.)

        allocate (xg(nngl), stat = ierr)
        xg=0.0d0
        call checkerr(ierr,'xg',ilog)
        call memory_monitor(sizeof(xg),'xg',.true.)

        allocate (yg(nngl), stat = ierr)
        yg=0.0d0
        call checkerr(ierr,'yg',ilog)
        call memory_monitor(sizeof(yg),'yg',.true.)

        allocate (zg(nngl), stat = ierr)
        zg=0.0d0
        call checkerr(ierr,'zg',ilog)
        call memory_monitor(sizeof(zg),'zg',.true.)

!c  assign coordinates to xg, yg, zg array. This is not required if
!c  use nodes(i)%x, nodes(i)%y, nodes(i)%z instead of xg, yg and zg
        do i = 1, nngl
          xg(i) = nodes(i)%x
          yg(i) = nodes(i)%y
          zg(i) = nodes(i)%z
        end do

        xlmin = minval(xg)
        xlmax = maxval(xg)
        ylmin = minval(yg)
        ylmax = maxval(yg)
        zlmin = minval(zg)
        zlmax = maxval(zg)

#ifdef PETSC
        call MPI_Allreduce(xlmin,xlmingbl,1,MPI_REAL8,MPI_MIN,         &
                           Petsc_Comm_World,ierr) 
        CHKERRQ(ierr)
        call MPI_Allreduce(xlmax,xlmaxgbl,1,MPI_REAL8,MPI_MAX,         &
                           Petsc_Comm_World,ierr) 
        CHKERRQ(ierr)
        call MPI_Allreduce(ylmin,ylmingbl,1,MPI_REAL8,MPI_MIN,         &
                           Petsc_Comm_World,ierr) 
        CHKERRQ(ierr) 
        call MPI_Allreduce(ylmax,ylmaxgbl,1,MPI_REAL8,MPI_MAX,         &
                           Petsc_Comm_World,ierr) 
        CHKERRQ(ierr)
        call MPI_Allreduce(zlmin,zlmingbl,1,MPI_REAL8,MPI_MIN,         &
                           Petsc_Comm_World,ierr) 
        CHKERRQ(ierr) 
        call MPI_Allreduce(zlmax,zlmaxgbl,1,MPI_REAL8,MPI_MAX,         &
                           Petsc_Comm_World,ierr) 
        CHKERRQ(ierr)  
#else
        xlmingbl = xlmin
        xlmaxgbl = xlmax
        ylmingbl = ylmin
        ylmaxgbl = ylmax
        zlmingbl = zlmin
        zlmaxgbl = zlmax
#endif

!c  calculate volumes
!c  Parallelized, OpenMP, DSU
        call cvolume

        !c set node flags, used for node selection
        allocate(node_iflags(nngl), stat = ierr)
        node_iflags = -1
        call checkerr(ierr,'node_iflags',ilog)
        call memory_monitor(sizeof(node_iflags),'node_iflags',.true.)

        allocate(cell_iflags(num_cells), stat = ierr)
        cell_iflags = -1
        call checkerr(ierr,'cell_iflags',ilog)
        call memory_monitor(sizeof(cell_iflags),'cell_iflags',.true.)

        allocate (mpropvs_cell(num_cells), stat = ierr)
        mpropvs_cell = 0
        call checkerr(ierr,'mpropvs_cell',ilog)
        call memory_monitor(sizeof(mpropvs_cell),'mpropvs_cell',.true.)

        allocate (property_cell_iflag(num_cells), stat = ierr)
        property_cell_iflag=0
        call checkerr(ierr,'property_cell_iflag',ilog)
        call memory_monitor(sizeof(property_cell_iflag),'property_cell_iflag',.true.)

        call cal_cell_node_inverse_dist

!c  domain decomposition
#ifdef PETSC
        if(varsat_flow) then
          call solver_dd_DMDACreate_flow_heat(.true.)
        end if

        if (heat_transport .and. decoupled_type_vs_heat > 1) then
          call solver_dd_DMDACreate_flow_heat(.false.)
        end if
#endif

      end if
#endif

!c  write spatial discretization parameters to generic output file
      if (b_enable_output .and. b_enable_output_gen) then

        write(igen,'(/72a)')('-',i=1,72)
        write(igen,'(a)') section_header(:l_string)
        write(igen,'(72a/)')('-',i=1,72)


#ifdef PETSC
        write(igen,'(a)')                                                &
          "spatial discretization for local grid without ghost nodes"
        if (discretization_type == 0) then
          write(igen,'(a,i10)')                                          &
                'number of control volumes in x-direction        = ',nvx
          write(igen,'(a,i10)')                                          &
                'number of control volumes in y-direction        = ',nvy
          write(igen,'(a,i10)')                                          &
                'number of control volumes in z-direction        = ',nvz
        end if
        write(igen,'(a,i10)')                                            &
              'total number of control volumes                 = ',nn

        write(igen,'(/a)') "spatial discretization for global grid"

        write(igen,'(/a)')                                               &
          "spatial discretization for local grid with ghost nodes"
        if (discretization_type == 0) then
          write(igen,'(a,i10)')                                          &
                'number of control volumes in x-direction        = ',nvxgl
          write(igen,'(a,i10)')                                          &
                'number of control volumes in y-direction        = ',nvygl
          write(igen,'(a,i10)')                                          &
                'number of control volumes in z-direction        = ',nvzgl
        end if
        write(igen,'(a,i10)')                                            &
              'total number of control volumes                 = ',nngl
#endif

        write(igen,'(/a)') "spatial discretization for global grid"
        write(igen,'(a,i10)')                                            &
              'number of control volumes in x-direction        = ',nvxgbl
        write(igen,'(a,i10)')                                            &
              'number of control volumes in y-direction        = ',nvygbl
        write(igen,'(a,i10)')                                            &
              'number of control volumes in z-direction        = ',nvzgbl
        write(igen,'(a,i10)')                                            &
              'total number of control volumes                 = ',nngbl

      end if

      goto 1000

998   continue
      if (rank == 0) then
        write(ilog,*) 'error configuration ', ierrcd
        write(ilog,*) 'section "',section_header(:l_string),'"'
        close(ilog)
      end if
#ifdef PETSC
      call petsc_mpi_finalize
#endif
      stop

999   continue
      if (rank == 0) then
        write(ilog,*) 'error reading input file, error code ', ierrcd
        write(ilog,*) 'section "',section_header(:l_string),'"'
        close(ilog)
      end if
#ifdef PETSC
      call petsc_mpi_finalize
#endif
      stop

1000  continue

      return
      end
