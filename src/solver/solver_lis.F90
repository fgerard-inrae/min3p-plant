!*****Revision Informations Automatically Generated by VisualSVN*****!
!---------------------------------------------------------------------
!> $ID:$
!> $Revision: 786 $
!> $Author: dsu $
!> $Date: 2021-01-06 21:41:32 -0800 (Wed, 06 Jan 2021) $
!> $URL: https://min3psvn.ubc.ca/svn/min3p_thcm/branches/dsu_new_add_2024Jan/src/solver/solver_lis.F90 $
!---------------------------------------------------------------------
!********************************************************************!

#ifdef LIS

!> module: solver_lis
!>
!> written by: Danyang Su
!>
!> module description:
!>
!> Module of solver interface for lis solver
!> (Library of Iterative Solvers for linear systems, pronounced [lis])
!>
!> Note:  
!> See http://www.ssisc.org/lis/index.en.html for detail


module solver_lis

    use solver_lis_common

    implicit none

#include "lisf.h"

    contains

    !>
    !> lis solver initialization 
    !>
    subroutine solver_lis_initialize()

      implicit none

      LIS_INTEGER :: ierrlis

      call lis_initialize(ierrlis)
      call CHKERR(ierrlis)

    end subroutine solver_lis_initialize

    !>
    !> lis solver finalization
    !>
    subroutine solver_lis_finalize()

      implicit none

      LIS_INTEGER :: ierrlis

      call lis_finalize(ierrlis)
      call CHKERR(ierrlis)

    end subroutine solver_lis_finalize

    !>
    !>  compute function B, only assemble the local part, without ghost nodes
    !>
    subroutine solver_lis_compute_function(nngl_in,ndof,b_in,b_lis,    &
                                           b_non_interlaced)

#ifdef OPENMP
      use omp_lib 
#endif

      use gen, only : rank, nprcs, node_idx_lg2l, node_idx_lg2pg,      &
                      numofthreads_global, numofloops_thred_global

      implicit none 

      !c passed variables
      integer :: nngl_in                     !total degrees of freedom
      integer :: ndof                        !degrees of freedom per node
      real*8, allocatable :: b_in(:)
      LIS_VECTOR :: b_lis
      logical :: b_non_interlaced

      !c local variables
      integer :: ivol, ivol_loc, ivol_pg, nvols, idof, irow, irow_pg
      LIS_INTEGER :: ierrlis

      !c begin function

      nvols = nngl_in/ndof

      if (b_non_interlaced) then
#ifdef OPENMP
      !$omp parallel                                                   &
      !$omp if (nngl_in > numofloops_thred_global)                     &
      !$omp num_threads(numofthreads_global)                           &
      !$omp default(shared)                                            &
      !$omp private (ivol,ivol_loc,ivol_pg,idof,irow,irow_pg)                  
      !$omp do schedule(static)
#endif
        do ivol = 1, nvols
#ifdef PETSC
          if(nprcs > 1) then
            ivol_loc = node_idx_lg2l(ivol)
            ivol_pg = node_idx_lg2pg(ivol)
          else
            ivol_loc = ivol 
            ivol_pg = ivol
          end if
#else
          ivol_loc = ivol
          ivol_pg = ivol
#endif
          if (ivol_loc > 0) then                                        !c not ghost node
            do idof = 1, ndof
              irow = nvols*(idof-1)+ivol                                !c one based index
              irow_pg = idof+ndof*(ivol_pg-1)
              call lis_vector_set_value(LIS_INS_VALUE,irow_pg,         &
                       b_in(irow),b_lis,ierrlis)
            end do
          end if
        end do
#ifdef OPENMP
      !$omp end do
      !$omp end parallel
#endif         
      else
#ifdef OPENMP
      !$omp parallel                                                   &
      !$omp if (nngl_in > numofloops_thred_global)                     &
      !$omp num_threads(numofthreads_global)                           &
      !$omp default(shared)                                            &
      !$omp private (ivol,ivol_loc,ivol_pg,idof,irow,irow_pg)                  
      !$omp do schedule(static)
#endif
        do ivol = 1, nvols
#ifdef PETSC
          if(nprcs > 1) then
            ivol_loc = node_idx_lg2l(ivol)
            ivol_pg = node_idx_lg2pg(ivol)
          else
            ivol_loc = ivol 
            ivol_pg = ivol
          end if
#else
          ivol_loc = ivol
          ivol_pg = ivol
#endif
          if (ivol_loc > 0) then              !c not ghost node
            do idof = 1, ndof 
              irow = idof+ndof*(ivol-1)                                 !c one based index
#ifdef PETSC
              if (nprcs > 1) then
                irow_pg = idof+ndof*(ivol_pg-1)            !c one based index
              else
                irow_pg = irow
              end if
#else
              irow_pg = irow
#endif
              call lis_vector_set_value(LIS_INS_VALUE,irow_pg,         &
                       b_in(irow),b_lis,ierrlis)
            end do
          end if
        end do
#ifdef OPENMP
      !$omp end do
      !$omp end parallel
#endif 
      end if

    end subroutine solver_lis_compute_function

    !>
    !> compute jacobian matrix A, only assemble the local part, without ghost nodes
    !>
    subroutine solver_lis_compute_jacobian(nngl_in,nn_in,ndof,         &
                                           b_non_interlaced,ia_in,a_in,&
                                           lis_ia_lg2pg,lis_ja_lg2pg,  &
                                           lis_nnz,lis_matvalue,lis_mat)
#ifdef OPENMP
      use omp_lib 
#endif
      use gen, only : rank, nprcs, b_enable_output, node_idx_lg2l,     &
                      numofloops_thred_global, numofthreads_global

      implicit none

      !c passed variables
      integer :: nngl_in
      integer :: nn_in
      integer :: ndof
      logical :: b_non_interlaced
      integer, allocatable :: ia_in(:)
      real*8, allocatable :: a_in(:)     
      integer, allocatable :: lis_ia_lg2pg(:)
      integer, allocatable :: lis_ja_lg2pg(:)
      integer :: lis_nnz
      real*8, allocatable :: lis_matvalue(:)
      LIS_MATRIX :: lis_mat
      

      !c local variables
      LIS_INTEGER :: ierrlis
      integer :: irow, irow_loc, istart, iend, ivol, ivol_loc, nvols,  &
                 idof, nvols_loc, jtemp, jtemp_pg

      !c begin function
      nvols = nngl_in/ndof
      nvols_loc = nn_in/ndof

      if (b_non_interlaced) then
#ifdef OPENMP
      !$omp parallel                                                   &
      !$omp if (nngl_in > numofloops_thred_global)                     &
      !$omp num_threads(numofthreads_global)                           &
      !$omp default(shared)                                            &
      !$omp private (ivol,ivol_loc,idof,irow,irow_loc,                 &
      !$omp istart,iend,jtemp_pg)                  
      !$omp do schedule(static)
#endif
        do ivol = 1, nvols
#ifdef PETSC
          if (nprcs > 1) then
            ivol_loc = node_idx_lg2l(ivol)
            if (ivol_loc < 0) then
              cycle
            end if
          else
            ivol_loc = ivol
          end if
#else
          ivol_loc = ivol
#endif
          do idof = 1, ndof 
            irow = ivol+nvols*(idof-1)                                !c one based index
            istart = ia_in(irow)
            iend = ia_in(irow+1)-1
            irow_loc = (ivol_loc-1)*ndof+idof                         !c one based index
            jtemp_pg = lis_ia_lg2pg(irow_loc-1)
            lis_matvalue(jtemp_pg:jtemp_pg+iend-istart) = a_in(istart:iend)
          end do
        end do
#ifdef OPENMP
      !$omp end do
      !$omp end parallel
#endif
      else
#ifdef OPENMP
      !$omp parallel                                                   &
      !$omp if (nngl_in > numofloops_thred_global)                     &
      !$omp num_threads(numofthreads_global)                           &
      !$omp default(shared)                                            &
      !$omp private (ivol,ivol_loc,idof,irow,irow_loc,istart,iend,     &
      !$omp jtemp,jtemp_pg)                  
      !$omp do schedule(static)
#endif
        do ivol = 1, nvols
#ifdef PETSC
          if (nprcs > 1) then
            ivol_loc = node_idx_lg2l(ivol)
            if (ivol_loc < 0) then
              cycle
            end if
          else
            ivol_loc = ivol
          end if
#else
          ivol_loc = ivol
#endif
          do idof = 1, ndof 
            irow = idof+ndof*(ivol-1)                                 !c one based index
            irow_loc = idof+ndof*(ivol_loc-1)                         !c one based index
            istart = ia_in(irow)
            iend = ia_in(irow+1)-1
        
            do jtemp = istart, iend
              jtemp_pg = lis_ia_lg2pg(irow_loc-1)+jtemp-istart        !c zero based index
              lis_matvalue(jtemp_pg) = a_in(jtemp)
            end do
          end do
        end do
#ifdef OPENMP
      !$omp end do
      !$omp end parallel
#endif
      end if

      call lis_matrix_set_csr(lis_nnz,lis_ia_lg2pg,lis_ja_lg2pg,       &
                              lis_matvalue,lis_mat,ierrlis)
      call CHKERR(ierrlis)

      call lis_matrix_assemble(lis_mat,ierrlis)
      call CHKERR(ierrlis)

    end subroutine solver_lis_compute_jacobian

    !>
    !> scatter results from lis solver back to local volumes
    !> based on petsc dmda local/global communication
    !> this fucntion can be optimized by passing flow, heat and reactive
    !> transport variables.
    !>
    subroutine solver_lis_scatter_results(nngl_in,nn_in,ndof,x_lis,    &
                                          x_inout,x_inout_loc,         &
                                          b_non_interlaced,bflag_flow, &
                                          bflag_heat,bflag_react)
#ifdef PETSC
#include <petscversion.h>
#if (PETSC_VERSION_MAJOR >= 3 && PETSC_VERSION_MINOR >= 8)
#include <petsc/finclude/petscsys.h>
#include <petsc/finclude/petscdmda.h>
#include <petsc/finclude/petscksp.h>
      use petscsys
      use petscdmda
      use petscksp
#endif
#endif


#ifdef OPENMP
      use omp_lib 
#endif

#ifdef PETSC
      use solver_snes_common, only : dmda_flow, x_flow, x_flow_loc,    &
                                     dmda_heat, x_heat, x_heat_loc,    &
                                     dmda_react, x_react, x_react_loc             
#endif

      use gen, only : rank, nprcs, node_idx_lg2l, node_idx_lg2pg,      &
                      numofthreads_global, numofloops_thred_global
                      

      implicit none 

#if (PETSC_VERSION_MAJOR >= 3 && PETSC_VERSION_MINOR >= 6 && PETSC_VERSION_MINOR < 8)
#include <petsc/finclude/petscsys.h>
#include <petsc/finclude/petscvec.h>
#include <petsc/finclude/petscvec.h90>
#include <petsc/finclude/petscdmda.h>
#include <petsc/finclude/petscksp.h>
#elif (PETSC_VERSION_MAJOR >= 3 && PETSC_VERSION_MINOR < 6)
#include <finclude/petscsys.h>
#include <finclude/petscvec.h>
#include <finclude/petscvec.h90>
#include <finclude/petscdmda.h>
#include <finclude/petscksp.h>
#endif

      !c passed variables
      integer :: nngl_in                         !total degrees of freedom with ghost nodes
      integer :: nn_in                           !total degrees of freedom without ghost nodes
      integer :: ndof                            !degrees of freedom per node
      LIS_VECTOR :: x_lis
      real*8, allocatable :: x_inout(:)          !local solution with ghost nodes
      real*8, allocatable :: x_inout_loc(:)      !local solution without ghost nodes
      logical :: b_non_interlaced
      logical :: bflag_flow
      logical :: bflag_heat
      logical :: bflag_react

      !c local variables
      integer :: ivol, ivol_loc, ivol_pg,  nvols, nvols_loc, idof,     &
                 irow, irow_loc, irow_pg 
      LIS_INTEGER :: is, ie, ierrlis

#ifdef PETSC
      PetscScalar,pointer :: vecpointer(:)
      PetscErrorCode :: ierrpetsc
#endif

      !c begin function
      nvols = nngl_in/ndof
      nvols_loc = nn_in/ndof

      !c get values from LIS solution vector
      call lis_vector_get_range(x_lis,is,ie,ierrlis)
      call CHKERR(ierrlis)

      call lis_vector_get_values(x_lis,is,nn_in,x_inout_loc,ierrlis)
      call CHKERR(ierrlis)
 
      if (nprcs <= 1) then
        if (b_non_interlaced) then
#ifdef OPENMP
      !$omp parallel                                                   &
      !$omp if(nngl_in > numofloops_thred_global)                      &
      !$omp num_threads(numofthreads_global)                           &
      !$omp default(shared)                                            &
      !$omp private(ivol,idof,irow,irow_loc)                  
      !$omp do schedule(static)
#endif
          do ivol = 1, nvols
            do idof = 1, ndof              
              irow = (idof-1)*nvols+ivol
              irow_loc = (ivol-1)*ndof+idof
              x_inout(irow) = x_inout_loc(irow_loc)
            end do
          end do
#ifdef OPENMP
      !$omp end do
      !$omp end parallel
#endif 
        else
#ifdef OPENMP
      !$omp parallel                                                   &
      !$omp if(nngl_in > numofloops_thred_global)                      &
      !$omp num_threads(numofthreads_global)                           &
      !$omp default(shared)                                            &
      !$omp private(irow)                  
      !$omp do schedule(static)
#endif
          do irow = 1, nngl_in
            x_inout(irow) = x_inout_loc(irow)
          end do
#ifdef OPENMP
      !$omp end do
      !$omp end parallel
#endif 
        end if    
        return  
      end if

      !c gather the value for ghost nodes and set value to local subdomain
#ifdef PETSC
      if (nprcs > 1) then
        if (bflag_flow) then         !flow problem or coupled flow and heat transport
          if(b_non_interlaced) then
#ifdef OPENMP
      !$omp parallel                                                   &
      !$omp if(nngl_in > numofloops_thred_global)                      &
      !$omp num_threads(numofthreads_global)                           &
      !$omp default(shared)                                            &
      !$omp private(ivol,idof,ivol_loc,ivol_pg,irow_loc,irow_pg)                  
      !$omp do schedule(static)
#endif
            do ivol = 1, nvols  
              ivol_loc = node_idx_lg2l(ivol)
              if (ivol_loc < 0) then
                cycle
              end if
              ivol_pg = node_idx_lg2pg(ivol)
              do idof = 1, ndof  
                irow_loc = idof+ndof*(ivol_loc-1)                       !local one based array
                irow_pg = idof+ndof*(ivol_pg-1)-1                       !petsc use zero based array
                call VecSetValues(x_flow,1,irow_pg,                    &
                                  x_inout_loc(irow_loc),               &  
                                  INSERT_VALUES,ierrpetsc) 
              end do           
            end do
#ifdef OPENMP
      !$omp end do
      !$omp end parallel
#endif
            call VecAssemblyBegin(x_flow,ierrpetsc)
            call VecAssemblyEnd(x_flow,ierrpetsc)
            CHKERRQ(ierrpetsc)

            ! Scatter ghost points to local vector, using the 2-step process
            !     DMGlobalToLocalBegin(), DMGlobalToLocalEnd().
            !  By placing code between these two statements, computations can be
            !  done while messages are in transition. 
            call DMGlobalToLocalBegin(dmda_flow%da,x_flow,INSERT_VALUES, &
                                      x_flow_loc,ierrpetsc)
          
            CHKERRQ(ierrpetsc)
            call DMGlobalToLocalEnd(dmda_flow%da,x_flow,INSERT_VALUES, &
                                    x_flow_loc,ierrpetsc) 
            CHKERRQ(ierrpetsc)

            call VecGetArrayF90(x_flow_loc,vecpointer,ierrpetsc)
            CHKERRQ(ierrpetsc) 

#ifdef OPENMP
      !$omp parallel                                                   &
      !$omp if(nngl_in > numofloops_thred_global)                      &
      !$omp num_threads(numofthreads_global)                           &
      !$omp default(shared)                                            &
      !$omp private(ivol,idof,irow,irow_loc)                  
      !$omp do schedule(static)
#endif
            do ivol = 1, nvols
              do idof = 1, ndof              
                irow = (idof-1)*nvols+ivol
                irow_loc = (ivol-1)*ndof+idof
                x_inout(irow) = vecpointer(irow_loc)
              end do
            end do
#ifdef OPENMP
      !$omp end do
      !$omp end parallel
#endif 

          else          !c interlaced
#ifdef OPENMP
      !$omp parallel                                                   &
      !$omp if(nngl_in > numofloops_thred_global)                      &
      !$omp num_threads(numofthreads_global)                           &
      !$omp default(shared)                                            &
      !$omp private(ivol,idof,ivol_loc,ivol_pg,irow_loc,irow_pg)                  
      !$omp do schedule(static)
#endif
            do ivol = 1, nvols  
              ivol_loc = node_idx_lg2l(ivol)
              ivol_pg = node_idx_lg2pg(ivol)
              do idof = 1, ndof            
                if (ivol_loc < 0) then
                  cycle
                end if
                irow_loc = idof+ndof*(ivol_loc-1)                         !local one based array
                irow_pg = idof+ndof*(ivol_pg-1)-1                         !petsc use zero based array
                call VecSetValues(x_flow,1,irow_pg,                    &
                                  x_inout_loc(irow_loc),               &  
                                  INSERT_VALUES,ierrpetsc) 
              end do           
            end do
#ifdef OPENMP
      !$omp end do
      !$omp end parallel
#endif
            call VecAssemblyBegin(x_flow,ierrpetsc)
            call VecAssemblyEnd(x_flow,ierrpetsc)
            CHKERRQ(ierrpetsc)

            ! Scatter ghost points to local vector, using the 2-step process
            !     DMGlobalToLocalBegin(), DMGlobalToLocalEnd().
            !  By placing code between these two statements, computations can be
            !  done while messages are in transition. 
            call DMGlobalToLocalBegin(dmda_flow%da,x_flow,INSERT_VALUES, &
                                      x_flow_loc,ierrpetsc)
          
            CHKERRQ(ierrpetsc)
            call DMGlobalToLocalEnd(dmda_flow%da,x_flow,INSERT_VALUES, &
                                    x_flow_loc,ierrpetsc) 
            CHKERRQ(ierrpetsc)

            call VecGetArrayF90(x_flow_loc,vecpointer,ierrpetsc)
            CHKERRQ(ierrpetsc) 

#ifdef OPENMP
      !$omp parallel                                                   &
      !$omp if(nngl_in > numofloops_thred_global)                      &
      !$omp num_threads(numofthreads_global)                           &
      !$omp default(shared)                                            &
      !$omp private(irow)                  
      !$omp do schedule(static)
#endif
            do irow = 1, nngl_in
              x_inout(irow) = vecpointer(irow)
            end do
#ifdef OPENMP
      !$omp end do
      !$omp end parallel
#endif
          end if

          call VecRestoreArrayF90(x_flow_loc,vecpointer,ierrpetsc)
          CHKERRQ(ierrpetsc)         
        end if

        if (bflag_heat) then      !decoupled heat transport 
          if(b_non_interlaced) then
#ifdef OPENMP
      !$omp parallel                                                   &
      !$omp if(nngl_in > numofloops_thred_global)                      &
      !$omp num_threads(numofthreads_global)                           &
      !$omp default(shared)                                            &
      !$omp private(ivol,idof,ivol_loc,ivol_pg,irow_loc,irow_pg)                  
      !$omp do schedule(static)
#endif
            do ivol = 1, nvols  
              ivol_loc = node_idx_lg2l(ivol)
              if (ivol_loc < 0) then
                cycle
              end if
              ivol_pg = node_idx_lg2pg(ivol)
              do idof = 1, ndof  
                irow_loc = idof+ndof*(ivol_loc-1)                       !local one based array
                irow_pg = idof+ndof*(ivol_pg-1)-1                       !petsc use zero based array
                call VecSetValues(x_heat,1,irow_pg,                    &
                                  x_inout_loc(irow_loc),               &  
                                  INSERT_VALUES,ierrpetsc) 
              end do           
            end do
#ifdef OPENMP
      !$omp end do
      !$omp end parallel
#endif
            call VecAssemblyBegin(x_heat,ierrpetsc)
            call VecAssemblyEnd(x_heat,ierrpetsc)
            CHKERRQ(ierrpetsc)

            ! Scatter ghost points to local vector, using the 2-step process
            !     DMGlobalToLocalBegin(), DMGlobalToLocalEnd().
            !  By placing code between these two statements, computations can be
            !  done while messages are in transition. 
            call DMGlobalToLocalBegin(dmda_heat%da,x_heat,INSERT_VALUES, &
                                      x_heat_loc,ierrpetsc)
          
            CHKERRQ(ierrpetsc)
            call DMGlobalToLocalEnd(dmda_heat%da,x_heat,INSERT_VALUES, &
                                    x_heat_loc,ierrpetsc) 
            CHKERRQ(ierrpetsc)

            call VecGetArrayF90(x_heat_loc,vecpointer,ierrpetsc)
            CHKERRQ(ierrpetsc) 

#ifdef OPENMP
      !$omp parallel                                                   &
      !$omp if(nngl_in > numofloops_thred_global)                      &
      !$omp num_threads(numofthreads_global)                           &
      !$omp default(shared)                                            &
      !$omp private(ivol,idof,irow,irow_loc)                  
      !$omp do schedule(static)
#endif
            do ivol = 1, nvols
              do idof = 1, ndof              
                irow = (idof-1)*nvols+ivol
                irow_loc = (ivol-1)*ndof+idof
                x_inout(irow) = vecpointer(irow_loc)
              end do
            end do
#ifdef OPENMP
      !$omp end do
      !$omp end parallel
#endif 

          else          !c interlaced
#ifdef OPENMP
      !$omp parallel                                                   &
      !$omp if(nngl_in > numofloops_thred_global)                      &
      !$omp num_threads(numofthreads_global)                           &
      !$omp default(shared)                                            &
      !$omp private(ivol,idof,ivol_loc,ivol_pg,irow_loc,irow_pg)                  
      !$omp do schedule(static)
#endif
            do ivol = 1, nvols  
              ivol_loc = node_idx_lg2l(ivol)
              ivol_pg = node_idx_lg2pg(ivol)
              do idof = 1, ndof            
                if (ivol_loc < 0) then
                  cycle
                end if
                irow_loc = idof+ndof*(ivol_loc-1)                         !local one based array
                irow_pg = idof+ndof*(ivol_pg-1)-1                         !petsc use zero based array
                call VecSetValues(x_heat,1,irow_pg,                    &
                                  x_inout_loc(irow_loc),               &  
                                  INSERT_VALUES,ierrpetsc) 
              end do           
            end do
#ifdef OPENMP
      !$omp end do
      !$omp end parallel
#endif
            call VecAssemblyBegin(x_heat,ierrpetsc)
            call VecAssemblyEnd(x_heat,ierrpetsc)
            CHKERRQ(ierrpetsc)

            ! Scatter ghost points to local vector, using the 2-step process
            !     DMGlobalToLocalBegin(), DMGlobalToLocalEnd().
            !  By placing code between these two statements, computations can be
            !  done while messages are in transition. 
            call DMGlobalToLocalBegin(dmda_heat%da,x_heat,INSERT_VALUES, &
                                      x_heat_loc,ierrpetsc)
          
            CHKERRQ(ierrpetsc)
            call DMGlobalToLocalEnd(dmda_heat%da,x_heat,INSERT_VALUES, &
                                    x_heat_loc,ierrpetsc) 
            CHKERRQ(ierrpetsc)

            call VecGetArrayF90(x_heat_loc,vecpointer,ierrpetsc)
            CHKERRQ(ierrpetsc) 

#ifdef OPENMP
      !$omp parallel                                                   &
      !$omp if(nngl_in > numofloops_thred_global)                      &
      !$omp num_threads(numofthreads_global)                           &
      !$omp default(shared)                                            &
      !$omp private(irow)                  
      !$omp do schedule(static)
#endif
            do irow = 1, nngl_in
              x_inout(irow) = vecpointer(irow)
            end do
#ifdef OPENMP
      !$omp end do
      !$omp end parallel
#endif
          end if

          call VecRestoreArrayF90(x_heat_loc,vecpointer,ierrpetsc)
          CHKERRQ(ierrpetsc)  
        end if

        if (bflag_react) then  !c reactive transport

          if(b_non_interlaced) then
#ifdef OPENMP
      !$omp parallel                                                   &
      !$omp if(nngl_in > numofloops_thred_global)                      &
      !$omp num_threads(numofthreads_global)                           &
      !$omp default(shared)                                            &
      !$omp private(ivol,idof,ivol_loc,ivol_pg,irow_loc,irow_pg)                  
      !$omp do schedule(static)
#endif
            do ivol = 1, nvols  
              ivol_loc = node_idx_lg2l(ivol)
              ivol_pg = node_idx_lg2pg(ivol)
              do idof = 1, ndof            
                if (ivol_loc < 0) then
                  cycle
                end if
                irow_loc = ivol_loc+nvols_loc*(idof-1)                    !local one based array
                irow_pg = ivol_pg+nvols*(idof-1)-1                        !petsc use zero based array
                call VecSetValues(x_react,1,irow_pg,                   &
                                  x_inout_loc(irow_loc),               &  
                                  INSERT_VALUES,ierrpetsc) 
              end do           
            end do
#ifdef OPENMP
      !$omp end do
      !$omp end parallel
#endif
            call VecAssemblyBegin(x_react,ierrpetsc)
            call VecAssemblyEnd(x_react,ierrpetsc)
            CHKERRQ(ierrpetsc)

            ! Scatter ghost points to local vector, using the 2-step process
            !     DMGlobalToLocalBegin(), DMGlobalToLocalEnd().
            !  By placing code between these two statements, computations can be
            !  done while messages are in transition. 
            call DMGlobalToLocalBegin(dmda_react%da,x_react,           &
                                      INSERT_VALUES,x_react_loc,       &
                                      ierrpetsc)
          
            CHKERRQ(ierrpetsc)
            call DMGlobalToLocalEnd(dmda_react%da,x_react,             &
                                    INSERT_VALUES,x_react_loc,         &
                                    ierrpetsc) 
            CHKERRQ(ierrpetsc)

            call VecGetArrayF90(x_react_loc,vecpointer,ierrpetsc)
            CHKERRQ(ierrpetsc) 

#ifdef OPENMP
      !$omp parallel                                                   &
      !$omp if(nngl_in > numofloops_thred_global)                      &
      !$omp num_threads(numofthreads_global)                           &
      !$omp default(shared)                                            &
      !$omp private(ivol,idof,irow,irow_loc)                  
      !$omp do schedule(static)
#endif
            do ivol = 1, nvols
              do idof = 1, ndof              
                irow = (idof-1)*nvols+ivol
                irow_loc = (ivol-1)*ndof+idof
                x_inout(irow) = vecpointer(irow_loc)
              end do
            end do
#ifdef OPENMP
      !$omp end do
      !$omp end parallel
#endif 

          else             !c interlaced
#ifdef OPENMP
      !$omp parallel                                                   &
      !$omp if(nngl_in > numofloops_thred_global)                      &
      !$omp num_threads(numofthreads_global)                           &
      !$omp default(shared)                                            &
      !$omp private(ivol,idof,ivol_loc,ivol_pg,irow_loc,irow_pg)                  
      !$omp do schedule(static)
#endif
            do ivol = 1, nvols  
              ivol_loc = node_idx_lg2l(ivol)
              ivol_pg = node_idx_lg2pg(ivol)
              do idof = 1, ndof            
                if (ivol_loc < 0) then
                  cycle
                end if
                irow_loc = idof+ndof*(ivol_loc-1)                         !local one based array
                irow_pg = idof+ndof*(ivol_pg-1)-1                         !petsc use zero based array
                call VecSetValues(x_react,1,irow_pg,                   &
                                  x_inout_loc(irow_loc),               &  
                                  INSERT_VALUES,ierrpetsc) 
              end do           
            end do
#ifdef OPENMP
      !$omp end do
      !$omp end parallel
#endif
            call VecAssemblyBegin(x_react,ierrpetsc)
            call VecAssemblyEnd(x_react,ierrpetsc)
            CHKERRQ(ierrpetsc)

            ! Scatter ghost points to local vector, using the 2-step process
            !     DMGlobalToLocalBegin(), DMGlobalToLocalEnd().
            !  By placing code between these two statements, computations can be
            !  done while messages are in transition. 
            call DMGlobalToLocalBegin(dmda_react%da,x_react,           &
                                      INSERT_VALUES,x_react_loc,       &
                                      ierrpetsc)
          
            CHKERRQ(ierrpetsc)
            call DMGlobalToLocalEnd(dmda_react%da,x_react,             &
                                    INSERT_VALUES,x_react_loc,         &
                                    ierrpetsc) 
            CHKERRQ(ierrpetsc)

            call VecGetArrayF90(x_react_loc,vecpointer,ierrpetsc)
            CHKERRQ(ierrpetsc) 
#ifdef OPENMP
      !$omp parallel                                                   &
      !$omp if(nngl_in > numofloops_thred_global)                      &
      !$omp num_threads(numofthreads_global)                           &
      !$omp default(shared)                                            &
      !$omp private(irow)                  
      !$omp do schedule(static)
#endif
            do irow = 1, nngl_in
              x_inout(irow) = vecpointer(irow)
            end do
#ifdef OPENMP
      !$omp end do
      !$omp end parallel
#endif
          end if

          call VecRestoreArrayF90(x_react_loc,vecpointer,ierrpetsc)
          CHKERRQ(ierrpetsc) 
        end if

      end if      
#endif      

    end subroutine solver_lis_scatter_results

    !>
    !> create lis solver space for flow problem
    !> domain decomposition is based on PETSc's DMDA for the MPI version
    !>
    subroutine solver_lis_create_flow
#ifdef PETSC
#include <petscversion.h>
#if (PETSC_VERSION_MAJOR >= 3 && PETSC_VERSION_MINOR >= 8)
#include <petsc/finclude/petscsys.h>
      use petscsys
#endif
#endif
      use dual, only : dual_porosity
      use m_heat_transport, only : heat_transport,                     &
                                   decoupled_type_vs_heat
      use gen, only :                                                  &
#ifndef PETSC
                      Petsc_Comm_World,                                &
#endif
                      rank, b_enable_output, numofthreads_global,      &
                      mem_cur, mem_max, memory_monitor,                &
                      nn, nngl, nngbl, njavs, njaglob, nprcs,          &
                      node_idx_lg2l, node_idx_lg2pg, iavs, javs, ilog

      implicit none
#ifdef PETSC
#if (PETSC_VERSION_MAJOR >= 3 && PETSC_VERSION_MINOR >= 6 && PETSC_VERSION_MINOR < 8)
#include <petsc/finclude/petscsys.h>
#elif (PETSC_VERSION_MAJOR >= 3 && PETSC_VERSION_MINOR < 6)
#include <finclude/petscsys.h>
#endif
#endif

      !c local variables
      LIS_INTEGER :: ierrlis
      LIS_INTEGER :: comm

      character(72) :: strbuffer

      integer :: ierr, ivol, istart, iend, i1, irow_loc, ncol_loc,     &
                 ivol_loc, jvol_pg, jtemp, jtemp2, ndof

      !c external subroutines
      external checkerr       

      comm = Petsc_Comm_World

      !c memory allocation for matrix mapping
      !c for non interlaced matrix, rows and columns are rearranged to make
      !c block matrix, same as the matrix used in PETSc

      if (heat_transport .and. decoupled_type_vs_heat <= 1) then
        allocate(lis_ia_lg2pg_flow(0:2*nn), stat = ierr)
        lis_ia_lg2pg_flow = -1
        call checkerr(ierr,'lis_ia_lg2pg_flow',ilog) 
        call memory_monitor(sizeof(lis_ia_lg2pg_flow),'lis_ia_lg2pg_flow',.true.)

        allocate(lis_ja_lg2pg_flow(0:njaglob), stat = ierr)
        lis_ja_lg2pg_flow = -1
        call checkerr(ierr,'lis_ja_lg2pg_flow',ilog)
        call memory_monitor(sizeof(lis_ja_lg2pg_flow),'lis_ja_lg2pg_flow',.true.)

        lis_ia_lg2pg_flow(0) = 0
        irow_loc = 0
        jtemp = 0
        lis_nnz_flow = 0

        do ivol = 1, nngl
          istart = iavs(ivol)
          iend = iavs(ivol+1)-1
#ifdef PETSC
          if (nprcs > 1) then
            ivol_loc = node_idx_lg2l(ivol)
          else
            ivol_loc = ivol
          end if
#else
          ivol_loc = ivol
#endif

          if (ivol_loc > 0) then           !current vol is not ghost node
            ncol_loc = 0
            !c for the first quadrant
            do i1 = istart, iend
#ifdef PETSC
              if (nprcs > 1) then
                jvol_pg = node_idx_lg2pg(javs(i1))
              else
                jvol_pg = javs(i1)
              end if
#else
              jvol_pg = javs(i1)
#endif

              ncol_loc = ncol_loc + 1
              lis_ja_lg2pg_flow(jtemp) = 2*jvol_pg-2  !0 based
              jtemp = jtemp + 1

            end do

            !c for the second quadrant
            do i1 = istart, iend
#ifdef PETSC
              if (nprcs > 1) then
                jvol_pg = node_idx_lg2pg(javs(i1))
              else
                jvol_pg = javs(i1)
              end if
#else
              jvol_pg = javs(i1)
#endif

              ncol_loc = ncol_loc + 1
              lis_ja_lg2pg_flow(jtemp) = 2*jvol_pg-1  !0 based
              jtemp = jtemp + 1

            end do

            irow_loc = irow_loc + 1
            lis_ia_lg2pg_flow(irow_loc) =lis_ia_lg2pg_flow(irow_loc-1)+ncol_loc
            lis_nnz_flow = lis_nnz_flow + ncol_loc

            !write(*,*) "irow_loc",irow_loc,"ja",                                &
            !           lis_ja_lg2pg_flow(lis_ia_lg2pg_flow(irow_loc-1):         &
            !                             lis_ia_lg2pg_flow(irow_loc-1)+ncol_loc-1)

            !c for the original third quadrant and fourth quadrant
            ncol_loc = 0
            !c for the third quadrant
            do i1 = istart, iend
#ifdef PETSC
              if (nprcs > 1) then
                jvol_pg = node_idx_lg2pg(javs(i1))
              else
                jvol_pg = javs(i1)
              end if
#else
              jvol_pg = javs(i1)
#endif

              ncol_loc = ncol_loc + 1
              lis_ja_lg2pg_flow(jtemp) = 2*jvol_pg-1  !0 based
              jtemp = jtemp + 1

            end do

            !c for the forth quadrant
            do i1 = istart, iend
#ifdef PETSC
              if (nprcs > 1) then
                jvol_pg = node_idx_lg2pg(javs(i1))
              else
                jvol_pg = javs(i1)
              end if
#else
              jvol_pg = javs(i1)
#endif

              ncol_loc = ncol_loc + 1
              lis_ja_lg2pg_flow(jtemp) = 2*jvol_pg-2  !0 based
              jtemp = jtemp + 1

            end do

            irow_loc = irow_loc + 1
            lis_ia_lg2pg_flow(irow_loc) =lis_ia_lg2pg_flow(irow_loc-1)+ncol_loc
            lis_nnz_flow = lis_nnz_flow + ncol_loc

            !write(*,*) "irow_loc",irow_loc,"ja",                                &
            !           lis_ja_lg2pg_flow(lis_ia_lg2pg_flow(irow_loc-1):         &
            !                             lis_ia_lg2pg_flow(irow_loc-1)+ncol_loc-1)

          end if

        end do

      else
        if (dual_porosity) then
          allocate(lis_ia_lg2pg_flow(0:2*nn), stat = ierr)
          lis_ia_lg2pg_flow = -1
          call checkerr(ierr,'lis_ia_lg2pg_flow',ilog)
          call memory_monitor(sizeof(lis_ia_lg2pg_flow),'lis_ia_lg2pg_flow',.true.)
        else
          allocate(lis_ia_lg2pg_flow(0:nn), stat = ierr)
          lis_ia_lg2pg_flow = -1
          call checkerr(ierr,'lis_ia_lg2pg_flow',ilog)
          call memory_monitor(sizeof(lis_ia_lg2pg_flow),'lis_ia_lg2pg_flow',.true.)
        end if

        allocate(lis_ja_lg2pg_flow(0:njavs), stat = ierr)
        lis_ja_lg2pg_flow = -1
        call checkerr(ierr,'lis_ja_lg2pg_flow',ilog)
        call memory_monitor(sizeof(lis_ja_lg2pg_flow),'lis_ja_lg2pg_flow',.true.)

        lis_ia_lg2pg_flow(0) = 0
        irow_loc = 0
        jtemp = 0
        lis_nnz_flow = 0

        do ivol = 1, nngl
          istart = iavs(ivol)
          iend = iavs(ivol+1)-1
#ifdef PETSC
          if (nprcs > 1) then
            ivol_loc = node_idx_lg2l(ivol)
          else
            ivol_loc = ivol
          end if
#else
          ivol_loc = ivol
#endif

          if (ivol_loc > 0) then           !current vol is not ghost node
            ncol_loc = 0
            do i1 = istart, iend
#ifdef PETSC
              if (nprcs > 1) then
                jvol_pg = node_idx_lg2pg(javs(i1))
              else
                jvol_pg = javs(i1)
              end if
#else
              jvol_pg = javs(i1)
#endif

              ncol_loc = ncol_loc + 1
              lis_ja_lg2pg_flow(jtemp) = jvol_pg-1    !0 based
              jtemp = jtemp + 1

            end do
            irow_loc = irow_loc + 1
            lis_ia_lg2pg_flow(irow_loc) =lis_ia_lg2pg_flow(irow_loc-1)+ncol_loc
            lis_nnz_flow = lis_nnz_flow + ncol_loc
          end if
        end do

        if (dual_porosity) then
          do ivol = 1, nn
            lis_ia_lg2pg_flow(ivol+nn) = lis_ia_lg2pg_flow(ivol+nn-1) +    &
                                    lis_ia_lg2pg_flow(ivol) -              &
                                    lis_ia_lg2pg_flow(ivol-1)
            lis_nnz_flow = lis_nnz_flow + lis_ia_lg2pg_flow(ivol) -        &
                           lis_ia_lg2pg_flow(ivol-1)
          end do
          do jtemp2 = 1,jtemp
            lis_ja_lg2pg_flow(jtemp+jtemp2) = lis_ja_lg2pg_flow(jtemp2-1)+nn 
          end do
        end if
      
      end if

      !c create matrix based on petsc domain decomposition
      if ((heat_transport .and. decoupled_type_vs_heat <= 1) .or.      &
           dual_porosity) then
        ndof = 2
      else
        ndof = 1
      end if

      allocate(lis_matvalue_flow(0:lis_nnz_flow-1), stat = ierr)
      lis_matvalue_flow = 0.0d0
      call checkerr(ierr,'lis_matvalue_flow',ilog) 
      call memory_monitor(sizeof(lis_matvalue_flow),'lis_matvalue_flow',.true.)

      !c create matrix and vectors
      call lis_matrix_create(comm,lis_a_flow,ierrlis)
      call CHKERR(ierrlis)

      call lis_matrix_set_size(lis_a_flow,ndof*nn,0,ierrlis) !nr: number of rows of global matrix
      call CHKERR(ierrlis)

      call lis_vector_duplicate(lis_a_flow,lis_b_flow,ierrlis)
      call CHKERR(ierrlis)

      call lis_vector_duplicate(lis_a_flow,lis_x_flow,ierrlis)
      call CHKERR(ierrlis)

      !c create global solution array for flow, used in ghost nodes
      allocate(lis_x_flow_loc(nn*ndof), stat = ierr)
      lis_x_flow_loc = 0.0d0
      call checkerr(ierr,'lis_x_flow_loc',ilog)  
      call memory_monitor(sizeof(lis_x_flow_loc),'lis_x_flow_loc',.true.)

      if (nprcs > 1) then
        allocate(lis_x_flow_gbl(nngbl*ndof), stat = ierr)
        lis_x_flow_gbl = 0.0d0
        call checkerr(ierr,'lis_x_flow_gbl',ilog)
        call memory_monitor(sizeof(lis_x_flow_gbl),'lis_x_flow_gbl',.true.)
      end if      

      !c create solver
      call lis_solver_create(lis_solver_flow,ierrlis)
      call CHKERR(ierrlis)

      !c solver options from command line
      !call lis_solver_set_optionC(lis_solver_flow,ierrlis)
      !call CHKERR(ierrlis) 

      !c set solver options
      call lis_solver_set_option(trim(lis_options_flow),             &
                                 lis_solver_flow,ierrlis)
      call CHKERR(ierrlis)

      !call lis_matrix_set_csr(lis_nnz_flow,lis_ia_lg2pg_flow,         &
      !                        lis_ja_lg2pg_flow,lis_matvalue_flow,    &
      !                        lis_a_flow,ierrlis)
      !call CHKERR(ierrlis)

      !call lis_matrix_assemble(lis_a_flow,ierrlis)
      !call CHKERR(ierrlis)

    end subroutine solver_lis_create_flow

    !>
    !> create lis solver space for decoupled heat transport problem
    !> domain decomposition is based on PETSc's DMDA for the MPI version
    !>
    subroutine solver_lis_create_heat
#ifdef PETSC
#include <petscversion.h>
#if (PETSC_VERSION_MAJOR >= 3 && PETSC_VERSION_MINOR >= 8)
#include <petsc/finclude/petscsys.h>
      use petscsys
#endif
#endif
      
      use dual, only : dual_porosity
      use m_heat_transport, only : iaheat, jaheat
      use gen, only :                                                  &
#ifndef PETSC
                      Petsc_Comm_World,                                &
#endif
                      rank, b_enable_output, numofthreads_global,      &
                      mem_cur, mem_max, memory_monitor,                &
                      nn, nngl, nngbl, njaheat, njaglob, nprcs,        &
                      node_idx_lg2l, node_idx_lg2pg, ilog

      implicit none
#ifdef PETSC
#if (PETSC_VERSION_MAJOR >= 3 && PETSC_VERSION_MINOR >= 6 && PETSC_VERSION_MINOR < 8)
#include <petsc/finclude/petscsys.h>
#elif (PETSC_VERSION_MAJOR >= 3 && PETSC_VERSION_MINOR < 6)
#include <finclude/petscsys.h>
#endif
#endif

      !c local variables
      LIS_INTEGER :: ierrlis
      LIS_INTEGER :: comm

      character(72) :: strbuffer

      integer :: ierr, ivol, istart, iend, i1, irow_loc, ncol_loc,     &
                 ivol_loc, jvol_pg, jtemp, jtemp2, ndof

      !c external subroutines
      external checkerr

      comm = Petsc_Comm_World

      !c memory allocation for matrix mapping
      !c for non interlaced matrix, rows and columns are rearranged to make
      !c block matrix, same as the matrix used in PETSc
      
      if (dual_porosity) then
        allocate(lis_ia_lg2pg_heat(0:2*nn), stat = ierr)
        lis_ia_lg2pg_heat = -1
        call checkerr(ierr,'lis_ia_lg2pg_heat',ilog)
        call memory_monitor(sizeof(lis_ia_lg2pg_heat),'lis_ia_lg2pg_heat',.true.)
      else
        allocate(lis_ia_lg2pg_heat(0:nn), stat = ierr)
        lis_ia_lg2pg_heat = -1
        call checkerr(ierr,'lis_ia_lg2pg_heat',ilog)
        call memory_monitor(sizeof(lis_ia_lg2pg_heat),'lis_ia_lg2pg_heat',.true.)
      end if

      allocate(lis_ja_lg2pg_heat(0:njaheat), stat = ierr)
      lis_ja_lg2pg_heat = -1
      call checkerr(ierr,'lis_ja_lg2pg_heat',ilog)
      call memory_monitor(sizeof(lis_ja_lg2pg_heat),'lis_ja_lg2pg_heat',.true.)

      lis_ia_lg2pg_heat(0) = 0
      irow_loc = 0
      jtemp = 0
      lis_nnz_heat = 0

      do ivol = 1, nngl
        istart = iaheat(ivol)
        iend = iaheat(ivol+1)-1
#ifdef PETSC
        if (nprcs > 1) then
          ivol_loc = node_idx_lg2l(ivol)
        else
          ivol_loc = ivol
        end if
#else
        ivol_loc = ivol
#endif

        if (ivol_loc > 0) then           !current vol is not ghost node
          ncol_loc = 0
          do i1 = istart, iend
#ifdef PETSC
            if (nprcs > 1) then
              jvol_pg = node_idx_lg2pg(jaheat(i1))
            else
              jvol_pg = jaheat(i1)
            end if
#else
            jvol_pg = jaheat(i1)
#endif

            ncol_loc = ncol_loc + 1
            lis_ja_lg2pg_heat(jtemp) = jvol_pg-1    !0 based
            jtemp = jtemp + 1
          end do
          irow_loc = irow_loc + 1
          lis_ia_lg2pg_heat(irow_loc) =lis_ia_lg2pg_heat(irow_loc-1)+ncol_loc
          lis_nnz_heat = lis_nnz_heat + ncol_loc
        end if
      end do

      if (dual_porosity) then
        do ivol = 1, nn
          lis_ia_lg2pg_heat(ivol+nn) = lis_ia_lg2pg_heat(ivol+nn-1) +    &
                                  lis_ia_lg2pg_heat(ivol) -              &
                                  lis_ia_lg2pg_heat(ivol-1)
          lis_nnz_heat = lis_nnz_heat + lis_ia_lg2pg_heat(ivol) -        &
                         lis_ia_lg2pg_heat(ivol-1)
        end do
        do jtemp2 = 1,jtemp
          lis_ja_lg2pg_heat(jtemp+jtemp2) = lis_ja_lg2pg_heat(jtemp2-1)+nn 
        end do
      end if
      
      !c create matrix based on petsc domain decomposition
      if (dual_porosity) then
        ndof = 2
      else
        ndof = 1
      end if

      allocate(lis_matvalue_heat(0:lis_nnz_heat-1), stat = ierr)
      lis_matvalue_heat = 0.0d0
      call checkerr(ierr,'lis_matvalue_heat',ilog) 
      call memory_monitor(sizeof(lis_matvalue_heat),'lis_matvalue_heat',.true.)

      !c create matrix and vectors
      call lis_matrix_create(comm,lis_a_heat,ierrlis)
      call CHKERR(ierrlis)

      call lis_matrix_set_size(lis_a_heat,ndof*nn,0,ierrlis) !nr: number of rows of global matrix
      call CHKERR(ierrlis)

      call lis_vector_duplicate(lis_a_heat,lis_b_heat,ierrlis)
      call CHKERR(ierrlis)

      call lis_vector_duplicate(lis_a_heat,lis_x_heat,ierrlis)
      call CHKERR(ierrlis)

      !c create global solution array for heat, used in ghost nodes
      allocate(lis_x_heat_loc(nn*ndof), stat = ierr)
      lis_x_heat_loc = 0.0d0
      call checkerr(ierr,'lis_x_heat_loc',ilog)  
      call memory_monitor(sizeof(lis_x_heat_loc),'lis_x_heat_loc',.true.)

      if (nprcs > 1) then
        allocate(lis_x_heat_gbl(nngbl*ndof), stat = ierr)
        lis_x_heat_gbl = 0.0d0
        call checkerr(ierr,'lis_x_heat_gbl',ilog)
        call memory_monitor(sizeof(lis_x_heat_gbl),'lis_x_heat_gbl',.true.)
      end if      

      !c create solver
      call lis_solver_create(lis_solver_heat,ierrlis)
      call CHKERR(ierrlis)

      !c solver options from command line
      !call lis_solver_set_optionC(lis_solver_heat,ierrlis)
      !call CHKERR(ierrlis) 

      !c set solver options
      call lis_solver_set_option(trim(lis_options_heat),             &
                                 lis_solver_heat,ierrlis)
      call CHKERR(ierrlis)

      !call lis_matrix_set_csr(lis_nnz_heat,lis_ia_lg2pg_heat,         &
      !                        lis_ja_lg2pg_heat,lis_matvalue_heat,    &
      !                        lis_a_heat,ierrlis)
      !call CHKERR(ierrlis)

      !call lis_matrix_assemble(lis_a_heat,ierrlis)
      !call CHKERR(ierrlis)
    end subroutine solver_lis_create_heat

    !>
    !> destroy lis solver for flow problem
    !>
    subroutine solver_lis_release_flow()

      use gen, only : mem_cur, mem_max, memory_monitor

      implicit none

      LIS_INTEGER :: ierrlis

      call lis_solver_destroy(lis_solver_flow,ierrlis)
      call CHKERR(ierrlis)
      !c lis_matrix_destroy causes error invalid pointer when codes finishes 
      !call lis_matrix_destroy(lis_a_flow,ierrlis)
      !call CHKERR(ierrlis)
      call lis_vector_destroy(lis_b_flow,ierrlis)
      call CHKERR(ierrlis)
      call lis_vector_destroy(lis_x_flow,ierrlis)
      call CHKERR(ierrlis)

      call memory_monitor(-sizeof(lis_ia_lg2pg_flow),'lis_ia_lg2pg_flow',.true.)
      call memory_monitor(-sizeof(lis_ja_lg2pg_flow),'lis_ja_lg2pg_flow',.true.)
      call memory_monitor(-sizeof(lis_matvalue_flow),'lis_matvalue_flow',.true.)
      call memory_monitor(-sizeof(lis_x_flow_loc),'lis_x_flow_loc',.true.)

      deallocate(lis_ia_lg2pg_flow)
      deallocate(lis_ja_lg2pg_flow)
      deallocate(lis_matvalue_flow)
      deallocate(lis_x_flow_loc)
 
    end subroutine solver_lis_release_flow

    !>
    !> destroy lis solver for decoupled heat problem
    !>
    subroutine solver_lis_release_heat()

      use gen, only : mem_cur, mem_max, memory_monitor

      implicit none

      LIS_INTEGER :: ierrlis

      call lis_solver_destroy(lis_solver_heat,ierrlis)
      call CHKERR(ierrlis)
      !c lis_matrix_destroy causes error invalid pointer when codes finishes 
      !call lis_matrix_destroy(lis_a_heat,ierrlis)
      !call CHKERR(ierrlis)
      call lis_vector_destroy(lis_b_heat,ierrlis)
      call CHKERR(ierrlis)
      call lis_vector_destroy(lis_x_heat,ierrlis)
      call CHKERR(ierrlis)

      call memory_monitor(-sizeof(lis_ia_lg2pg_heat),'lis_ia_lg2pg_heat',.true.)
      call memory_monitor(-sizeof(lis_ja_lg2pg_heat),'lis_ja_lg2pg_heat',.true.)
      call memory_monitor(-sizeof(lis_matvalue_heat),'lis_matvalue_heat',.true.)
      call memory_monitor(-sizeof(lis_x_heat_loc),'lis_x_heat_loc',.true.)

      deallocate(lis_ia_lg2pg_heat)
      deallocate(lis_ja_lg2pg_heat)
      deallocate(lis_matvalue_heat)
      deallocate(lis_x_heat_loc)
 
    end subroutine solver_lis_release_heat

    !>
    !> lis solver for flow and heat transport problem
    !> domain decomposition is based on PETSc's DMDA for the MPI version
    !>
    subroutine solver_lis_solve_flow(ilog,idetail,a_in,b_in,           &
                      x_inout,ia_in,ja_in,nngl_in,nn_in,ndof,itsolv,   &
                      over_flow,rnorm,row_idx_l2pg,col_idx_l2pg,       &
                      b_non_interlaced)

      use dens, only : iter_sia
      use gen, only : rank, nprcs, iter_vs, iter_seep, ittot_vs,       &
                      iter_glob, ittot_glob, mtime, iprt_flow_lis,     &
                      b_output_matrix, b_enable_output,                &
                      itimestep_output_matrix

      implicit none

      !c passed variables
      integer :: ilog                             !c log file unit
      integer :: idetail                          !c information level
      real*8, allocatable :: a_in(:)              !c local matrix with ghost nodes
      real*8, allocatable :: b_in(:)              !c local rhs with ghost nodes
      real*8, allocatable :: x_inout(:)           !c solution with ghost nodes
      integer, allocatable :: ia_in(:)            !c local matrix structure with ghost nodes ia
      integer, allocatable :: ja_in(:)            !c local matrix structure with ghost nodes, ja
      integer :: nngl_in                          !c total row number of local matrix, with ghost nodes
      integer :: nn_in                            !c total row number of local matrix, without ghost node
      integer :: ndof                             !c degrees of freedom per node
      integer :: itsolv                           !c interation number
      integer, allocatable :: row_idx_l2pg(:)     !c local row index to global row index
      integer, allocatable :: col_idx_l2pg(:)     !c local column index to global column index
      logical :: over_flow                        !c flag to indicate if solver is failed
      real*8 :: rnorm                             !c residual norm
      logical :: b_non_interlaced                 !c flag to indicate if matrix is interlaced

      !c local variables
      integer :: info_debug
      integer :: ivol, nvols, idof
      character(72) :: strinum
      LIS_INTEGER :: ierrlis, iter, iter_double, iter_quad
      LIS_REAL :: resid
      real*8 :: lis_time                          !c lis solver the total time in seconds
      real*8 :: lis_itime                         !c lis solver the time in seconds of the iterations
      real*8 :: lis_ptime                         !c lis solver the time in seconds of the preconditioning
      real*8 :: lis_p_c_time                      !c lis solver the time in seconds of the creation of the preconditioner
      real*8 :: lis_p_i_time                      !c lis solver the time in seconds of the iterations in the preconditioner
      real*8 :: l2g_rhs_time                      !c time spent in seconds of local to global rhs value setting
      real*8 :: l2g_matrix_time                   !c time spent in seconds of local to global matrix assembly
      real*8 :: g2l_solution_time                 !c time spent in seconds of global to local solution
      real*8 :: tot_solver_time                   !c total time spent in seconds of solver

      real*8 :: cputime

      external cputime

      !c begin solver
      info_debug = 0

      over_flow = .false.

      if(b_enable_output .and. (b_output_matrix .or.                   &
         itimestep_output_matrix == mtime)) then
        if(ittot_vs > 0) then
          write(strinum, *) ittot_vs
        else
          write(strinum, *) ittot_glob
        end if
        strinum = "_"//trim(adjustl(strinum))
      end if

      !c assemble rhs
      l2g_rhs_time = cputime()

      call solver_lis_compute_function(nngl_in,ndof,b_in,lis_b_flow,   &
                  b_non_interlaced)

      l2g_rhs_time = cputime() - l2g_rhs_time
      
      if(b_enable_output .and. (b_output_matrix .or.                   &
         itimestep_output_matrix==mtime)) then
        call lis_output_vector(lis_b_flow,LIS_FMT_MM,                  &
                 "b_flow_in"//trim(strinum)//".mtx",ierrlis)
        call CHKERR(ierrlis)
      end if

      !c compute jacobian matrix
      l2g_matrix_time = cputime()

      call solver_lis_compute_jacobian(nngl_in,nn_in,ndof,             &
                                       b_non_interlaced,ia_in,a_in,    &
                                       lis_ia_lg2pg_flow,              &
                                       lis_ja_lg2pg_flow,              &
                                       lis_nnz_flow,                   &
                                       lis_matvalue_flow,              &
                                       lis_a_flow)

      l2g_matrix_time = cputime() - l2g_matrix_time

      if(b_enable_output .and. (b_output_matrix .or.                   &
         itimestep_output_matrix==mtime)) then
        call lis_output_matrix(lis_a_flow,LIS_FMT_MM,                  &
                 "a_flow_in"//trim(strinum)//".mtx",ierrlis)
        call CHKERR(ierrlis)
      end if

      !c solve equation
      call lis_solve(lis_a_flow,lis_b_flow,lis_x_flow,                 &
                     lis_solver_flow,ierrlis)
      call CHKERR(ierrlis)

      if(b_enable_output .and. (b_output_matrix .or.                   &
         itimestep_output_matrix==mtime)) then
        call lis_output_vector(lis_x_flow,LIS_FMT_MM,                  &
                 "x_flow_out"//trim(strinum)//".mtx",ierrlis)
        call CHKERR(ierrlis)
      end if

      !c get solver informations
      call lis_solver_get_iterex(lis_solver_flow,iter,iter_double,     &
                                 iter_quad,ierrlis)
      call CHKERR(ierrlis)
      itsolv = max(iter,iter_double,iter_quad)

      call lis_solver_get_timeex(lis_solver_flow,lis_time,lis_itime,   &
                                 lis_ptime,lis_p_c_time,lis_p_i_time,  &
                                 ierrlis)
      call CHKERR(ierrlis)
      
      call lis_solver_get_residualnorm(lis_solver_flow,resid,ierrlis)
      call CHKERR(ierrlis)
      rnorm = resid

      if((itsolv >= lis_maxit_flow .and. rnorm > lis_abstol_flow).or.&
         rnorm /= rnorm) then
        over_flow = .true.
        if(rank == 0 .and. idetail > 1) then
          write(*,'(1x,a,1x,i6,1x,a,1x,1pe15.6e3)')                    &
                "solver failure: iteration",itsolv,"rnorm",rnorm
          write(ilog,'(1x,a,1x,i6,1x,a,1x,1pe15.6e3)')                 &
                "solver failure: iteration",itsolv,"rnorm",rnorm
        end if
      end if

      !c scatter solution back to local domain
      g2l_solution_time = cputime()

      call solver_lis_scatter_results(nngl_in,nn_in,ndof,lis_x_flow,   &
                                      x_inout,lis_x_flow_loc,          &
                                      b_non_interlaced,.true.,.false., &
                                      .false.)     

      g2l_solution_time = cputime() - g2l_solution_time

      tot_solver_time = l2g_rhs_time + l2g_matrix_time + lis_ptime +   &
                        lis_itime + g2l_solution_time

      if (b_enable_output .and. rank == 0) then
        write(iprt_flow_lis, "(i8,1x,3(i3, 1x),i8,1x,6(1pe15.6e3,2x))")&
              mtime, iter_sia, iter_seep, max(iter_vs,iter_glob),      &
              max(ittot_vs,ittot_glob), l2g_rhs_time, l2g_matrix_time, &
              lis_ptime, lis_itime, g2l_solution_time, tot_solver_time 
      end if

    end subroutine solver_lis_solve_flow

    !>
    !> lis solver for decoupled heat transport problem
    !> domain decomposition is based on PETSc's DMDA for the MPI version
    !>
    subroutine solver_lis_solve_heat(ilog,idetail,a_in,b_in,           &
                      x_inout,ia_in,ja_in,nngl_in,nn_in,ndof,itsolv,   &
                      over_flow,rnorm,row_idx_l2pg,col_idx_l2pg,       &
                      b_non_interlaced)

      use dens, only : iter_sia
      use gen, only : rank, nprcs, iter_heat, iter_seep, ittot_heat,   &
                      mtime, iprt_heat_lis, b_output_matrix,           &
                      b_enable_output, itimestep_output_matrix

      implicit none

      !c passed variables
      integer :: ilog                             !c log file unit
      integer :: idetail                          !c information level
      real*8, allocatable :: a_in(:)              !c local matrix with ghost nodes
      real*8, allocatable :: b_in(:)              !c local rhs with ghost nodes
      real*8, allocatable :: x_inout(:)           !c solution with ghost nodes
      integer, allocatable :: ia_in(:)            !c local matrix structure with ghost nodes ia
      integer, allocatable :: ja_in(:)            !c local matrix structure with ghost nodes, ja
      integer :: nngl_in                          !c total row number of local matrix, with ghost nodes
      integer :: nn_in                            !c total row number of local matrix, without ghost node
      integer :: ndof                             !c degrees of freedom per node
      integer :: itsolv                           !c interation number
      integer, allocatable :: row_idx_l2pg(:)     !c local row index to global row index
      integer, allocatable :: col_idx_l2pg(:)     !c local column index to global column index
      logical :: over_flow                        !c flag to indicate if solver is failed
      real*8 :: rnorm                             !c residual norm
      logical :: b_non_interlaced                 !c flag to indicate if matrix is interlaced

      !c local variables
      integer :: info_debug
      integer :: ivol, nvols, idof
      character(72) :: strinum
      LIS_INTEGER :: ierrlis, iter, iter_double, iter_quad
      LIS_REAL :: resid
      real*8 :: lis_time                          !c lis solver the total time in seconds
      real*8 :: lis_itime                         !c lis solver the time in seconds of the iterations
      real*8 :: lis_ptime                         !c lis solver the time in seconds of the preconditioning
      real*8 :: lis_p_c_time                      !c lis solver the time in seconds of the creation of the preconditioner
      real*8 :: lis_p_i_time                      !c lis solver the time in seconds of the iterations in the preconditioner
      real*8 :: l2g_rhs_time                      !c time spent in seconds of local to global rhs value setting
      real*8 :: l2g_matrix_time                   !c time spent in seconds of local to global matrix assembly
      real*8 :: g2l_solution_time                 !c time spent in seconds of global to local solution
      real*8 :: tot_solver_time                   !c total time spent in seconds of solver

      real*8 :: cputime

      external cputime

      !c begin solver
      info_debug = 0

      over_flow = .false.

      if(b_enable_output .and. (b_output_matrix .or.                   &
         itimestep_output_matrix == mtime)) then
        write(strinum, *) ittot_heat
        strinum = "_"//trim(adjustl(strinum))
      end if

      !c assemble rhs
      l2g_rhs_time = cputime()

      call solver_lis_compute_function(nngl_in,ndof,b_in,lis_b_heat,   &
                  b_non_interlaced)

      l2g_rhs_time = cputime() - l2g_rhs_time
      
      if(b_enable_output .and. (b_output_matrix .or.                   &
         itimestep_output_matrix==mtime)) then
        call lis_output_vector(lis_b_heat,LIS_FMT_MM,                  &
                 "b_heat_in"//trim(strinum)//".mtx",ierrlis)
        call CHKERR(ierrlis)
      end if

      !c compute jacobian matrix
      l2g_matrix_time = cputime()

      call solver_lis_compute_jacobian(nngl_in,nn_in,ndof,             &
                                       b_non_interlaced,ia_in,a_in,    &
                                       lis_ia_lg2pg_heat,              &
                                       lis_ja_lg2pg_heat,              &
                                       lis_nnz_heat,                   &
                                       lis_matvalue_heat,              &
                                       lis_a_heat)

      l2g_matrix_time = cputime() - l2g_matrix_time

      if(b_enable_output .and. (b_output_matrix .or.                   &
         itimestep_output_matrix==mtime)) then
        call lis_output_matrix(lis_a_heat,LIS_FMT_MM,                  &
                 "a_heat_in"//trim(strinum)//".mtx",ierrlis)
        call CHKERR(ierrlis)
      end if

      !c solve equation
      call lis_solve(lis_a_heat,lis_b_heat,lis_x_heat,                 &
                     lis_solver_heat,ierrlis)
      call CHKERR(ierrlis)

      if(b_enable_output .and. (b_output_matrix .or.                   &
         itimestep_output_matrix==mtime)) then
        call lis_output_vector(lis_x_heat,LIS_FMT_MM,                  &
                 "x_heat_out"//trim(strinum)//".mtx",ierrlis)
        call CHKERR(ierrlis)
      end if

      !c get solver informations
      call lis_solver_get_iterex(lis_solver_heat,iter,iter_double,     &
                                 iter_quad,ierrlis)
      call CHKERR(ierrlis)
      itsolv = max(iter,iter_double,iter_quad)

      call lis_solver_get_timeex(lis_solver_heat,lis_time,lis_itime,   &
                                 lis_ptime,lis_p_c_time,lis_p_i_time,  &
                                 ierrlis)
      call CHKERR(ierrlis)
      
      call lis_solver_get_residualnorm(lis_solver_heat,resid,ierrlis)
      call CHKERR(ierrlis)
      rnorm = resid

      if((itsolv >= lis_maxit_heat .and. rnorm > lis_abstol_heat).or.  &
         rnorm /= rnorm) then
        over_flow = .true.
        if(rank == 0 .and. idetail > 1) then
          write(*,'(1x,a,1x,i6,1x,a,1x,1pe15.6e3)')                    &
                "solver failure: iteration",itsolv,"rnorm",rnorm
          write(ilog,'(1x,a,1x,i6,1x,a,1x,1pe15.6e3)')                 &
                "solver failure: iteration",itsolv,"rnorm",rnorm
        end if
      end if

      !c scatter solution back to local domain
      g2l_solution_time = cputime()

      call solver_lis_scatter_results(nngl_in,nn_in,ndof,lis_x_heat,   &
                                      x_inout,lis_x_heat_loc,          &
                                      b_non_interlaced,.false.,.true., &
                                      .false.)     

      g2l_solution_time = cputime() - g2l_solution_time

      tot_solver_time = l2g_rhs_time + l2g_matrix_time + lis_ptime +   &
                        lis_itime + g2l_solution_time

      if (b_enable_output .and. rank == 0) then
        write(iprt_heat_lis, "(i8,1x,3(i3, 1x),i8,1x,6(1pe15.6e3,2x))")&
              mtime, iter_sia, iter_seep, iter_heat, ittot_heat,       &
              l2g_rhs_time, l2g_matrix_time, lis_ptime, lis_itime,     &
              g2l_solution_time, tot_solver_time 
      end if

    end subroutine solver_lis_solve_heat

    !>
    !> create lis solver space for reactive transport problem
    !> domain decomposition is based on PETSc's DMDA for the MPI version
    !>
    subroutine solver_lis_create_react
#ifdef PETSC
#include <petscversion.h>
#if (PETSC_VERSION_MAJOR >= 3 && PETSC_VERSION_MINOR >= 8)
#include <petsc/finclude/petscsys.h>
      use petscsys
#endif
#endif

      use gen, only :                                                  &
#ifndef PETSC
                      Petsc_Comm_World,                                &
#endif
                      rank, b_enable_output, numofthreads_global,      &
                      mem_cur, mem_max, memory_monitor,                &
                      n, nn, nngl, nngbl, njart, nprcs,                &
                      node_idx_lg2l, col_idx_l2pg_rt, iart, jart, ilog

      implicit none
#ifdef PETSC
#if (PETSC_VERSION_MAJOR >= 3 && PETSC_VERSION_MINOR >= 6 && PETSC_VERSION_MINOR < 8)
#include <petsc/finclude/petscsys.h>
#elif (PETSC_VERSION_MAJOR >= 3 && PETSC_VERSION_MINOR < 6)
#include <finclude/petscsys.h>
#endif
#endif

      !c local variables
      LIS_INTEGER :: ierrlis
      LIS_INTEGER :: comm

      character(72) :: strbuffer

      integer :: ierr, irow, irow_rt, istart, iend, irow_loc, ncol_loc,&
                 ivol_loc, jtemp, jtemp2, idof, ndof

      !c external subroutines
      external checkerr       

      comm = Petsc_Comm_World

      ndof = n

      !c memory allocation for matrix mapping

      allocate(lis_ia_lg2pg_react(0:ndof*nn), stat = ierr)
      lis_ia_lg2pg_react = -1
      call checkerr(ierr,'lis_ia_lg2pg_react',ilog) 
      call memory_monitor(sizeof(lis_ia_lg2pg_react),'lis_ia_lg2pg_react',.true.)

      allocate(lis_ja_lg2pg_react(0:njart), stat = ierr)
      lis_ja_lg2pg_react = -1
      call checkerr(ierr,'lis_ja_lg2pg_react',ilog)
      call memory_monitor(sizeof(lis_ja_lg2pg_react),'lis_ja_lg2pg_react',.true.)

      lis_ia_lg2pg_react(0) = 0
      irow_loc = 0
      jtemp = 0
      lis_nnz_react = 0

      do irow = 1, nngl
#ifdef PETSC
        if (nprcs > 1) then
          ivol_loc = node_idx_lg2l(irow)
        else
          ivol_loc = irow
        end if
#else
        ivol_loc = irow
#endif

        if (ivol_loc > 0) then           !current vol is not ghost node
          do idof = 1, ndof
            irow_rt = ndof*(irow-1)+idof
            istart = iart(irow_rt)
            iend = iart(irow_rt+1)-1
            ncol_loc = 0
            do jtemp2 = istart, iend
              ncol_loc = ncol_loc + 1
#ifdef PETSC
              if (nprcs > 1) then
                lis_ja_lg2pg_react(jtemp) = col_idx_l2pg_rt(jtemp2)-1  !0 based
              else
                lis_ja_lg2pg_react(jtemp) = jart(jtemp2)-1             !0 based
              end if
#else
              lis_ja_lg2pg_react(jtemp) = jart(jtemp2)-1               !0 based 
#endif
              jtemp = jtemp + 1              
            end do
  
            irow_loc = irow_loc + 1
            lis_ia_lg2pg_react(irow_loc) =lis_ia_lg2pg_react(irow_loc-1)+ncol_loc
            lis_nnz_react = lis_nnz_react + ncol_loc            
          end do

        end if
      end do

      allocate(lis_matvalue_react(0:lis_nnz_react-1), stat = ierr)
      lis_matvalue_react = 0.0d0
      call checkerr(ierr,'lis_matvalue_react',ilog)
      call memory_monitor(sizeof(lis_matvalue_react),'lis_matvalue_react',.true.)
 
      !c create matrix and vectors
      call lis_matrix_create(comm,lis_a_react,ierrlis)
      call CHKERR(ierrlis)

      !call lis_matrix_set_size(lis_a_react,0,ndof*nngbl,ierrlis) !nr: number of rows of global matrix
      call lis_matrix_set_size(lis_a_react,ndof*nn,0,ierrlis)
      call CHKERR(ierrlis)


      call lis_vector_duplicate(lis_a_react,lis_b_react,ierrlis)
      call CHKERR(ierrlis)

      call lis_vector_duplicate(lis_a_react,lis_x_react,ierrlis)
      call CHKERR(ierrlis)

      !c create global solution array for reactive transport, used in ghost nodes
      allocate(lis_x_react_loc(nn*ndof), stat = ierr)
      lis_x_react_loc = 0.0d0
      call checkerr(ierr,'lis_x_react_loc',ilog)
      call memory_monitor(sizeof(lis_x_react_loc),'lis_x_react_loc',.true.)

      if (nprcs > 1) then
        allocate(lis_x_react_gbl(nngbl*ndof), stat = ierr)
        lis_x_react_gbl = 0.0d0
        call checkerr(ierr,'lis_x_react_gbl',ilog)
        call memory_monitor(sizeof(lis_x_react_gbl),'lis_x_react_gbl',.true.)
      end if

      !c create solver
      call lis_solver_create(lis_solver_react,ierrlis)
      call CHKERR(ierrlis) 

      !c solver options from command line
      !call lis_solver_set_optionC(lis_solver_react,ierrlis)
      !call CHKERR(ierrlis) 

      !c set solver options
      call lis_solver_set_option(trim(lis_options_react),              &
                                 lis_solver_react,ierrlis)
      call CHKERR(ierrlis)


      !call lis_matrix_set_csr(nnz_react,ia_lg2pg_react,               &
      !                        ja_lg2pg_react,matvalue_react,          &
      !                        lis_a_react,ierrlis)
      !call CHKERR(ierrlis)

      !call lis_matrix_assemble(lis_a_react,ierrlis)
      !call CHKERR(ierrlis)

 
    end subroutine solver_lis_create_react

    !>
    !> destroy lis solver for reactive transport problem
    !>
    subroutine solver_lis_release_react()

      use gen, only : mem_cur, mem_max, memory_monitor

      implicit none

      LIS_INTEGER :: ierrlis

      call lis_solver_destroy(lis_solver_react,ierrlis)
      call CHKERR(ierrlis)
      !c lis_matrix_destroy causes error invalid pointer when codes finishes 
      !call lis_matrix_destroy(lis_a_react,ierrlis)
      !call CHKERR(ierrlis)
      call lis_vector_destroy(lis_b_react,ierrlis)
      call CHKERR(ierrlis)
      call lis_vector_destroy(lis_x_react,ierrlis)
      call CHKERR(ierrlis)
 
      !c free space
      call memory_monitor(-sizeof(lis_ia_lg2pg_react),'lis_ia_lg2pg_react',.true.)
      call memory_monitor(-sizeof(lis_ja_lg2pg_react),'lis_ja_lg2pg_react',.true.)
      call memory_monitor(-sizeof(lis_matvalue_react),'lis_matvalue_react',.true.)
      call memory_monitor(-sizeof(lis_x_react_loc),'lis_x_react_loc',.true.)

      deallocate(lis_ia_lg2pg_react)
      deallocate(lis_ja_lg2pg_react)
      deallocate(lis_matvalue_react)
      deallocate(lis_x_react_loc)
 
    end subroutine solver_lis_release_react

    !>
    !> lis solver for reactive transport problem
    !> domain decomposition is based on PETSc's DMDA for the MPI version
    !>
    subroutine solver_lis_solve_react(ilog,idetail,a_in,b_in,          &
                      x_inout,ia_in,ja_in,nngl_in,nn_in,ndof,itsolv,   &
                      over_flow,rnorm,row_idx_l2pg,col_idx_l2pg,       &
                      b_non_interlaced)

      use dens, only : iter_sia
      use gen, only : rank, ittot_rt, iter_rt, mtime, iprt_react_lis,  &
                      b_output_matrix, b_enable_output,                &
                      itimestep_output_matrix

      implicit none

      !c passed variables
      integer :: ilog                             !c log file unit
      integer :: idetail                          !c information level
      real*8, allocatable :: a_in(:)              !c local matrix with ghost nodes
      real*8, allocatable :: b_in(:)              !c local rhs with ghost nodes
      real*8, allocatable :: x_inout(:)           !c solution with ghost nodes
      integer, allocatable :: ia_in(:)            !c local matrix structure with ghost nodes ia
      integer, allocatable :: ja_in(:)            !c local matrix structure with ghost nodes, ja
      integer :: nngl_in                          !c total row number of local matrix, with ghost nodes
      integer :: nn_in                            !c total row number of local matrix, without ghost node
      integer :: ndof                             !c degrees of freedom per node
      integer :: itsolv                           !c interation number
      integer, allocatable :: row_idx_l2pg(:)     !c local row index to global row index
      integer, allocatable :: col_idx_l2pg(:)     !c local column index to global column index
      logical :: over_flow                        !c flag to indicate if solver is failed
      real*8 :: rnorm                             !c residual norm
      logical :: b_non_interlaced                 !c flag to indicate if matrix is interlaced

      !c local variables
      integer :: info_debug
      character(72) :: strinum
      LIS_INTEGER :: ierrlis, iter, iter_double, iter_quad
      LIS_REAL :: resid
      real*8 :: lis_time                          !c lis solver the total time in seconds
      real*8 :: lis_itime                         !c lis solver the time in seconds of the iterations
      real*8 :: lis_ptime                         !c lis solver the time in seconds of the preconditioning
      real*8 :: lis_p_c_time                      !c lis solver the time in seconds of the creation of the preconditioner
      real*8 :: lis_p_i_time                      !c lis solver the time in seconds of the iterations in the preconditioner
      real*8 :: l2g_rhs_time                      !c time spent in seconds of local to global rhs value setting
      real*8 :: l2g_matrix_time                   !c time spent in seconds of local to global matrix assembly
      real*8 :: g2l_solution_time                 !c time spent in seconds of global to local solution
      real*8 :: tot_solver_time                   !c total time spent in seconds of solver

      real*8 :: cputime

      external cputime

      !c begin solver
      info_debug = 0

      over_flow = .false.

      if(b_enable_output .and. (b_output_matrix .or.                   &
         itimestep_output_matrix == mtime)) then
        if(ittot_rt > 0) then
          write(strinum, *) ittot_rt
         end if
        strinum = "_"//trim(adjustl(strinum))
      end if

      !c assemble rhs
      l2g_rhs_time = cputime()

      call solver_lis_compute_function(nngl_in,ndof,b_in,lis_b_react,   &
                  b_non_interlaced)

      l2g_rhs_time = cputime() - l2g_rhs_time
      
      if(b_enable_output .and. (b_output_matrix .or.                    &
         itimestep_output_matrix==mtime)) then
        call lis_output_vector(lis_b_react,LIS_FMT_MM,                  &
                 "b_react_in"//trim(strinum)//".mtx",ierrlis)
        call CHKERR(ierrlis)
      end if

      !c compute jacobian matrix
      l2g_matrix_time = cputime()

      call solver_lis_compute_jacobian(nngl_in,nn_in,ndof,              &
                                       b_non_interlaced,ia_in,a_in,     &
                                       lis_ia_lg2pg_react,              &
                                       lis_ja_lg2pg_react,              &
                                       lis_nnz_react,                   &
                                       lis_matvalue_react,              &
                                       lis_a_react)

      l2g_matrix_time = cputime() - l2g_matrix_time

      if(b_enable_output .and. (b_output_matrix .or.                    &
         itimestep_output_matrix==mtime)) then
        call lis_output_matrix(lis_a_react,LIS_FMT_MM,                  &
                 "a_react_in"//trim(strinum)//".mtx",ierrlis)
        call CHKERR(ierrlis)
      end if

      !c solve equation
      call lis_solve(lis_a_react,lis_b_react,lis_x_react,               &
                     lis_solver_react,ierrlis)
      call CHKERR(ierrlis)

      if(b_enable_output .and. (b_output_matrix .or.                    &
         itimestep_output_matrix==mtime)) then
        call lis_output_vector(lis_x_react,LIS_FMT_MM,                  &
                 "x_react_out"//trim(strinum)//".mtx",ierrlis)
        call CHKERR(ierrlis)
      end if

      !c get solver informations
      call lis_solver_get_iterex(lis_solver_react,iter,iter_double,      &
                                 iter_quad,ierrlis)
      call CHKERR(ierrlis)
      itsolv = max(iter,iter_double,iter_quad)

      call lis_solver_get_timeex(lis_solver_react,lis_time,lis_itime,   &
                                 lis_ptime,lis_p_c_time,lis_p_i_time,   &
                                 ierrlis)
      call CHKERR(ierrlis)
      
      call lis_solver_get_residualnorm(lis_solver_react,resid,ierrlis)
      call CHKERR(ierrlis)
      rnorm = resid

      if((itsolv >= lis_maxit_react .and. rnorm > lis_abstol_react).or.&
         rnorm /= rnorm) then
        over_flow = .true.
        if(rank == 0 .and. idetail > 1) then
          write(*,'(1x,a,1x,i6,1x,a,1x,1pe15.6e3)')                    &
                "solver failure: iteration",itsolv,"rnorm",rnorm
          write(ilog,'(1x,a,1x,i6,1x,a,1x,1pe15.6e3)')                 &
                "solver failure: iteration",itsolv,"rnorm",rnorm
        end if
      end if

      !c scatter solution back to local domain
      g2l_solution_time = cputime()

      call solver_lis_scatter_results(nngl_in,nn_in,ndof,lis_x_react,  &
                                      x_inout,lis_x_react_loc,         &
                                      b_non_interlaced,.false.,.false.,&
                                      .true.)

      g2l_solution_time = cputime() - g2l_solution_time

      tot_solver_time = l2g_rhs_time + l2g_matrix_time + lis_ptime +   &
                        lis_itime + g2l_solution_time

      if (b_enable_output .and. rank == 0) then
        write(iprt_react_lis,"(i8,1x,2(i3,1x),i8,1x,6(1pe15.6e3,2x))") &
              mtime, iter_sia, iter_rt, ittot_rt,                      &
              l2g_rhs_time, l2g_matrix_time, lis_ptime, lis_itime,     &
              g2l_solution_time, tot_solver_time
      end if

    end subroutine solver_lis_solve_react

end module solver_lis

#endif
